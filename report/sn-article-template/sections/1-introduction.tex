\section{Introduction}

Boardgaming has become a really popular hobby and business in the past years.

The field is very broad and the games themselves can be very different.
At the core some elements are shared. Identifying some of these, is the goal of this project.
By scrapping review from the web platform \href{https://boardgamegeek.com/}{BoardGameGeek (BGG)}
to be collected in a corpus, this project aims in identifying the following aspects
\footnote{For a deeper insight into the domain definitions take a look at: \href{https://www.goblins.net/goblinpedia}{Goblinpedia - La tana dei Goblin}}:
% todo Da qui in giu sistema
\begin{itemize}
    \item{\textbf{Luck:}} How much randomization si present in the game.
    The higher the degree of luck in a game the lower the agency power of a player.
    Most games involve some form of luck, almost every dice game, while others do not at all or limit it as much as possible (e.g. "Guards of Atlantis 2").

    \item{\textbf{Bookkeeping:}} Is mostly a negative feature of some games.
    It is the manual recording of data or execution of automatic/semi-automatic game processes.

    \item{\textbf{Downtime:}} It is the passive time in which a player has no agency over the game.

    \item{\textbf{Interaction:}} Degree of influence of one player w.r.t to the others.
    It can be be direct like trading in "Catan" or indirect like gaining valuables in "Wyrmspan".

    \item{\textbf{Bash the leader:}} Is a phenomena present in some games where in order to win players
    have to prevent the victory of whoever is in advantage at the moment.
    This characteristic is most times exploitable by players by not acting against the current leader and instead
    trying to get closer to victory themselves.
    This forces others to sacrifice their possible victory and prioritize bashing.
    A game that most times features bashing is "Root".

    \item{\textbf{Complicated vs Complex}} A game is considered complicated if it has a steep learning curve
    but after learning the rules and the basics it is not difficult to master.
    The results of ones actions are predictable and immediate.
    An example for a complicated game is "Zombicide".
    Complex games on the other hand require critical thinking in order to achieve victory.
    Those games are hard to master and a difference in skill is easily noticeable. A good example could be "Go".
\end{itemize}

The proposed problem shares significant similarities with various aspect extractions/ sentiment analysis solutions.
This made me believe that the problem could be reducible to the same task in another domain.
For this reason for I re-implemented \textit{Attention Based Aspect Extraction}  (ABAE)\cite{he-etal-2017-unsupervised}
that was proposed for that very problem.
Alongside ABAE tweaked versions of the \textit{Latent Dirichlet Allocation} (LDA) were also studied.

\paragraph{Latent Dirichlet Allocation}
LDA is a topic modelling method that has also been widely used for aspect extraction under the unsupervised learning framework.
%%% todo controlla se ok concettualemnte generatore -> inferrer ma gneration step non fa parte di lda defintion
%%%         come soluizione solo del suo processo
It is a probabilistic model in which documents are assumed to be generated by a mixture of topics.
LDA does not directly classify documents but assigns a topic distribution to the input.
Rather than predefined, these latent topics are inferred from the corpus that is given during the model generation.
Words are not bound to a single class and can appear across multiple ones with different probability.
The topic distribution for each document is drawn from a Dirichlet prior from which it gets its name.
%%%%

LDA has shown to be quite effective before the arrival of Transformers. A limit to overcome is that aspect extraction
is more fine-grained than simple topic modelling.
To tackle this problem I decided to explore two possible solutions:
\begin{itemize}
    \item{\textit{Local-LDA}}: A commonly used tweak on LDA in aspect extraction.

    We feed the model sentences so that the topic extraction is local.
    % todo scrivi in conclusioni che LocalLDA era underwhelming per via del fatto che le review sono al piu solitmante righe.
    \item{\textit{NOUN-LDA}}: In the opinion mining research it has been observed that % ref paper
    the main holder of information when identifying aspects are nouns.

    I tried to apply this heuristic by generating LDA on a nouns only processed dataset.
\end{itemize}

\paragraph{Attention Based Aspect Extraction}
ABAE is a neural approach model. Its overall structure can be described as:
\begin{itemize}
    \item {\textbf{Embeddings}}: Generation of embeddings for each word in a \textit{bag of words} input.
    The embeddings model we use is an implementation of \texit{Word2Vec} and runs on the default parameters defined by the \textit{gensim} library.
    \item {\textbf{Attention}}: Weight the embeddings in the sentence using an attention mechanism.
    \item {\textbf{Autoencoder}}: The weighted embeddings are passed to an auto-encoder that reduces the dimensional
    space to the target \textit{aspect} size and reconstructs the attention weighted input.
\end{itemize}

The training objective of the model is to minimize the difference between the decoded sentence reconstruction
and the originally calculated sentence embedding.

\paragraph{}
The trickiest part of the experiment is the lack of ground truth that brings us to an unsupervised learning framework.
In order to overcome this problem we used some commonly used clustering metrics to be able to draw some sort of conclusion on which we
take a deeper look in the coming section.

\subsection{Experimental Setup and Development Environment}
All the training procedures and notebooks were ran locally.

To make full use of the GPU power (\textit{NVIDIA RTX 3070Ti}), dedicated drivers were needed.
The project ran on CUDA 11.8.
A PyTorch backend was used.
This choice was pivoted by the fact that the technology is very popular among the research community.
Libraries and other references are listed in the GitHub repository\cite{Fichera_Muffin_vs_Chihuahua_2024}.

For reproducibility purposes all seeds are set in the code.


