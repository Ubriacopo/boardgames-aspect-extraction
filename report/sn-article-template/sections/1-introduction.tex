\section{Introduction}

Playing boardgames has become really popular hobby and business in the last years.

The field of boardgames is very broad and games can be very different.
But at the core some elements are shared and identifying those, is the goal of this project.
By scrapping review from the web platform \href{https://boardgamegeek.com/}{BoardGameGeek (BGG)}
to be collected in a corpus, this project aims in identifying the following aspects
\footnote{For a deeper insight into the domain definitions take a look at: \href{https://www.goblins.net/goblinpedia}{Goblinpedia - La tana dei Goblin}}:
% todo Da qui in giu sistema
\begin{itemize}
    \item{\textbf{Luck}:} The degree of luck involved in the game.
    Most games involve some form of luck, basically any dice game, while others not at all (e.g. "Guards of Atlantis 2").
    \item{\textbf{Bookkeeping}} How much a player has to keep focus and track on resources or element that involve the winning condition
    \item{\textbf{Downtime}} How much the player has to wait between interacting with the game between turns if there is any
    \item{\textbf{Interaction}} Degree of influence of one player w.r.t to the others
    \item{\textbf{Bash the leader}} If and when to focus on the winning player to avoid him winning.
    A game that has common situations of bashing is Root.
    \item{\textbf{Complicated vs Complex}} The complexity is a difficulty that does not scaled down with the skill while
    a complicated game has only a step initial learning curve.
\end{itemize}

The proposed problem shares significant similarities with various aspect extractions/ sentiment analysis solutions
developed on the Citysearch corpus?TODO REF. This made me believe that the problem could be reduceable to the same task
in another domain reason for I re-implemented Attention Based Aspect Extraction (ABAE)\cite{he-etal-2017-unsupervised} that was proposed for that very problem.
Alongside ABAE I also studied tweaked versions of the Latent Dirichlet Allocation (LDA).
% todo lda paper ref

\paragraph{Latent Dirichlet Allocation}
LDA has been a widely used aspect extraction method in the field for a long time when working in the unsupervised
learning framework, which is our case.

% todo rileggi e correggi
It is a probabilistic model that works under the assumptions that documents observed are created by a generative model.
The documents generated do not tightly belong to a unique class but are generated by a mixture of latent topics.
These topics can overlap meaning that a word is not in an univocal association with a single topic.
The topic distribution in the generative model is drawn from the Dirichlet distribution from which it gets its name.

LDA has shown to be quite effective before the arrival of Transformers. A limit to overcome is that aspect extraction
is more fine grained than simple topic modelling. To solve this problem I decided to explore two possible solutions:
\begin{itemize}
    \item{\textit{Local-LDA}}: A commonly used tweak on LDA.

    We feed the model sentences so that the topic extraction is local.
    % todo scrivi in conclusioni che LocalLDA era underwhelming per via del fatto che le review sono al piu solitmante righe.
    \item{\textit{NOUN-LDA}}: In the opinion mining research it has been observed that % ref paper
    the main holder of information when identifying aspects are nouns.

    I tried to apply this heuristic by generating LDA on a nouns only processed dataset.
\end{itemize}

\paragraph{Attention Based Aspect Extraction}
When ABAE was introduced the developers aimed to tackle the pitfalls of LDA and is based on two core concepts: Embeddings and Attention.
% todo continua
More precisely ABAE is a form of autoencoder where the feature matrix encodes a set of aspect embeddings.
The input generated embeddings of a sentence are weighted by an attention mechanism ?ref to paper di attention?
that helps the process to focus on relevant parts of the input sentence.
The training objective of the model is to minimize the difference between the decoded input embedding, sentence reconstruction,
and the originally calculated sentence embedding.
? fn descritta matematichese
$$J(\theta) = $$

\paragraph{}
The trickiest part of the experiment is the lack of ground truth that brings us to an unsupervised learning framework.
In order to overcome this problem we used some commonly used clustering metrics to be able to draw some sort of conclusion on which we
take a deeper look in the section .%todo ref section

\subsection{Experimental Setup and Development Environment}
To make full use of the GPU power, CUDA drivers were needed.
The project ran on CUDA 11.8.
To implement the ABAE model I used Keras with a Pytorch backend.
This choice was pivoted by the fact that PyTorch is very popular among the research community.
Libraries and other references are listed in the GitHub repository\cite{Fichera_Muffin_vs_Chihuahua_2024}

For reproducibility purposes all seeds are set.

\begin{center}
    \begin{table}
        \begin{tabular}{|c l|}
            \hline
            Component & Model \\ [0.5ex]
            \hline\hline
            GPU       & NVIDIA GeForce RTX3070Ti \\
            \hline
            CPU       & AMD Ryzen 7 5800x        \\
            \hline
            RAM       & 32 GB (2x16GB) DDR4      \\
            \hline
            OS        & Windows 11               \\
            \hline
        \end{tabular}
        \caption{Brief overview of the machine specs}
        \label{specs}

    \end{table}

\end{center}


