\section{Concluding remarks}

While it is not possible to estimate a real application usage of the model the
coherence score tells use enough about the models on the dataset.
The results do not compare well with what hoped as the more complex solution yields a worse metric value.

Some possible issues in the approach were most probably in the processing of the data.
At posterior analysis maybe mapping the numbers all to a single num tag might have hurt the final model.
This could mean that the processing pipeline may be too aggressive and degraded the data.

If not the processing pipeline the real problem could be the dataset.
Reviews might be too similar and therefore measure the quality of it could also be a step to improve the results.
Also, the scrapped dataset from BGG is assumed to contain a good portion of reviews citing the searched aspects,
but we have no guarantee of it.
This could be improved by applying some heuristic or exploiting information given by BGG when downloading the reviews.
An idea could be to use the complexity rating of a game.
We could suppose that games with high or low complexity are most likely to cite this characteristic in reviews.
Other ideas like this could be studied.


Another idea that could be applied is that to run the found model to filter out reviews that too harshly rely
on identified aspects that do not fit the requirements, e.g. "Game components".
This way we would be using a first, more general, model that allows us to specialize our dataset better.
From the new dataset a new model could be studied that probably would achieve better quality measured by the metric.

Lastly for ABAE fixing the negative sample size might have been a mistake.
Twenty reviews opposed to a single in contrastive learning in a low complexity dataset, which
might be our case, is reason for the underwhelming performance of the neural model.

In the end the quality of the dataset should have been better took care for.