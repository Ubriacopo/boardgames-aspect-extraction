\section{Research question and methodology}

The task for which I developed a solution falls is a classification problem.
To identify the aspects defined by the requirements we have to first retrieve a dataset.

Before beginning building a corpus was necessary.


The problem defined more rigorously.
We want to elaborate a classification model thatfsdasd given a sentence/review is able to identify one of the classes defined.
## formal definition ##

\subsection{Dataset and pre-processing}
BGG offers a simple yet effective API to scrap data from their platform.
The API lacks a direct method for listing all boardgames, reason for the existence of a dedicated game information
repository ?todo ntoa pie pagina con link?.

To tackle possible issues of the raw data various different pre-processing pipelines were designed using modular
processing components combined.
The comments scrapped can be subject to a special formatting and, while it is possible to find the country of origin
of a reviewer if he decides to share it, the language of a comment is not explicit.
Thus, all pipelines share a filter on the language of the review, removing all non english ones.

A further step in the processing pipeline which also is the only one introducing in domain knowledge introduced was relative
to the removal of "Kickstarter" relative reviews.
A good portion of the reviews on BGG do not actually give an insight on the game aspects we are inspecting,
but focus strictly on the experience and quality of service of the product coming from a popular crowdfunding platform like "Kickstarter".
To avoid having many redundant and low information records in the dataset, and so give more space to possible informative ones,
we apply the simple heuristic of filtering out any review containing some keywords related to it (e.g. "ks", "pledge").

Reviews are in every iterations split on words recognized and transformed to their lemma thanks to a pre-trained
POS tagger and processor: \textit{spacy}. To furhter reduce redundant and undesired information the pre-processing pipeline
maps game names and numbers to generic tags.
Another step all pipelines have in common is to filter out too short review as we expect to hardly learn anything new from them.

The final designed and used pipelines are:
\begin{itemize}
    \item {\textit{default}}: The default pipeline refers to ABAE specification.

    \item {\textit{NOUN}}: Takes a spin on the default pipeline by filtering all words that are not recognized as nouns
    by the used tagger as discussed in % todo ref a LDA che spiega perche allinzio

    \item {\textit{NOUN-sentence}}: Works like NOUN but at very start of the process the review is split on sentences.
    The splitting creates branches that all work and produce single entries for the processed dataset.

    \item {\textit{default-sentence}}: Variation on default like NOUN-sentence.
\end{itemize}

After running a pipeline any duplicated review is discarded to avoid having repetitions and further introducing of
bias in the dataset.
All these pipelines have been used for an initial evaluation of the models but only for the LDA hyperparameter
tuning the work progressed on more than one of them. This is simply because ABAE is very time consuming while LDA
gave us the possibility to explore more various data types.

\subsection{Metrics}
% todo

\subsection{Developed models}
Once the processed dataset generation has been completed we try the two different approaches to see the best way
to process for each task. Both starting points have been intialized with a 'standard' set of hyperparameters
to later be tuned.

\subsection{LDA}
The model has at first been trained with a smaller dataset of around 80k records in hope to be able to
capture the desired information with less information.
Of the pipelines used the most promising in terms of coherence were the non sentence based models but,
expecting to extract more global features than the locality of the ones given by sentence splitting I
decided to pursuit two paths: NOUN and default-sentence datasets.
While the goal of the project is not much on the quality of the results to actually be able to draw
some conclusions a hyperparameter tuning procedure has been run focusing for LDA on its most important parameter:
the number of topics K.


- default first apporach
- why K is important and why we started with 14
- how tuning worked -> focus on K and not other parameters as there are some that are more important
- metrics used for eavluation

\subsection{ABAE}
- loss fn ->
- optimizer -> fixed lr flexible
- batch size -> why it is important
- epochs low for contraint reasons
- how tuning worked (random search)
- metrics used for evaluation
