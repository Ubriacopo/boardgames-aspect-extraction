\section{Experimental results}


\paragraph{LDA}
After trying

Firstly random runs were executed to have a general idea of the unoptimized solution quality.

Later hyperparameter tuning was applied on 15 different values of $K$ in the range of $K\in [7,35]$
on the three datasets generated by the processing pipeline.
Both datasets showed a preference for a lower value of $K$ as the coherence significantly decreases for
increasing $K$ therefore more values in the range $[7,13]$ were done.

As expected by the decreasing complexity of the dataset the NOUN models perform better in terms of coherence.
The best selected $K$s were then ran on the final data.

\paragraph{ABAE}
For ABAE as for LDA was run on the different datasets.
> cosa ho fatto con ABAE prima di tuning

For ABAE around 20 different configurations for hyperparameters were evaluated.
Some of these performed slightly better than others but, overall, there only was a minor gain in topic coherence with
w.r.t the standard configuration that was used during the first attempt.

new scaletta
- at first glance bad results
- finer analyis suggests that some aspects are bad cohernec while others good
- lda
    - results first + tune + final
    - infer table
    - before after tuning results
- abae
    - results first + tune + final
    - infer table
    - 80k better cohrence ->  unbalanced dataset
    - on non relevant topics
    - before after tuining results

- to finally compare performance of the models a new test set was developed

Results for the project were at first glance underwhelming.
Topic coherence was overall low.
This phenomena might be given by the locality and complexity that derives from the fine-grained aspects to identify.

% Table tuning reults
For ABAE around 20 different configurations for hyperparameters were evaluated.
Some of these performed slightly better than others but, overall, there only was a minor gain in topic coherence with
w.r.t the standard configuration that was used during the first attempt.

Measured values for LDA are a little better but still not in optimal ranges for topic modelling applications.
% todo devi fare coso
For LDA the best processing pipeline in terms of result did not yield the most interpretable model in fact,
the sentence model while performing worse returned a more valuable model.
The identified gold aspects mapped to the identified ones are reported in table. % todo ref table
From the inferred aspects we see that the model has a hard time distinguishing %todo quali
which might be given by the fact that the dataset is not balanced well enough.

The reviews obtained from BGG mostly talk about complexity and components and less about % vbedi se  qualcosa
this unbalance in the data could have led to a bad identification of some required aspects.

ABAE results are reported in table.
The results are less promising in terms of coherence but the found aspects map well to the domain requested
ones.
As expected ABAE is able to better extract underlying associations that allows it to recognize more complex relationships
between potential aspects.

Another possible remark on the results is the processing pipeline while starting directly from the same
seed and sampling the same reviews works differently than the sentence level.

The splitting procedure produces more records from a more similar context thus probably reducing the
quality of the reviews.
