\section{Experimental results}
The first performed runs without tuning the hyperparameters gave us a broad idea
of the unoptimized solution quality on the evaluated metrics.

Both LDA and ABAE only had a small boost in performance by tuning the hyperparameters.
The best identified configurations were then used to make a final evaluation.
The different models were compared where possible.

\paragraph{LDA}
At first an additional processing pipeline was applied for LDA which splits sentences and filters them to have nouns only.
The approach was dropped as the loss of information was too high and the generated model was not on par with the others.

Hyperparameter tuning was therefore done on the two datasets: \textit{NOUN-only} and \textit{sentence}
with final best found $K=7$ for both.
As expected by the decreasing complexity of the dataset, the noun models perform better in terms of coherence.
The NOUN model was unable to recognize some key requirements this probably given by the limited number of aspects of the final configuration.

The sentence model also resulted under-segmented and did not align with the requirements.
For this reason a higher promising value from hyperparameter optimization of $K=13$ was selected
to see if the solution could be improved.
The new model indeed outperformed the best expected model with $K=7$ in both \textit{topic coherence} and \textit{perplexity}.


\begin{center}

    % todo fai questo
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect   & Top relative words                                              & Gold Aspect \\ [0.5ex]
            \hline\hline
            Low-complexity    & \textit{accessible, approachable, light, casual}                &                     \\
            Riddles/Puzzles   & \textit{puzzle, riddle, logic, deductive, reasoning}            & Complex/Complicated \\
            Convoluted        & \textit{tedious, convoluted, punish, predictable, chaotic}      &                     \\
            \hline
            Frustration       & \textit{ready, frustrate, play, stop}                           & Downtime            \\
            \hline
            Instructions      & \textit{instruction, manual, answer, phrase, interpretation}    & Bookkeeping         \\
            \hline
            Interaction       & \textit{player, potentially, opponent, investor, passive, vote} &                     \\
            Cooperation       & \textit{coop, gm, competitively}                                & Interaction         \\
            Bluffing/Plotting & \textit{speculation, bluffing, partnership, auction}            &                     \\
            \hline
            Attack/Protect    & \textit{invade, protect, defend}                                & Bash the leader     \\
            \hline
            -                 & \textit{no aspect matched}                                      & Luck                \\
            \hline
            Various           & ...                                                             & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final NOUN-LDA ($K =13$) model trained on the full data (~310k records).
        One can observe that for \textit{luck} no relation was found.
        The aspect seems to be hardly separable from the others.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{nounlda}

    \end{table}

\end{center}

% todo devi fare coso
For LDA the best processing pipeline in terms of result did not yield the most interpretable model in fact,
the sentence one while performing worse on the measured metrics during human inspection it seemed to be more valuable.

\paragraph{ABAE}
Experiments on ABAE were performed on both the noun and default generation pipelines.
Initially they were done on a small subset of the dataset.

Hyperparameter tuning was performed on 20 different configurations on the full dataset with some performing generally better.
The best settings were chosen by trading off the loss and measured coherence prioritizing lower coherence.
\begin{center}
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect   & Top relative words                                              & Gold Aspect \\ [0.5ex]
            \hline\hline
            Low-complexity    & \textit{accessible, approachable, light, casual}                &                     \\
            Riddles/Puzzles   & \textit{puzzle, riddle, logic, deductive, reasoning}            & Complex/Complicated \\
            Convoluted        & \textit{tedious, convoluted, punish, predictable, chaotic}      &                     \\
            \hline
            Frustration       & \textit{ready, frustrate, play, stop}                           & Downtime            \\
            \hline
            Instructions      & \textit{instruction, manual, answer, phrase, interpretation}    & Bookkeeping         \\
            \hline
            Interaction       & \textit{player, potentially, opponent, investor, passive, vote} &                     \\
            Cooperation       & \textit{coop, gm, competitively}                                & Interaction         \\
            Bluffing/Plotting & \textit{speculation, bluffing, partnership, auction}            &                     \\
            \hline
            Attack/Protect    & \textit{invade, protect, defend}                                & Bash the leader     \\
            \hline
            -                 & \textit{no aspect matched}                                      & Luck                \\
            \hline
            Various           & ...                                                             & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final ABAE model trained on the full data (~310k records).
        One can observe that for \textit{luck} no relation was found.
        The aspect seems to be hardly separable from the others.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{best-310}

    \end{table}

\end{center}

\begin{center}
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect    & Top relative words                                     & Gold Aspect \\ [0.5ex]
            \hline\hline
            Strategy-Asymmetry & \textit{tactic, layer, tactical, strategic, asymetric} & Complex/Complicated \\
            Weight             & \textit{weight, playtime, length, long}                &                     \\
            \hline
            Frustration        & \textit{tend, frustrating, annyoying, drag, problem}   & Downtime            \\
            Analysis Paralysis & \textit{decision, choice, planning, paralzsis}         &                     \\
            \hline
            Game mechanisms    & \textit{scenario, progression, app, ai}                & Bookkeeping         \\
            Ruleset            & \textit{rule, explain, teach, learn, rulset}           &                     \\
            \hline
            Cooperation        & \textit{cooperative, coop, party, family}              & Interaction         \\
            \hline
            Player blocking    & \textit{opponent, force, block, avoid}                 & Bash the leader     \\
            \hline
            Cards/Dice         & \textit{card, flip, face, dice, randmolu}              & Luck                \\
            \hline
            Various            & ...                                                    & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final ABAE model trained on a subsample of the data (~80k records).
        It is more balanced comapred to the other observed model that more heavily relies on compelxity and identifies
        more non relevant aspects for the problem to currently solve.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{best-80}
    \end{table}

\end{center}
% todo gold inferred by best ABAE and best LDA


\begin{center}
    \begin{table}
        \begin{tabular}{c r r r r r}
            \hline
            Model      & $\overline{C}$ & $\overline{C}_5$ & $\overline{C'}_5$ & $l$  & Perplexity\\ [0.5ex]
            \hline
            ABAE       & -12.62         & -10.65           & -10.16            & 3.98 & /          \\
            \hline
            ABAE-small & -6.72          & -5.49            & -3.63             & 4.06 & /          \\
            \hline % todo valori
            NOUN-LDA   & -2.71          & -2.34            & -2.39             & /    & 6.99       \\
            \hline  % todo valori
            sent-LDA   & 3.0            & 2.1              & /                 & -3   & n.d.       \\
            \hline
        \end{tabular}
        \caption{Evaluation results. All evalauted on the same test setw ith $C'$ being the coherence only in relevant aspects.
        }
        \label{performance-review}

    \end{table}

\end{center}

The best overall configuration was indeed the lowest in loss but, compared to the reduced
dataset version, it performed way worse in coherence metrics.
To further investigate the "80k" version of the base model was evaluated on the bigger test set.
It could be supposed that, not only there was bias for some identified aspects, but that the extended
data allowed the model to recognize less prominent patterns.

% todo rileggi
A last model on the optimal settings but trained on a downsized dataset was run.
The mapping between identified aspects and gold ones are reported in table \#\ref{best-80}.
Coherence is overall lower and the aspects seem to be stronger than the one identified by the full data model.

\paragraph{}
Unlike what expected by the ABAE paper LDA outperforms in measured metrics ABAE.
By looking at the found aspects it seems like the neural model is better at capturing more complex relations.

Despite giving a better expected performance the LDA models' mapped aspects were not
as convincing during human evaluation and unable.
They also had a hard time recognizing some required aspects.
This could be related to the loss of contextual information that relates well to
some aspects like downtime where most times it is referred as a frustrating activity
often associated with a negative adjective value thing that is lost by the NOUN only approach.

The task itself could be hard to solve as some aspects tend to overlap like downtime that is most
often related to bookkeeping.
Final models evaluation metrics of the dataset are reported in table \ref{performance-review}.
