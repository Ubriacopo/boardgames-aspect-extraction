\section{Experimental results}
The first performed runs without tuning the hyperparameters gave us a broad idea
of the unoptimized solution quality on the evaluated metrics.

Both LDA and ABAE actually had only a small boost in performance by tuning the hyperparameters.
The best identified configurations were then used to make a final evaluation and try compare the different models where possible.

\paragraph{LDA}
At first an additional processing pipeline was applied for LDA which splits sentences and filters them to have nouns only.
The approach was dropped as the loss of information was too high and the generated model was not on par with the others.

Hyperparameter tuning was therefore done on the two datasets: \textit{NOUN-only} and \textit{sentence} with final best $K=7$ for both.
As expected by the decreasing complexity of the dataset, the noun models perform better in terms of coherence.
The NOUN model was unable to recognize some key requirements this probably given by the limited number of aspects of the final configuration.

The sentence model also resulted under-segmented and did not align with the requirements.
For this reason a higher values of $K$ were selected to see if the solution could be improved.
It was indeed so up to a certain $K$ where the topics resulted too fragmented with a final $K=13$ outperforming
the best expected model with $K=7$ in both \textit{topic coherence} and \textit{perplexity}.

\begin{center}

    % todo fai questo
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect   & Top relative words                                              & Gold Aspect \\ [0.5ex]
            \hline\hline
            Low-complexity    & \textit{accessible, approachable, light, casual}                &                     \\
            Riddles/Puzzles   & \textit{puzzle, riddle, logic, deductive, reasoning}            & Complex/Complicated \\
            Convoluted        & \textit{tedious, convoluted, punish, predictable, chaotic}      &                     \\
            \hline
            Frustration       & \textit{ready, frustrate, play, stop}                           & Downtime            \\
            \hline
            Instructions      & \textit{instruction, manual, answer, phrase, interpretation}    & Bookkeeping         \\
            \hline
            Interaction       & \textit{player, potentially, opponent, investor, passive, vote} &                     \\
            Cooperation       & \textit{coop, gm, competitively}                                & Interaction         \\
            Bluffing/Plotting & \textit{speculation, bluffing, partnership, auction}            &                     \\
            \hline
            Attack/Protect    & \textit{invade, protect, defend}                                & Bash the leader     \\
            \hline
            -                 & \textit{no aspect matched}                                      & Luck                \\
            \hline
            Various           & ...                                                             & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final NOUN-LDA ($K =13$) model trained on the full data (~310k records).
        One can observe that for \textit{luck} no relation was found.
        The aspect seems to be hardly separable from the others.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{nounlda}

    \end{table}

\end{center}

% todo devi fare coso
For LDA the best processing pipeline in terms of result did not yield the most interpretable model in fact,
the sentence one while performing worse on the measured metrics during human inspection it seemed to be more valuable.

\paragraph{ABAE}
Experiments on ABAE were performed on both the noun and default generation pipelines.
Initially they were done on a small subset of the dataset.

Hyperparameter tuning was performed on 20 different configurations on the full dataset with some performing generally better.
The best settings were chosen by trading off the loss and measured coherence prioritizing lower coherence.
\begin{center}
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect   & Top relative words                                              & Gold Aspect \\ [0.5ex]
            \hline\hline
            Low-complexity    & \textit{accessible, approachable, light, casual}                &                     \\
            Riddles/Puzzles   & \textit{puzzle, riddle, logic, deductive, reasoning}            & Complex/Complicated \\
            Convoluted        & \textit{tedious, convoluted, punish, predictable, chaotic}      &                     \\
            \hline
            Frustration       & \textit{ready, frustrate, play, stop}                           & Downtime            \\
            \hline
            Instructions      & \textit{instruction, manual, answer, phrase, interpretation}    & Bookkeeping         \\
            \hline
            Interaction       & \textit{player, potentially, opponent, investor, passive, vote} &                     \\
            Cooperation       & \textit{coop, gm, competitively}                                & Interaction         \\
            Bluffing/Plotting & \textit{speculation, bluffing, partnership, auction}            &                     \\
            \hline
            Attack/Protect    & \textit{invade, protect, defend}                                & Bash the leader     \\
            \hline
            -                 & \textit{no aspect matched}                                      & Luck                \\
            \hline
            Various           & ...                                                             & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final ABAE model trained on the full data (~310k records).
        One can observe that for \textit{luck} no relation was found.
        The aspect seems to be hardly separable from the others.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{best-310}

    \end{table}

\end{center}

\begin{center}
    \begin{table}
        \begin{tabular}{c l c}
            \hline
            Inferred Aspect    & Top relative words                                     & Gold Aspect \\ [0.5ex]
            \hline\hline
            Strategy-Asymmetry & \textit{tactic, layer, tactical, strategic, asymetric} & Complex/Complicated \\
            Weight             & \textit{weight, playtime, length, long}                &                     \\
            \hline
            Frustration        & \textit{tend, frustrating, annyoying, drag, problem}   & Downtime            \\
            Analysis Paralysis & \textit{decision, choice, planning, paralzsis}         &                     \\
            \hline
            Game mechanisms    & \textit{scenario, progression, app, ai}                & Bookkeeping         \\
            Ruleset            & \textit{rule, explain, teach, learn, rulset}           &                     \\
            \hline
            Cooperation        & \textit{cooperative, coop, party, family}              & Interaction         \\
            \hline
            Player blocking    & \textit{opponent, force, block, avoid}                 & Bash the leader     \\
            \hline
            Cards/Dice         & \textit{card, flip, face, dice, randmolu}              & Luck                \\
            \hline
            Various            & ...                                                    & Misc.               \\
            \hline
        \end{tabular}
        \caption{Gold inferred aspects on the final ABAE model trained on a subsample of the data (~80k records).
        It is more balanced comapred to the other observed model that more heavily relies on compelxity and identifies
        more non relevant aspects for the problem to currently solve.
        The various mapped to "Misc" are not reported but can be looked up in the repository.
        }
        \label{best-80}
    \end{table}

\end{center}



\begin{center}
    \begin{table}
        \begin{tabular}{c r r r r r}
            \hline
            Model      & $\overline{C}$ & $\overline{C}_5$ & $\overline{C'}_5$ & $l$  & Perplexity\\ [0.5ex]
            \hline
            ABAE       & -12.62         & -10.65           & -10.16            & 3.98 & /          \\
            \hline
            ABAE-small & -6.72          & -5.49            & -3.63             & 4.06 & /          \\
            \hline % todo valori
            NOUN-LDA   & -2.71          & -2.34            & -2.39             & /    & 6.99       \\
            \hline  % todo valori
            sent-LDA   & 3.0            & 2.1              & /                 & -3   & n.d.       \\
            \hline
        \end{tabular}
        \caption{Evaluation results. All evalauted on the same test setw ith $C'$ being the coherence only in relevant aspects.
        }
        \label{performance-review}

    \end{table}

\end{center}

The best overall configuration was indeed the lowest in loss but, compared to the reduced
dataset version, it performed way worse in coherence metrics.
To further investigate the "80k" version of the base model was evaluated on the bigger test set.
Results indicate that there might be some bias towards some topics of interest in the dataset
that by increasing the dataset size get more relevant.

One could suppose that not only there was bias for some identified aspects but that the extended
data allowed the model to recognize less prominent patterns.

A last model on the optimal settings but trained on a subset of data was run and the aspects
associated to gold ones as reported in the association table.
These seem to be stronger than the ones of the full dataset model.

\paragraph{}
Unlike what expected by the ABAE paper LDA outperforms in measured metrics ABAE.
By looking at the found aspects it seems like the neural model is better at capturing more complex relations.

Despite giving a better expected performance the mapped aspects were not
as convincing during human evaluation and unable to recognize some required aspects.
This could be related to the loss of contextual information that relate well to
some aspects like downtime where most times it is referred as a frustrating activity
often associated with a negative adjective value thing that is lost by this approach.

The task itself could be hard to solve as some aspects tend to overlap like downtime that is most often related to bookkeeping.
Final classification on the dataset are reported in table \ref{performance-review}.
