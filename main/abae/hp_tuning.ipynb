{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# https://www.reddit.com/r/computervision/comments/kfhc3u/how_does_one_fine_tune_cnn_hyperparameter_when/\n",
    "# Guarda tipo di genetic algorithms (YOLO)\n",
    "# Tuning hyperparameters in the context of large datasets can be a problem. I should investigate further."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter, \\\n",
    "    RandomTunableDiscreteParameter\n",
    "\n",
    "seed = 1408\n",
    "config_path = \"./output/config\"\n",
    "config_gen = UniqueParametersConfigFsGenerator(patience=10, seen_configurations_path=config_path)\n",
    "\n",
    "# Parameters definition:\n",
    "embedding_sizes = [70, 100, 160, 250, 340, 430]\n",
    "config_gen.add_parameter('embedding_size', RandomTunableDiscreteParameter(values_list=embedding_sizes, seed=seed))\n",
    "config_gen.add_parameter('aspect_size', RandomTunableOffsetParameter(value_range=(7, 20), step=2, seed=seed))\n",
    "config_gen.add_parameter('negative_sample_size', RandomTunableOffsetParameter(value_range=(8, 20), step=2, seed=seed))\n",
    "config_gen.add_parameter('epochs', RandomTunableOffsetParameter(value_range=(5, 15), step=2, seed=seed))\n",
    "\n",
    "np.random.seed(seed)\n",
    "learning_rates = (10 ** np.random.uniform(-5, -3, 10)).tolist()\n",
    "\n",
    "print(\"Possible learning rates are: \")\n",
    "pprint(learning_rates)\n",
    "\n",
    "config_gen.add_parameter(\"learning_rate\", RandomTunableDiscreteParameter(values_list=learning_rates, seed=seed))\n",
    "config_gen.add_parameter(\"batch_size\", RandomTunableDiscreteParameter(values_list=[64, 128, 256, 512, 1024], seed=seed))"
   ],
   "id": "70f5e33eb12cdf47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'd love to make K-fold CV but for time constraints it is just not viable. <br>\n",
    "Since the dataset is big enough we resort to the classic validation set."
   ],
   "id": "8ade87e75e8b035"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.abae.dataset import PositiveNegativeABAEDataset\n",
    "from main.abae.evaluation import ABAEEvaluationProcessor\n",
    "from main.abae.model_manager import ABAEManagerConfig, ABAEManager\n",
    "import pandas as pd\n",
    "\n",
    "corpus_path = \"../dataset/output/pos_tagged/pre_processed.310k.noun_only.csv\"\n",
    "corpus = pd.read_csv(corpus_path)\n",
    "\n",
    "split_dataset = np.array_split(corpus, 4)\n",
    "\n",
    "validation_split = split_dataset[0]  # On what to compute the validation metrics 25% of ds for validation\n",
    "train = pd.concat([split_dataset[index] for index in range(len(split_dataset)) if index != 0])\n",
    "\n",
    "results = []\n",
    "n_folds = 5\n",
    "# This script can be re-run as often as desired as the history is persisted\n",
    "for i in range(1):  # How many different configurations we want to see\n",
    "    config = next(config_gen)\n",
    "    run_id = uuid4()\n",
    "\n",
    "    print(f\"Running configuration = {config} ({i + 1}/10)\")\n",
    "    run_result = dict(config=config, cv_coh=[], npmi_coh=[], max_margin_loss=[])\n",
    "    abae_config = ABAEManagerConfig.from_configuration(f\"{run_id}\", config)\n",
    "    abae_manager = ABAEManager.from_scratch(abae_config, train, override=True)\n",
    "\n",
    "    # Now we train:\n",
    "    abae_manager.train(train)\n",
    "\n",
    "    # Now for evaluation\n",
    "    # Max margin loss:\n",
    "    vocabulary = abae_manager.generator.emb_model.vocabulary()\n",
    "    max_seq_len = abae_config.max_seq_len\n",
    "    negative_sample_size = abae_config.negative_sample_size\n",
    "    eval_ds = PositiveNegativeABAEDataset(validation_split, vocabulary, max_seq_len, negative_sample_size)\n",
    "\n",
    "    model = abae_manager.get_compiled_model(refresh=False)\n",
    "    run_result['max_margin_loss'] = model.evaluate(DataLoader(eval_ds, batch_size=abae_config.batch_size))\n",
    "\n",
    "    # Coherence metrics:\n",
    "    processor = ABAEEvaluationProcessor(abae_manager, validation_split)\n",
    "    run_result['silhouette_score'] = processor.silhouette_score()\n",
    "    validation = validation_split['comments'].apply(lambda x: x.split(' '))\n",
    "\n",
    "    for top in [3, 10, 25]:\n",
    "        cv_coh = processor.c_v_coherence_model(top_n=100, ds=validation)\n",
    "        npmi_coh = processor.c_npmi_coherence_model(top_n=top, ds=validation)\n",
    "        run_result['cv_coh'].append({top: cv_coh.get_coherence()})\n",
    "        run_result['npmi_coh'].append({top: npmi_coh.get_coherence()})\n"
   ],
   "id": "e8e684f9df523167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's analyze the results:",
   "id": "2f8135fd1eadbfc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "35ac0910145c55d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
