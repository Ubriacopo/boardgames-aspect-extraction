{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:34.790196Z",
     "start_time": "2025-03-03T16:42:34.787135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will still do ABAE. I made wrong assumptions. I need a test set -> Coherence cant be my only reference metric.\n",
    "# Re-run on restaurant to see if I did good."
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:01.544380Z",
     "start_time": "2025-03-03T16:42:55.603276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from main.abae.dataset import PositiveNegativeABAEDataset\n",
    "from main.abae.model_manager import ABAEManagerConfig, ABAEDefaultManagerFactory\n",
    "import pandas as pd\n",
    "\n",
    "# First attempt config.\n",
    "corpus = \"../../output/ds/pre_processed.80k.csv\"\n",
    "\n",
    "config = ABAEManagerConfig(model_name=\"abae\", corpus_file_path=corpus)\n",
    "dataset = pd.read_csv(config.corpus_file_path)"
   ],
   "id": "c10bc1217fb36896",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on: First attempt",
   "id": "5b4c80db434e5d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:02.122729Z",
     "start_time": "2025-03-03T16:43:01.551433Z"
    }
   },
   "cell_type": "code",
   "source": "abae_manager = ABAEDefaultManagerFactory().factory_method(config)",
   "id": "5a69bfe8f6898c5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/91186 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e58ed0e1d1b240b1898a64969506457d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from output\\abae\\abae.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'output\\\\abae\\\\abae.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from output\\abae\\abae.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'output\\\\abae\\\\abae.embeddings.model', 'datetime': '2025-03-03T17:43:01.937665', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n",
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the existing found model as requested in path ./output/abae/abae.aspect_embeddings.model\n",
      "Creating new model\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:36.132799Z",
     "start_time": "2025-03-03T16:42:35.262132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = PositiveNegativeABAEDataset(\n",
    "    dataset=dataset,\n",
    "    vocabulary=abae_manager.generator.emb_model.vocabulary(),\n",
    "    negative_size=config.negative_sample_size,\n",
    "    max_seq_length=config.max_seq_len\n",
    ")"
   ],
   "id": "c9a99e3bc5f1932e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/91186 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "769cce75d166436d8471906c765e6f44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__del__'\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mPositiveNegativeABAEDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mabae_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memb_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocabulary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnegative_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnegative_sample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_seq_len\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\main\\abae\\dataset.py:44\u001B[0m, in \u001B[0;36mPositiveNegativeABAEDataset.__init__\u001B[1;34m(self, dataset, vocabulary, max_seq_length, negative_size, use_lowest_pad)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m, vocabulary: \u001B[38;5;28mdict\u001B[39m, max_seq_length: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     42\u001B[0m              negative_size: \u001B[38;5;28mint\u001B[39m, use_lowest_pad: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnegative_size \u001B[38;5;241m=\u001B[39m negative_size\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_lowest_pad\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\main\\abae\\dataset.py:18\u001B[0m, in \u001B[0;36mABAEDataset.__init__\u001B[1;34m(self, dataset, vocabulary, max_seq_length, use_lowest_pad)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# If this flag is true the padding will go to the min between (max_seq_length, max(s)) where\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# max(s) is the longest found string (divided in words) we found in our documents.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_lowest_pad \u001B[38;5;241m=\u001B[39m use_lowest_pad\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\main\\dataset\\dataset.py:17\u001B[0m, in \u001B[0;36mBaseBoardgameDataset.__init__\u001B[1;34m(self, dataset, vocabulary)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocabulary \u001B[38;5;241m=\u001B[39m vocabulary\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset: DataFrame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\main\\abae\\dataset.py:28\u001B[0m, in \u001B[0;36mABAEDataset.process_dataset\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m     25\u001B[0m padding_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length\n\u001B[0;32m     27\u001B[0m max_found_length \u001B[38;5;241m=\u001B[39m temp_ds\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(x))\u001B[38;5;241m.\u001B[39mmax()\n\u001B[1;32m---> 28\u001B[0m with_lost_information \u001B[38;5;241m=\u001B[39m \u001B[43mtemp_ds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     30\u001B[0m with_lost_information \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe loose information on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwith_lost_information\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwith_lost_information\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(temp_ds)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m% of ds).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_lowest_pad:\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4700\u001B[0m, in \u001B[0;36mSeries.map\u001B[1;34m(self, arg, na_action)\u001B[0m\n\u001B[0;32m   4620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\n\u001B[0;32m   4621\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4622\u001B[0m     arg: Callable \u001B[38;5;241m|\u001B[39m Mapping \u001B[38;5;241m|\u001B[39m Series,\n\u001B[0;32m   4623\u001B[0m     na_action: Literal[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   4624\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[0;32m   4625\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4626\u001B[0m \u001B[38;5;124;03m    Map values of Series according to an input mapping or function.\u001B[39;00m\n\u001B[0;32m   4627\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4698\u001B[0m \u001B[38;5;124;03m    dtype: object\u001B[39;00m\n\u001B[0;32m   4699\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4700\u001B[0m     new_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_values, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39m__finalize__(\n\u001B[0;32m   4702\u001B[0m         \u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4703\u001B[0m     )\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2981\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mlib.pyx:2543\u001B[0m, in \u001B[0;36mpandas._libs.lib.maybe_convert_objects\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\numpy\\core\\numeric.py:330\u001B[0m, in \u001B[0;36mfull\u001B[1;34m(shape, fill_value, dtype, order, like)\u001B[0m\n\u001B[0;32m    328\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m fill_value\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m    329\u001B[0m a \u001B[38;5;241m=\u001B[39m empty(shape, dtype, order)\n\u001B[1;32m--> 330\u001B[0m \u001B[43mmultiarray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopyto\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43munsafe\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m a\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "abae_manager.train(ds)  # 91k records",
   "id": "7f762e580d8ba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m713/713\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m206s\u001B[0m 288ms/step - loss: 5.2341 - max_margin_loss: 5.2330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.src.callbacks.history.History at 0x1550da6ac30>,\n",
       " <Functional name=functional_3, built=True>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:07.067889Z",
     "start_time": "2025-03-03T16:43:06.856495Z"
    }
   },
   "cell_type": "code",
   "source": "iteration_model = abae_manager.get_compiled_model()",
   "id": "f728c26f9857e51f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:08.032151Z",
     "start_time": "2025-03-03T16:43:08.025638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from evaluation import ABAEEvaluationProcessor\n",
    "\n",
    "inv_vocab = abae_manager.generator.emb_model.model.wv.index_to_key\n",
    "processor = ABAEEvaluationProcessor.generate_for_model(iteration_model, inv_vocab)\n",
    "# We can now call and build a coherence evaluator, or we can do one by hand"
   ],
   "id": "fe4452f8e985d601",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:36.160708200Z",
     "start_time": "2025-03-03T16:37:11.599408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "coherence_model = processor.c_npmi_coherence_model(top_n=10, ds=dataset['comments'].apply(lambda x: x.split(' ')))\n",
    "\n",
    "coherence_model.get_coherence()"
   ],
   "id": "b6bd30ff7be772ee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary<7658 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary<10855 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary<13210 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary<15218 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary<16894 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary<18444 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary<19869 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary<21149 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary<22281 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<22448 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 91186 documents (total 704536 corpus positions)\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Dictionary\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<22448 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 91186 documents (total 704536 corpus positions)\", 'datetime': '2025-03-03T17:37:12.329203', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator<processes=15, batch_size=64> to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:30 batches submitted to accumulate stats from 1920 documents (-2824 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:42 batches submitted to accumulate stats from 2688 documents (-3800 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:74 batches submitted to accumulate stats from 4736 documents (-6466 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:77 batches submitted to accumulate stats from 4928 documents (-6664 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:149 batches submitted to accumulate stats from 9536 documents (-12402 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:152 batches submitted to accumulate stats from 9728 documents (-12520 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:157 batches submitted to accumulate stats from 10048 documents (-12797 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:180 batches submitted to accumulate stats from 11520 documents (-14690 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:192 batches submitted to accumulate stats from 12288 documents (-15473 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:194 batches submitted to accumulate stats from 12416 documents (-15585 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:197 batches submitted to accumulate stats from 12608 documents (-15783 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:272 batches submitted to accumulate stats from 17408 documents (-22780 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:273 batches submitted to accumulate stats from 17472 documents (-22682 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:294 batches submitted to accumulate stats from 18816 documents (-24495 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:311 batches submitted to accumulate stats from 19904 documents (-25910 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:351 batches submitted to accumulate stats from 22464 documents (-29143 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:363 batches submitted to accumulate stats from 23232 documents (-29874 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:367 batches submitted to accumulate stats from 23488 documents (-29930 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:370 batches submitted to accumulate stats from 23680 documents (-30115 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:392 batches submitted to accumulate stats from 25088 documents (-31836 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:407 batches submitted to accumulate stats from 26048 documents (-32656 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:423 batches submitted to accumulate stats from 27072 documents (-33679 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:480 batches submitted to accumulate stats from 30720 documents (-39080 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:491 batches submitted to accumulate stats from 31424 documents (-39697 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:494 batches submitted to accumulate stats from 31616 documents (-39795 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:502 batches submitted to accumulate stats from 32128 documents (-40447 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:522 batches submitted to accumulate stats from 33408 documents (-42144 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:547 batches submitted to accumulate stats from 35008 documents (-44101 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:556 batches submitted to accumulate stats from 35584 documents (-44695 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:558 batches submitted to accumulate stats from 35712 documents (-44732 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:577 batches submitted to accumulate stats from 36928 documents (-46148 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:590 batches submitted to accumulate stats from 37760 documents (-47263 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:598 batches submitted to accumulate stats from 38272 documents (-47908 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:611 batches submitted to accumulate stats from 39104 documents (-48860 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:618 batches submitted to accumulate stats from 39552 documents (-49263 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:689 batches submitted to accumulate stats from 44096 documents (-56323 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:761 batches submitted to accumulate stats from 48704 documents (-62669 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:788 batches submitted to accumulate stats from 50432 documents (-64819 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:800 batches submitted to accumulate stats from 51200 documents (-65640 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:815 batches submitted to accumulate stats from 52160 documents (-66605 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:870 batches submitted to accumulate stats from 55680 documents (-71930 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:881 batches submitted to accumulate stats from 56384 documents (-72828 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:911 batches submitted to accumulate stats from 58304 documents (-75254 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:945 batches submitted to accumulate stats from 60480 documents (-77342 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:956 batches submitted to accumulate stats from 61184 documents (-78174 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:961 batches submitted to accumulate stats from 61504 documents (-78476 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:970 batches submitted to accumulate stats from 62080 documents (-78955 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:985 batches submitted to accumulate stats from 63040 documents (-80312 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1013 batches submitted to accumulate stats from 64832 documents (-82650 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1037 batches submitted to accumulate stats from 66368 documents (-84526 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1047 batches submitted to accumulate stats from 67008 documents (-85659 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1103 batches submitted to accumulate stats from 70592 documents (-90650 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1155 batches submitted to accumulate stats from 73920 documents (-94629 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1219 batches submitted to accumulate stats from 78016 documents (-100035 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1221 batches submitted to accumulate stats from 78144 documents (-100057 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1224 batches submitted to accumulate stats from 78336 documents (-100171 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1283 batches submitted to accumulate stats from 82112 documents (-105833 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1321 batches submitted to accumulate stats from 84544 documents (-108586 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1325 batches submitted to accumulate stats from 84800 documents (-108847 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1363 batches submitted to accumulate stats from 87232 documents (-111990 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1364 batches submitted to accumulate stats from 87296 documents (-111955 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1365 batches submitted to accumulate stats from 87360 documents (-111934 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1380 batches submitted to accumulate stats from 88320 documents (-113018 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1381 batches submitted to accumulate stats from 88384 documents (-112996 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1406 batches submitted to accumulate stats from 89984 documents (-115133 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1407 batches submitted to accumulate stats from 90048 documents (-114947 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1410 batches submitted to accumulate stats from 90240 documents (-115099 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:15 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 166930 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.20050169543561938"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:36.161734700Z",
     "start_time": "2025-03-03T16:37:30.567009Z"
    }
   },
   "cell_type": "code",
   "source": "coherence_model.get_coherence_per_topic()",
   "id": "8351b5d4ca41a5a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1298328076991827,\n",
       " -0.3099134062514218,\n",
       " -0.1410714048773639,\n",
       " -0.4294875341228398,\n",
       " -0.18663794390456034,\n",
       " -0.3611803321881761,\n",
       " -0.40561051166438694,\n",
       " -0.06539005266596187,\n",
       " -0.3642236789799106,\n",
       " -0.12881660694144323,\n",
       " 0.14022147463393575,\n",
       " -0.07558067746964896,\n",
       " -0.22434788976145623,\n",
       " -0.12515236420625445]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:36.161734700Z",
     "start_time": "2025-03-03T16:36:47.016410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "coh = processor.u_mass_coherence_model(top_n=100, ds=dataset['comments'].apply(lambda x: x.split(' ')))\n",
    "coh.get_coherence()"
   ],
   "id": "692ee359262ba376",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 91000 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17.4115041503226"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:42:36.162734700Z",
     "start_time": "2025-03-03T16:20:06.747947Z"
    }
   },
   "cell_type": "code",
   "source": "coh.get_coherence_per_topic()",
   "id": "539f43f762cb58ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.25131441525433473,\n",
       " -19.542568970596935,\n",
       " -0.8987464202409412,\n",
       " -20.444470965018994,\n",
       " -3.5804277357546592,\n",
       " -20.017026950192054,\n",
       " -19.04357780447795,\n",
       " -4.339466989859753,\n",
       " -21.5622225938978,\n",
       " -20.554169882275417,\n",
       " -3.8358616383835153,\n",
       " -5.533389397541525,\n",
       " -3.6526542464332996,\n",
       " -0.25131441525433473]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:33.171167Z",
     "start_time": "2025-03-03T16:43:16.719159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Good human judgement is C_V\n",
    "coh = processor.c_v_coherence_model(top_n=100, ds=dataset['comments'].apply(lambda x: x.split(' ')))\n",
    "coh.get_coherence()"
   ],
   "id": "83cdabcc33d147b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary<7658 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary<10855 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary<13210 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary<15218 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary<16894 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary<18444 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary<19869 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary<21149 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary<22281 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<22448 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 91186 documents (total 704536 corpus positions)\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Dictionary\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<22448 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 91186 documents (total 704536 corpus positions)\", 'datetime': '2025-03-03T17:43:17.490119', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator<processes=15, batch_size=64> to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:15 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 91373 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46951325597595467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A first score of 0.47 is not too bad considering we trained on a subset of data. <br>\n",
    "It's also worth noting that by adjusting some hp we might boost this.\n",
    "\n",
    "For C_V a good range of values, as for a rule of thumb, is 0.5-0.7"
   ],
   "id": "f4c2f7e213283256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T16:43:33.466218Z",
     "start_time": "2025-03-03T16:43:33.195198Z"
    }
   },
   "cell_type": "code",
   "source": "coh.get_coherence_per_topic()",
   "id": "81eebfd8de9f5e51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6734846093622886,\n",
       " 0.3842562311689465,\n",
       " 0.5729853802703821,\n",
       " 0.5299957364201994,\n",
       " 0.3558408256143046,\n",
       " 0.5463975198242397,\n",
       " 0.6588644232851955,\n",
       " 0.3928530874981103,\n",
       " 0.3281340157121037,\n",
       " 0.6223650022085558,\n",
       " 0.2825007401357925,\n",
       " 0.38961207232890294,\n",
       " 0.46663232916056874,\n",
       " 0.36926361067377544]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
