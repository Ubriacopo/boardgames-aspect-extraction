{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What hyperparameters to tune:\n",
    "Take a look here:\n",
    "> https://stats.stackexchange.com/questions/349761/reasonable-hyperparameter-range-for-latent-dirichlet-allocation\n",
    "\n",
    "So our go to are:\n",
    "- Topics number\n",
    "- alpha: Document-Topic Density\n",
    "- beta: Word-Topic Density\n"
   ],
   "id": "292963eb46a0bb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parameters definition",
   "id": "3df2fb11f81389c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter\n",
    "\n",
    "seed = 1408\n",
    "config_generator = UniqueParametersConfigFsGenerator(100, \"./output/config\", \"seen_configurations.noun_only.json\")\n",
    "\n",
    "# The amount of topics we want to look for\n",
    "# Topics are by far the most relevant parameter to tune, alpha and beta will come later maybe:\n",
    "config_generator.add_parameter('topics', RandomTunableOffsetParameter(value_range=(7, 72), step=5, seed=seed))\n",
    "# config_generator.add_parameter('alpha', RandomTunableOffsetParameter(value_range=(0.005, 1.0), step=0.5, seed=seed))\n",
    "# config_generator.add_parameter('beta', RandomTunableDiscreteParameter(values_list=beta, seed=seed))"
   ],
   "id": "c3b4f253e6a7bc89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hp_tuning import LDATuningProcedure\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/pos_tagged/pre_processed.310k.noun_only.csv\")\n",
    "procedure = LDATuningProcedure(generator=config_generator, top=[3, 10, 25], folds=5)\n",
    "procedure.run(corpus, 10, stop_words)  # Try 10 different configurations.\n",
    "procedure.store_results(\"./output/config/hp_tuning_results.noun_only.json\")"
   ],
   "id": "d2f265db6af3cae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "74343a360c102020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter\n",
    "\n",
    "seed = 1408\n",
    "config_generator = UniqueParametersConfigFsGenerator(\n",
    "    100, \"./output/config\", \"seen_configurations.noun_only_sentence.json\"\n",
    ")\n",
    "\n",
    "# The amount of topics we want to look for\n",
    "# Topics are by far the most relevant parameter to tune, alpha and beta will come later maybe:\n",
    "config_generator.add_parameter('topics', RandomTunableOffsetParameter(value_range=(7, 72), step=5, seed=seed))\n",
    "# config_generator.add_parameter('alpha', RandomTunableOffsetParameter(value_range=(0.005, 1.0), step=0.5, seed=seed))\n",
    "# config_generator.add_parameter('beta', RandomTunableDiscreteParameter(values_list=beta, seed=seed))"
   ],
   "id": "620bb258fd22a9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hp_tuning import LDATuningProcedure\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/pos_tagged_sentence_level/pre_processed.310k.noun_only.csv\")\n",
    "procedure = LDATuningProcedure(generator=config_generator, top=[3, 10, 25], folds=5)\n",
    "procedure.run(corpus, 10, stop_words)  # Try 10 different configurations.\n",
    "procedure.store_results(\"./output/config/hp_tuning_results.noun_only_sent.json\")"
   ],
   "id": "19cc6f2b1ffe2f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a50567b67cf8f6ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter\n",
    "\n",
    "seed = 1408\n",
    "config_generator = UniqueParametersConfigFsGenerator(\n",
    "    100, \"./output/config\", \"seen_configurations.default_sentence.json\"\n",
    ")\n",
    "\n",
    "# The amount of topics we want to look for\n",
    "# Topics are by far the most relevant parameter to tune, alpha and beta will come later maybe:\n",
    "config_generator.add_parameter('topics', RandomTunableOffsetParameter(value_range=(7, 72), step=5, seed=seed))\n",
    "# config_generator.add_parameter('alpha', RandomTunableOffsetParameter(value_range=(0.005, 1.0), step=0.5, seed=seed))\n",
    "# config_generator.add_parameter('beta', RandomTunableDiscreteParameter(values_list=beta, seed=seed))"
   ],
   "id": "9eee3cb9f5e02092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hp_tuning import LDATuningProcedure\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/default_sentences/pre_processed.310k.csv\")\n",
    "procedure = LDATuningProcedure(generator=config_generator, top=[3, 10, 25], folds=5)\n",
    "results = procedure.run(corpus, 10, stop_words)  # Try 10 different configurations.\n",
    "procedure.store_results(\"./output/config/hp_tuning_results.default_sentence.json\")"
   ],
   "id": "6fe059e90ca9e30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize the results:",
   "id": "efa4c8a5675b3fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(json.load(open(\"./output/config/hp_tuning_results.default_sentence.json\")))\n",
    "data['topics'] = data['config'].map(lambda o: o['topics'])\n",
    "data['perplexity'] = data['perplexity'].map(lambda x: np.mean(x))\n",
    "for i in [3, 10, 25]:\n",
    "    data[f'{i}_npmi_coh'] = data['npmi_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "    data[f'{i}_cv_coh'] = data['cv_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "# Refined data\n",
    "data = data.drop(columns=['config', 'npmi_coh', 'cv_coh'])"
   ],
   "id": "70a7f6597fee5433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "data = data.sort_values(by=\"topics\")\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_cv_coh'], mode='lines', name='top-3'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_cv_coh'], mode='lines', name='top-10'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_cv_coh'], mode='lines', name='top-25'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_npmi_coh'], mode='lines', name='top-3', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_npmi_coh'], mode='lines', name='top-10', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_npmi_coh'], mode='lines', name='top-25', line=dict(dash='dash')))\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(\n",
    "    title=dict(text='Average CV coherence over k-folds per model (K different) - Default sentences'),\n",
    "    xaxis=dict(title=dict(text='Model topics K')),\n",
    "    yaxis=dict(title=dict(text='CV coherence')),\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Average perplexity for the current dataset LDA approach is of: {data[['perplexity', 'topics']]}\")"
   ],
   "id": "774103175a8a2d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.DataFrame(json.load(open(\"./output/config/hp_tuning_results.noun_only.json\")))\n",
    "data['topics'] = data['config'].map(lambda o: o['topics'])\n",
    "data['perplexity'] = data['perplexity'].map(lambda x: np.mean(x))\n",
    "for i in [3, 10, 25]:\n",
    "    data[f'{i}_npmi_coh'] = data['npmi_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "    data[f'{i}_cv_coh'] = data['cv_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "# Refined data\n",
    "data = data.drop(columns=['config', 'npmi_coh', 'cv_coh'])"
   ],
   "id": "9912de62ec696e26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "data = data.sort_values(by=\"topics\")\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_cv_coh'], mode='lines', name='top-3'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_cv_coh'], mode='lines', name='top-10'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_cv_coh'], mode='lines', name='top-25'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_npmi_coh'], mode='lines', name='top-3', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_npmi_coh'], mode='lines', name='top-10', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_npmi_coh'], mode='lines', name='top-25', line=dict(dash='dash')))\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(\n",
    "    title=dict(text='Average CV coherence over k-folds per model (K different) - Noun Only'),\n",
    "    xaxis=dict(title=dict(text='Model topics K')),\n",
    "    yaxis=dict(title=dict(text='CV coherence')),\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Average perplexity for the current dataset LDA approach is of: {data[['perplexity', 'topics']]}\")"
   ],
   "id": "4b4310492832e9cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This tuning shows a degradation of coherence with the increase of the topics. <br>\n",
    "Let's try reducing the range to the values in range [7,20]"
   ],
   "id": "1f816b541d7851ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.DataFrame(json.load(open(\"./output/config/hp_tuning_results.noun_only_sent.json\")))\n",
    "data['topics'] = data['config'].map(lambda o: o['topics'])\n",
    "data['perplexity'] = data['perplexity'].map(lambda x: np.mean(x))\n",
    "for i in [3, 10, 25]:\n",
    "    data[f'{i}_npmi_coh'] = data['npmi_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "    data[f'{i}_cv_coh'] = data['cv_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "# Refined data\n",
    "data = data.drop(columns=['config', 'npmi_coh', 'cv_coh'])"
   ],
   "id": "2cb0f4505b9746bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "data = data.sort_values(by=\"topics\")\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_cv_coh'], mode='lines', name='top-3'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_cv_coh'], mode='lines', name='top-10'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_cv_coh'], mode='lines', name='top-25'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_npmi_coh'], mode='lines', name='top-3', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_npmi_coh'], mode='lines', name='top-10', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_npmi_coh'], mode='lines', name='top-25', line=dict(dash='dash')))\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(\n",
    "    title=dict(text='Average CV coherence over k-folds per model (K different) - Noun sentences'),\n",
    "    xaxis=dict(title=dict(text='Model topics K')),\n",
    "    yaxis=dict(title=dict(text='CV coherence')),\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Average perplexity for the current dataset LDA approach is of: {data[['perplexity', 'topics']]}\")"
   ],
   "id": "d645401c4c6fe7d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nouns only on sentences perform worst for our metrics and are rather inconsistent",
   "id": "5de2cc947f6734b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# On the three datasets seen we just opt to get deeper with the ones:\n",
    "# - hp_tuning_results.noun_only.json (Most robust in every metric)\n",
    "# - hp_tuning_results.default_sentence.json (Even if this one seems less promising)"
   ],
   "id": "46841324fb1af619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parameter redefinition",
   "id": "64d2884b825b9d97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter\n",
    "\n",
    "seed = 1408\n",
    "config_generator = UniqueParametersConfigFsGenerator(100, \"./output/config\", \"seen_configurations.noun_only.json\")\n",
    "\n",
    "# The amount of topics we want to look for\n",
    "# Topics are by far the most relevant parameter to tune, alpha and beta will come later maybe:\n",
    "config_generator.add_parameter('topics', RandomTunableOffsetParameter(value_range=(7, 20), step=1, seed=seed))\n",
    "# config_generator.add_parameter('alpha', RandomTunableOffsetParameter(value_range=(0.005, 1.0), step=0.5, seed=seed))\n",
    "# config_generator.add_parameter('beta', RandomTunableDiscreteParameter(values_list=beta, seed=seed))"
   ],
   "id": "d755cd3a8be7065f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hp_tuning import LDATuningProcedure\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/pos_tagged/pre_processed.310k.noun_only.csv\")\n",
    "procedure = LDATuningProcedure(generator=config_generator, top=[3, 10, 25], folds=5)\n",
    "results = procedure.run(corpus, 10, stop_words)  # Try 10 different configurations.\n",
    "procedure.store_results(\"./output/config/hp_tuning_results.noun_only.json\")"
   ],
   "id": "547a8d4e1843892a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results",
   "id": "35a907669032adc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c5c22f974d0dd2f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from main.hp_tuning import UniqueParametersConfigFsGenerator, RandomTunableOffsetParameter\n",
    "\n",
    "seed = 1408\n",
    "config_generator = UniqueParametersConfigFsGenerator(\n",
    "    100, \"./output/config\", \"seen_configurations.default_sentence.json\"\n",
    ")\n",
    "\n",
    "# The amount of topics we want to look for\n",
    "# Topics are by far the most relevant parameter to tune, alpha and beta will come later maybe:\n",
    "config_generator.add_parameter('topics', RandomTunableOffsetParameter(value_range=(7, 20), step=1, seed=seed))\n",
    "# config_generator.add_parameter('alpha', RandomTunableOffsetParameter(value_range=(0.005, 1.0), step=0.5, seed=seed))\n",
    "# config_generator.add_parameter('beta', RandomTunableDiscreteParameter(values_list=beta, seed=seed))"
   ],
   "id": "2be7b3f0e7eabc05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hp_tuning import LDATuningProcedure\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/default_sentences/pre_processed.310k.csv\")\n",
    "procedure = LDATuningProcedure(generator=config_generator, top=[3, 10, 25], folds=5)\n",
    "results = procedure.run(corpus, 10, stop_words)  # Try 10 different configurations.\n",
    "procedure.store_results(\"./output/config/hp_tuning_results.default_sentence.json\")"
   ],
   "id": "e741fae8caef3985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Results graphs:",
   "id": "41c3eb4add43a238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "data = pd.DataFrame(json.load(open(\"./output/config/hp_tuning_results.noun_only.json\")))\n",
    "data['topics'] = data['config'].map(lambda o: o['topics'])\n",
    "data['perplexity'] = data['perplexity'].map(lambda x: np.mean(x))\n",
    "for i in [3, 10, 25]:\n",
    "    data[f'{i}_npmi_coh'] = data['npmi_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "    data[f'{i}_cv_coh'] = data['cv_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "# Refined data\n",
    "data = data.drop(columns=['config', 'npmi_coh', 'cv_coh'])"
   ],
   "id": "ae12af0d6713e55b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "data = data.sort_values(by=\"topics\")\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_cv_coh'], mode='lines', name='top-3'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_cv_coh'], mode='lines', name='top-10'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_cv_coh'], mode='lines', name='top-25'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_npmi_coh'], mode='lines', name='top-3', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_npmi_coh'], mode='lines', name='top-10', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_npmi_coh'], mode='lines', name='top-25', line=dict(dash='dash')))\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(\n",
    "    title=dict(text='Average CV coherence over k-folds per model (K different)'),\n",
    "    xaxis=dict(title=dict(text='Model topics K')),\n",
    "    yaxis=dict(title=dict(text='CV coherence')),\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Average perplexity for the current dataset LDA approach is of: {data[['perplexity', 'topics']]}\")"
   ],
   "id": "c131e0d1892a60c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(json.load(open(\"./output/config/hp_tuning_results.default_sentence.json\")))\n",
    "data['topics'] = data['config'].map(lambda o: o['topics'])\n",
    "data['perplexity'] = data['perplexity'].map(lambda x: np.mean(x))\n",
    "for i in [3, 10, 25]:\n",
    "    data[f'{i}_npmi_coh'] = data['npmi_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "    data[f'{i}_cv_coh'] = data['cv_coh'].map(lambda x: np.mean(x[str(i)]))\n",
    "# Refined data\n",
    "data = data.drop(columns=['config', 'npmi_coh', 'cv_coh'])"
   ],
   "id": "a43627589b442af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the given searched configurations the best settings seems to be K=11",
   "id": "78f58cf8be2bb152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "data = data.sort_values(by=\"topics\")\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_cv_coh'], mode='lines', name='top-3'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_cv_coh'], mode='lines', name='top-10'))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_cv_coh'], mode='lines', name='top-25'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['3_npmi_coh'], mode='lines', name='top-3', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['10_npmi_coh'], mode='lines', name='top-10', line=dict(dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=data['topics'], y=data['25_npmi_coh'], mode='lines', name='top-25', line=dict(dash='dash')))\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(\n",
    "    title=dict(text='Average CV coherence over k-folds per model (K different)'),\n",
    "    xaxis=dict(title=dict(text='Model topics K')),\n",
    "    yaxis=dict(title=dict(text='CV coherence')),\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Average perplexity for the current dataset LDA approach is of: {data[['perplexity', 'topics']]}\")"
   ],
   "id": "cb319a153e560fab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ad403d5945dcfad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets try to tune alpha and beta",
   "id": "8db37a429aface9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model import LdaModelGenerator, LdaGeneratorConfig\n",
    "\n",
    "stop_words = ['game', 'play', '<game_name>']\n",
    "corpus = pd.read_csv(\"../dataset/output/pos_tagged/pre_processed.310k.noun_only.csv\")\n",
    "model, dictionary = LdaModelGenerator(LdaGeneratorConfig(\"k_tuned\", topics=11), stop_words).make_model(corpus)"
   ],
   "id": "bd2cde6838c1a1d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"../dataset/output/pos_tagged/pre_processed.310k.noun_only.test.csv\")\n",
    "texts = test_df['comments'].apply(lambda x: x.split(' '))\n",
    "\n",
    "i_results = dict(cv_coh={}, npmi_coh={})\n",
    "for top in [3, 5, 10, 15, 20, 25, 50]:\n",
    "    cv_coh = CoherenceModel(model, texts=texts, coherence='c_v', topn=top)\n",
    "    npmi_coh = CoherenceModel(model, texts=texts, coherence='c_npmi', topn=top)\n",
    "    i_results['cv_coh'][top] = cv_coh.get_coherence()\n",
    "    i_results['npmi_coh'][top] = npmi_coh.get_coherence()"
   ],
   "id": "de92840c8b52a75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "i_results",
   "id": "a7f1ea952d3063ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T00:02:07.911138Z",
     "start_time": "2025-03-13T00:02:07.903690Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_topics(num_words=30)",
   "id": "29bc7b17af5352bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"action\" + 0.033*\"point\" + 0.028*\"worker\" + 0.027*\"placement\" + 0.024*\"card\" + 0.024*\"resource\" + 0.023*\"turn\" + 0.021*\"building\" + 0.015*\"round\" + 0.013*\"scoring\" + 0.012*\"board\" + 0.012*\"way\" + 0.011*\"end\" + 0.010*\"order\" + 0.010*\"victory\" + 0.010*\"lot\" + 0.010*\"mechanism\" + 0.009*\"thing\" + 0.009*\"bonus\" + 0.009*\"tile\" + 0.009*\"mechanic\" + 0.009*\"area\" + 0.009*\"space\" + 0.008*\"money\" + 0.008*\"engine\" + 0.008*\"opponent\" + 0.008*\"player\" + 0.008*\"cube\" + 0.007*\"management\" + 0.007*\"selection\"'),\n",
       " (1,\n",
       "  '0.205*\"card\" + 0.036*\"luck\" + 0.022*\"character\" + 0.020*\"player\" + 0.018*\"hand\" + 0.016*\"minute\" + 0.014*\"filler\" + 0.014*\"decision\" + 0.013*\"ability\" + 0.012*\"set\" + 0.012*\"draw\" + 0.012*\"turn\" + 0.011*\"time\" + 0.010*\"deck\" + 0.009*\"round\" + 0.009*\"opponent\" + 0.009*\"way\" + 0.008*\"randomness\" + 0.007*\"mission\" + 0.007*\"event\" + 0.007*\"effect\" + 0.006*\"power\" + 0.006*\"number\" + 0.006*\"choice\" + 0.006*\"hero\" + 0.006*\"drafting\" + 0.005*\"end\" + 0.005*\"mechanic\" + 0.005*\"skill\" + 0.005*\"dice\"'),\n",
       " (2,\n",
       "  '0.091*\"rule\" + 0.026*\"system\" + 0.026*\"scenario\" + 0.025*\"hour\" + 0.022*\"time\" + 0.017*\"player\" + 0.016*\"war\" + 0.014*\"combat\" + 0.013*\"battle\" + 0.013*\"rulebook\" + 0.013*\"campaign\" + 0.011*\"mechanic\" + 0.011*\"wargame\" + 0.011*\"book\" + 0.009*\"experience\" + 0.008*\"way\" + 0.008*\"map\" + 0.008*\"unit\" + 0.008*\"theme\" + 0.008*\"lot\" + 0.007*\"thing\" + 0.007*\"bit\" + 0.006*\"video\" + 0.006*\"miniature\" + 0.006*\"gameplay\" + 0.006*\"board\" + 0.005*\"faction\" + 0.005*\"history\" + 0.005*\"complexity\" + 0.004*\"army\"'),\n",
       " (3,\n",
       "  '0.094*\"dice\" + 0.053*\"puzzle\" + 0.027*\"roll\" + 0.021*\"thing\" + 0.019*\"story\" + 0.019*\"point\" + 0.017*\"time\" + 0.016*\"theme\" + 0.016*\"bit\" + 0.015*\"series\" + 0.015*\"euro\" + 0.014*\"way\" + 0.014*\"lot\" + 0.013*\"fun\" + 0.013*\"room\" + 0.013*\"luck\" + 0.011*\"case\" + 0.009*\"mechanic\" + 0.009*\"decision\" + 0.009*\"experience\" + 0.008*\"part\" + 0.007*\"end\" + 0.007*\"risk\" + 0.006*\"die\" + 0.006*\"adventure\" + 0.006*\"number\" + 0.005*\"idea\" + 0.005*\"choice\" + 0.005*\"element\" + 0.005*\"escape\"'),\n",
       " (4,\n",
       "  '0.131*\"lot\" + 0.099*\"expansion\" + 0.049*\"fun\" + 0.031*\"bit\" + 0.030*\"base\" + 0.017*\"dungeon\" + 0.015*\"monster\" + 0.015*\"replayability\" + 0.015*\"rule\" + 0.014*\"variety\" + 0.013*\"thing\" + 0.012*\"character\" + 0.012*\"choice\" + 0.012*\"time\" + 0.011*\"mechanic\" + 0.011*\"theme\" + 0.010*\"depth\" + 0.008*\"card\" + 0.008*\"fiddly\" + 0.008*\"decision\" + 0.007*\"mini\" + 0.007*\"board\" + 0.007*\"option\" + 0.007*\"power\" + 0.007*\"gameplay\" + 0.006*\"way\" + 0.006*\"set\" + 0.006*\"module\" + 0.006*\"adventure\" + 0.006*\"crawler\"'),\n",
       " (5,\n",
       "  '0.121*\"fun\" + 0.058*\"people\" + 0.048*\"group\" + 0.043*\"family\" + 0.030*\"kid\" + 0.026*\"friend\" + 0.023*\"time\" + 0.021*\"party\" + 0.016*\"gaming\" + 0.015*\"experience\" + 0.013*\"brain\" + 0.013*\"night\" + 0.013*\"lot\" + 0.012*\"bit\" + 0.010*\"filler\" + 0.010*\"board\" + 0.008*\"adult\" + 0.008*\"strategy\" + 0.008*\"light\" + 0.007*\"blast\" + 0.007*\"day\" + 0.007*\"racing\" + 0.007*\"wife\" + 0.007*\"dexterity\" + 0.007*\"theme\" + 0.006*\"memory\" + 0.006*\"chess\" + 0.006*\"person\" + 0.006*\"favorite\" + 0.006*\"mechanic\"'),\n",
       " (6,\n",
       "  '0.272*\"player\" + 0.045*\"tile\" + 0.029*\"interaction\" + 0.020*\"board\" + 0.017*\"turn\" + 0.014*\"time\" + 0.013*\"bit\" + 0.010*\"count\" + 0.010*\"strategy\" + 0.010*\"point\" + 0.010*\"number\" + 0.009*\"downtime\" + 0.009*\"scoring\" + 0.008*\"way\" + 0.008*\"lot\" + 0.008*\"option\" + 0.007*\"company\" + 0.007*\"decision\" + 0.007*\"train\" + 0.006*\"score\" + 0.006*\"rule\" + 0.006*\"end\" + 0.006*\"round\" + 0.005*\"stock\" + 0.005*\"move\" + 0.005*\"route\" + 0.005*\"mechanic\" + 0.005*\"min\" + 0.005*\"thing\" + 0.004*\"variant\"'),\n",
       " (7,\n",
       "  '0.053*\"rating\" + 0.048*\"strategy\" + 0.044*\"time\" + 0.034*\"design\" + 0.019*\"way\" + 0.017*\"bit\" + 0.016*\"theme\" + 0.014*\"player\" + 0.013*\"mechanic\" + 0.012*\"auction\" + 0.010*\"luck\" + 0.009*\"experience\" + 0.009*\"problem\" + 0.009*\"bidding\" + 0.009*\"concept\" + 0.008*\"decision\" + 0.008*\"depth\" + 0.008*\"mechanism\" + 0.008*\"thing\" + 0.007*\"idea\" + 0.007*\"negotiation\" + 0.006*\"tactic\" + 0.006*\"lot\" + 0.006*\"point\" + 0.006*\"element\" + 0.006*\"winner\" + 0.006*\"collection\" + 0.006*\"people\" + 0.006*\"hour\" + 0.005*\"wife\"'),\n",
       " (8,\n",
       "  '0.065*\"version\" + 0.042*\"gamer\" + 0.030*\"control\" + 0.029*\"area\" + 0.026*\"co\" + 0.024*\"theme\" + 0.021*\"deduction\" + 0.021*\"artwork\" + 0.020*\"team\" + 0.019*\"role\" + 0.017*\"app\" + 0.017*\"mechanic\" + 0.016*\"word\" + 0.016*\"op\" + 0.015*\"level\" + 0.012*\"replay\" + 0.011*\"value\" + 0.011*\"bit\" + 0.011*\"clue\" + 0.010*\"fun\" + 0.010*\"non\" + 0.010*\"time\" + 0.009*\"gameplay\" + 0.009*\"component\" + 0.009*\"story\" + 0.008*\"learning\" + 0.008*\"difficulty\" + 0.008*\"curve\" + 0.008*\"challenge\" + 0.007*\"board\"'),\n",
       " (9,\n",
       "  '0.070*\"time\" + 0.053*\"box\" + 0.036*\"year\" + 0.035*\"table\" + 0.030*\"component\" + 0.026*\"piece\" + 0.021*\"board\" + 0.020*\"quality\" + 0.019*\"copy\" + 0.014*\"setup\" + 0.011*\"production\" + 0.010*\"child\" + 0.009*\"space\" + 0.009*\"version\" + 0.008*\"price\" + 0.008*\"condition\" + 0.008*\"shelf\" + 0.008*\"trade\" + 0.008*\"rule\" + 0.007*\"couple\" + 0.007*\"faction\" + 0.007*\"gameplay\" + 0.007*\"son\" + 0.006*\"age\" + 0.006*\"day\" + 0.006*\"insert\" + 0.006*\"token\" + 0.006*\"bit\" + 0.005*\"art\" + 0.005*\"size\"'),\n",
       " (10,\n",
       "  '0.056*\"card\" + 0.055*\"theme\" + 0.047*\"deck\" + 0.046*\"solo\" + 0.044*\"art\" + 0.032*\"edition\" + 0.030*\"building\" + 0.020*\"gameplay\" + 0.020*\"builder\" + 0.020*\"mechanic\" + 0.018*\"mode\" + 0.017*\"trick\" + 0.016*\"bit\" + 0.014*\"component\" + 0.013*\"engine\" + 0.012*\"fan\" + 0.012*\"coop\" + 0.010*\"multiplayer\" + 0.010*\"solitaire\" + 0.009*\"variant\" + 0.009*\"artwork\" + 0.009*\"style\" + 0.008*\"twist\" + 0.007*\"experience\" + 0.006*\"way\" + 0.006*\"legacy\" + 0.006*\"mechanism\" + 0.006*\"rule\" + 0.006*\"improvement\" + 0.006*\"deckbuilder\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "91994c02c07db95a",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
