By referring to the paper we want to make our own implementation.

- First thing we will be needing is a way to communicate with bgg to extract the comments
- We will be needing for an embedding model as they measure relative distance

> Rather than using all available information, attention mechanism aims to focus
> on the most pertinent information for a task

We use attention

#### Final idea:
We make our own implementation of the ABAE 

## Note

> For the ABAE model, we initialize the word embedding matrix E with word vectors trained by
> word2vec with negative sampling on each dataset,setting the embedding size to 200, window size to
> 10, and negative sample size to 5

> https://stackoverflow.com/questions/64145666/fine-tuning-of-bert-word-embeddings

## Word2vec-like or BERT-like?

This question requires further research:
> A point I haven't seen brought up is tokenization. Word2Vec and Glove handle whole words, and can't easily handle
> words they haven't seen before. FastText (based on Word2Vec) is word-fragment based and can usually handle unseen words,
> although it still generates one vector per word. Elmo is purely character-based, providing vectors for each character
> that can combined through a deep learning model or simply averaged to get a word vector (edit: the off-the-shelf
> implementation gives whole-word vectors like this already). BERT has it's own method of chunking unrecognized words into
> ngrams it recognizes (e.g. circumlocution might be broken into "circum", "locu" and "tion"), and these ngrams can be
> averaged into whole-word vectors.
> ELMo and BERT incorporate context, handling polysemy and nuance much better (e.g. sentences like "Time flies like an
> arrow. Fruit flies like bananas") . This in general improves performance notably on downstream tasks. However, they're
> designed using whole sentences as context, and in some applications you might be working on individual words or phrases
> and their context in a sentence isn't easily available, in which case Word2Vec or GloVe might be better.
> Availability of pretrained vectors in other languages also varies widely. FastText, for example, has models in dozens
> of languages. Bert has a general multilingual model and a Chinese pretrained model published.
> Bert is also designed to be fine-tuned easily, and is designed so you can drop it into a classifier without having to
> do much network building or customization. Although note that fine-tuning of these vectors can potentially hurt
> generalization, especially if your data set is small.
> Technically Bert is considered state-of-the-art, but compared to some of the practical concerns like whether you have
> a good context and whether you have a lot of obscure words, what's state-of-the-art may be a minor consideration.
> https://www.reddit.com/r/MachineLearning/comments/aptwxm/d_what_are_the_main_differences_between_the_word/

> Note that BERT is a pre-trained model that has been trained from unlabeled data extracted from the BooksCorpus featuring 800 million words, and the English Wikipedia featuring 2,500 million words. Since BERT gives context-enriched embedding, it outperformed traditional NLP models such as Word2Vec on text processing tasks such as semantic and sentiment analysis [68]. The word embeddings generated by Word2Vec are contextindependent and cannot address the problem of polysemous words [68]. ...
> https://www.researchgate.net/publication/357321164_Comparison_of_Text_Sentiment_Analysis_based_on_Bert_and_Word2vec


