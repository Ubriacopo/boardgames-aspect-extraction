By referring to the paper we want to make our own implementation.

- First thing we will be needing is a way to communicate with bgg to extract the comments
- We will be needing for an embedding model as they measure relative distance:
    - Do we want our own trained on the corpus of BGG? Do we want to fine-tune an existing one?
      Or do we simply want to use an exiting one without further steps?
    - **Probably the way to go is to fine-tune an existing one, but it could be a fun exercise to
      compare one trained only on BGG corpus and one fine-tuned.**
- Instead of going for Markov models or random fields we stay at the state of the art
  Unsupervised Neural Models. (We have no labeled data)

> Rather than using all available information, attention mechanism aims to focus
> on the most pertinent information for a task

We use attention

#### Final idea:

We make our own implementation of the ABAE and see if I can come up with another model
and run it as well to see how bad I am at modelling and how much thought goes to find a good architecture.

## Note

> For the ABAE model, we initialize the word embedding matrix E with word vectors trained by
> word2vec with negative sampling on each dataset,setting the embedding size to 200, window size to
> 10, and negative sample size to 5

Come pensavo. Ma vedi cosa sono window e negative sample <br>
Per embeddings guarda:
> https://stackoverflow.com/questions/64145666/fine-tuning-of-bert-word-embeddings

## Word2vec-like or BERT-like?

This question requires further research:
> A point I haven't seen brought up is tokenization. Word2Vec and Glove handle whole words, and can't easily handle
> words they haven't seen before. FastText (based on Word2Vec) is word-fragment based and can usually handle unseen words,
> although it still generates one vector per word. Elmo is purely character-based, providing vectors for each character
> that can combined through a deep learning model or simply averaged to get a word vector (edit: the off-the-shelf
> implementation gives whole-word vectors like this already). BERT has it's own method of chunking unrecognized words into
> ngrams it recognizes (e.g. circumlocution might be broken into "circum", "locu" and "tion"), and these ngrams can be
> averaged into whole-word vectors.
> ELMo and BERT incorporate context, handling polysemy and nuance much better (e.g. sentences like "Time flies like an
> arrow. Fruit flies like bananas") . This in general improves performance notably on downstream tasks. However, they're
> designed using whole sentences as context, and in some applications you might be working on individual words or phrases
> and their context in a sentence isn't easily available, in which case Word2Vec or GloVe might be better.
> Availability of pretrained vectors in other languages also varies widely. FastText, for example, has models in dozens
> of languages. Bert has a general multilingual model and a Chinese pretrained model published.
> Bert is also designed to be fine-tuned easily, and is designed so you can drop it into a classifier without having to
> do much network building or customization. Although note that fine-tuning of these vectors can potentially hurt
> generalization, especially if your data set is small.
> Technically Bert is considered state-of-the-art, but compared to some of the practical concerns like whether you have
> a good context and whether you have a lot of obscure words, what's state-of-the-art may be a minor consideration.
> https://www.reddit.com/r/MachineLearning/comments/aptwxm/d_what_are_the_main_differences_between_the_word/

> Note that BERT is a pre-trained model that has been trained from unlabeled data extracted from the BooksCorpus featuring 800 million words, and the English Wikipedia featuring 2,500 million words. Since BERT gives context-enriched embedding, it outperformed traditional NLP models such as Word2Vec on text processing tasks such as semantic and sentiment analysis [68]. The word embeddings generated by Word2Vec are contextindependent and cannot address the problem of polysemous words [68]. ...
> https://www.researchgate.net/publication/357321164_Comparison_of_Text_Sentiment_Analysis_based_on_Bert_and_Word2vec

## Going further

ABAE only recognizes Aspects, we have to give a sentiment to each one found in the comments to make
a final avarage or median on all the comments to classify a boardgame.

