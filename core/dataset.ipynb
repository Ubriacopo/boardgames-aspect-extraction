{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-02T17:29:20.643929Z",
     "start_time": "2025-01-02T17:29:20.639918Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "random_state = 281997"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# todo: rewritre\n",
    "\n",
    "## Subsample the data\n",
    "We should limit the number of reviews, how many? Let's look at some case studies:\n",
    "\n",
    "- Amazon Product Reviews\n",
    "Size: Varies by category, but subsets of 5,000 to 20,000 reviews are common.\n",
    "- Yelp Dataset\n",
    "Size: Typically, 8,000 to 15,000 reviews are used in research for unsupervised aspect extraction.\n",
    "- TripAdvisor Reviews\n",
    "Size: Around 5,000 to 10,000 reviews in unsupervised experiments.\n",
    "\n",
    "For unsupervised learning, 5,000â€“10,000 reviews is a reasonable starting point for recognizing 6 aspects. <br>\n",
    "More reviews may improve diversity and robustness but come with increased computational costs.\n",
    "\n",
    "\n"
   ],
   "id": "3320e5f9231d0f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:47:13.569030Z",
     "start_time": "2024-12-17T14:47:13.566295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\""
   ],
   "id": "a22206e9f1313532",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special Scenario: Kickstarter\n",
    "Many reviews on BGG reference the Kickstarter campaigns of the games. <br>\n",
    "Most of these reviews are not informative and are not useful for training the model."
   ],
   "id": "6335025923ce881c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:18:16.928672Z",
     "start_time": "2024-12-16T14:18:11.717452Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(corpus_file)"
   ],
   "id": "d6928c901ad983df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How Many comments contain Kickstarter?\n",
    "Let's measure it! And while at it check how many of these are short comments:"
   ],
   "id": "266cadd4c010a7bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:35:53.640722Z",
     "start_time": "2024-12-16T14:35:50.492164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kickstarter_subset = dataset[dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "print(\n",
    "    f\"The subset is {len(kickstarter_subset) / len(dataset) * 100}% of \"\n",
    "    f\"the original with a total of {len(kickstarter_subset)} comments.\"\n",
    ")"
   ],
   "id": "20c396e5b00ccef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subset is 1.619698530100178% of the original with a total of 34600 comments.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:36:21.755560Z",
     "start_time": "2024-12-16T14:36:21.694972Z"
    }
   },
   "cell_type": "code",
   "source": "kickstarter_counts = (kickstarter_subset[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()",
   "id": "9e87ed48892647b8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:04.489192Z",
     "start_time": "2024-12-16T14:36:58.495575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = dataset[~dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "counts = (ds[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()"
   ],
   "id": "ccd9591d38d03896",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:06.128123Z",
     "start_time": "2024-12-16T14:37:06.125028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"We loose a total of: {kickstarter_counts.get(True) / (kickstarter_counts.get(True) + counts.get(True)) * 100}% possible extractions from the dataset. \\nLess than 1.1, we can just ignore Kickstarter comments\"\n",
    ")"
   ],
   "id": "d054bb1a376db041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We loose a total of: 1.0276979373944586% possible extractions from the dataset. \n",
      "Less than 1.1, we can just ignore Kickstarter comments\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:38:03.843450Z",
     "start_time": "2024-12-16T14:37:57.295257Z"
    }
   },
   "cell_type": "code",
   "source": "ds.to_csv(\"../data/corpus.csv\", index=False)",
   "id": "618d06513fce2184",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How many times are game titles referenced in the corpus? Let's see!\n",
   "id": "511e19578cae31b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.398981400Z",
     "start_time": "2024-12-14T16:50:09.661053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "# As we use regex pattern to check we might have some problems with some game names.\n",
    "# These include cases like: [kosmopoli:t], [redacted], or **, or ???. They are only 9 games.\n",
    "# This should not change the results of our inspections too dramatically especially because they are not as popular as other games.\n",
    "game_names = game_names.drop([3011, 6800, 13330, 14764, 19280, 20312, 21764, 21796, 25651])\n",
    "\n",
    "print(f\"We have a total of {len(game_names)} games.\")\n",
    "match_string = \"|\".join(game_names)"
   ],
   "id": "5a061b08726d4845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 25890 games.\n"
     ]
    }
   ],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.400980900Z",
     "start_time": "2024-12-14T16:50:25.255940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We do match case for those unfortunate cases where game names are actually common use terms like Risk, Get Lucky etc...\n",
    "# I prefer to underestimate than overestimate in this case as it seems the wisest of the two approaches.\n",
    "game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]"
   ],
   "id": "c5c56b170b318d52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacop\\AppData\\Local\\Temp\\ipykernel_24384\\3484882493.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.402979900Z",
     "start_time": "2024-12-14T23:50:35.789468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"A total {len(dataset) - len(game_named_subset)} games have no reference to game names. This is {len(game_named_subset) / len(dataset) * 100}%\")  #These many games contain game references reviews."
   ],
   "id": "bc0015f502636892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total 864815 games have no reference to game names. This is 59.51619698530101%\n"
     ]
    }
   ],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A total of 864815 reference game names at least once in the comment. <br> Replacing those with the \\<GAME_NAME> token might be beneficial to reduce noise in the data",
   "id": "3f91cf6b225bff4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The downloaded information from the BGG API might not be informative, faulty or bloated with useless information. <br>\n",
    "In order to avoid this we apply some pre-processing steps in order to filter out information we don't need, that may be entire records or some of the \n",
    "text inside a line.\n",
    "\n",
    "During the process we already make the tokenization and stemming of the text using the ```spacy```\n"
   ],
   "id": "dd77baf7691d9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Some parts of torch that are used by Spacy are deprecated, we can ignore them \n",
    "# (The new 3.8 Spacy has some little issues, so we keep it like it is for now)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "id": "64a698bac2ddfbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Spacy\n",
    "To download the model and use it with spacy:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "id": "ee94d5b53e9a4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Best compromise between accuracy and speed\n",
    "model = spacy.load(\"en_core_web_md\")"
   ],
   "id": "a27a63bb78a7956c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PreProcessingService\n",
    "Class that holds the process to clean the text and produce a stemmed corpus. <br/> This will then be persisted in a file to avoid re-processing the same data."
   ],
   "id": "1e98af23ba814cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:07:40.544219Z",
     "start_time": "2024-12-16T22:07:21.159935Z"
    }
   },
   "cell_type": "code",
   "source": "from core.pre_processing import CleanTextRule",
   "id": "8d6e62a1a3aada51",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.407980200Z",
     "start_time": "2024-12-09T14:34:22.946931Z"
    }
   },
   "cell_type": "code",
   "source": "demo_text = \"This is a demo text. Isn't Root just an amazing game? I love it!\"",
   "id": "2a4b2f815ea5d707",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BGG noise removal\n",
    "BGG comments can carry metadata such as images and some pseudo-html tags. <br>\n",
    "To avoid processing those we simply remove them applying two regexes:"
   ],
   "id": "d1e697b1e80ef34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.408982Z",
     "start_time": "2024-12-09T14:35:14.660540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As defined in the PreProcessingService\n",
    "clean_tags_regex = r\"(?i)\\[(?P<tag>[A-Z]+)\\].*?\\[/\\1\\]\"\n",
    "keep_tag_content_regex = r\"(?i)\\[(?P<tag>[a-z]+)(=[^\\]]+)?\\](.*?)\\[/\\1\\]\""
   ],
   "id": "d8e102ce5c62c959",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CleanTextRule(clean_tags_regex).process(\n",
    "    \"This is a test for processing [IMG]https://cf.geekdo-static.com/mbs/mb_5855_0.gif[/IMG] as content\"\n",
    ")"
   ],
   "id": "e85f6b511ed108c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "CleanTextRule(keep_tag_content_regex, r'\\3').process(\"This is a test for processing [b=323]bold[/b] as content\")",
   "id": "4f22f395bed61768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Language detection\n",
    "While it of course would be amazing to have a model with multiple languages support, we are focusing on English. <br>\n",
    "To filter out foreign languages we use the ```langdetect``` library."
   ],
   "id": "13ce78a96095cb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fast_langdetect import detect\n",
    "\n",
    "german_sentence = \"Naja, ich finde die Siedler von Catan immer noch besser\"\n",
    "print(f\"For the demo sentence: \\\"{demo_text}\\\" we detected: {detect(demo_text)['lang']}\")\n",
    "print(f\"For the demo sentence: \\\"{german_sentence}\\\" we detected: {detect(german_sentence)['lang']}\")"
   ],
   "id": "c83a85ac0f4dc4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.415328100Z",
     "start_time": "2024-12-14T23:54:30.156163Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir hatten heute viel spass\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 288,
   "source": [
    "from pre_processing import FilterLanguageRule\n",
    "\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"Wir hatten heute viel spass\"))\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"We had lots of fun today\"))"
   ],
   "id": "2585199cc9cea657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split reviews in sentences\n",
    "This should help us generate shorter input data and also giving more granularity to the reviews making aspect extraction simpler. <br>\n",
    "An approach like this was taken by different academic studies such as:\n",
    "- Improving unsupervised neural aspect extraction for online discussions using out-of-domain classification (https://arxiv.org/abs/2006.09766)\n",
    "- Embarrassingly Simple Unsupervised Aspect Extraction (https://arxiv.org/abs/2004.13580)"
   ],
   "id": "c146ffa86b845a9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:32.519548Z",
     "start_time": "2024-12-17T18:48:32.422124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import SplitSentencesRule\n",
    "\n",
    "SplitSentencesRule().process(\n",
    "    \"Brilliant!  Fits right into my wheelhouse all around and game weight.\"\n",
    "    \"This also has some of the most and best player interaction I've experienced in a game!\"\n",
    ")"
   ],
   "id": "23c6059c22517fa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brilliant!',\n",
       " 'Fits right into my wheelhouse all around and game weight.',\n",
       " \"This also has some of the most and best player interaction I've experienced in a game!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and lemmatization\n",
    "Using ```spacy``` we tokenize the text and then we lemmatize it. <br>"
   ],
   "id": "9d09de9d83f7803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextRule\n",
    "\n",
    "LemmatizeTextRule().process(demo_text)  # (Should be considered private)"
   ],
   "id": "dd3379b4e9a69b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove too narrow texts\n",
    "Comments (reviews) that are too short might not be informative. <br>\n",
    "We already remove stopwords and punctuation, so we can filter out comments that are too short but we better set a reasonable threshold (not too high). This step is done by the PreProcessingService aswell."
   ],
   "id": "2f3b1f7ff4e3d626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pre_processing import ShortTextFilterRule\n",
    "\n",
    "ShortTextFilterRule(4).process(['this', 'is', 'short'])"
   ],
   "id": "bd43f68840b97cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Remove Dates\n",
    "I believe dates can be good information but not if too specific. Thus, we replace the actual dates with a custom <DATE> token using ```DateMatcherReplacementRule```. <br>\n",
    "This allows us to maintain the information but reduce the granularity of it."
   ],
   "id": "8878f029fc1eebfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:54:10.911294Z",
     "start_time": "2024-12-15T12:54:09.655199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextWithMatcherRules, DateMatcherReplacementRule, GameNamesMatcherReplacementRule\n",
    "import spacy\n",
    "\n",
    "text = \"Rating previous Gloomhaven to February 2017: 8.1 Rating previous to Oct 2017: 9.45 Rating previous to June 2018: 7.72. 10/10/2023\"\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "LemmatizeTextWithMatcherRules(nlp, rules=[DateMatcherReplacementRule(nlp.vocab), ]).process(text)"
   ],
   "id": "697c316a53bbce74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rate',\n",
       " 'previous',\n",
       " '<GAME_NAME>',\n",
       " '<DATE>',\n",
       " '8.1',\n",
       " 'rating',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '9.45',\n",
       " 'rate',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '7.72',\n",
       " '<DATE>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Delete duplicated rows\n",
    "There might, and there are, duplicate rows in our dataset. These are filtered out by the ```PreProcessingService``` after each step of processing. <br>\n",
    "It subsets on the original comment and game_id."
   ],
   "id": "c32d531102eac81d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch Process",
   "id": "eb8ea8a516f6e333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:20:24.031570Z",
     "start_time": "2024-12-20T21:20:23.956631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\"\n",
    "# Our known game names.\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']"
   ],
   "id": "651e810f292f936b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:20:24.370220Z",
     "start_time": "2024-12-20T21:20:24.363015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specially tailored possible cases\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "print(len(game_names))"
   ],
   "id": "eb92bc2ea9925397",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25901\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This pre-processing might not be perfect BUT it is good enough and probably a step in the right direction. <br>\n",
    "A complete model or well thought way to recognize board games is desirable but a long task on its own."
   ],
   "id": "b9b685dbf42c8041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:21:51.947414Z",
     "start_time": "2024-12-20T21:20:25.081569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import swifter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # We use small as we don't need anything over the top.\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()"
   ],
   "id": "21975e4afdf3ecb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25901 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2a1f8f143af4c9cba5ca080a4a4f5d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:21:56.928072Z",
     "start_time": "2024-12-20T21:21:51.992704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")"
   ],
   "id": "9c4393244a12e2c1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will create these datasets:\n",
    "- ```default_pipeline```: 64k\n",
    "- ```full_pipeline```: 64k, 256k and 512k\n",
    "\n",
    "This is under the assumption that the more data yeild better models."
   ],
   "id": "5b39a6cfdbfe8e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:26:58.100208Z",
     "start_time": "2024-12-20T21:26:47.651833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import DatasetGeneration\n",
    "from core.dataset_sampler import BggDatasetRandomBalancedSampler, BggDatasetLongestSampler\n",
    "\n",
    "random_states = [2, 8, 97]\n",
    "combinations: [DatasetGeneration] = [\n",
    "    DatasetGeneration(pipeline, 50000, BggDatasetRandomBalancedSampler(10000, corpus_file, random_states[0])),\n",
    "    DatasetGeneration(pipeline, 100000, BggDatasetRandomBalancedSampler(20000, corpus_file, random_states[1])),\n",
    "    DatasetGeneration(pipeline, 200000, BggDatasetRandomBalancedSampler(40000, corpus_file, random_states[2])),\n",
    "]"
   ],
   "id": "887cccead51b9810",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T22:48:00.009212Z",
     "start_time": "2024-12-20T21:27:00.375251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('We will generate a total of:', len(combinations), ' datasets')\n",
    "for combination in combinations:\n",
    "    pipeline, target_size, sampler = combination\n",
    "    longest_affix = \"_longest\" if type(sampler) is BggDatasetLongestSampler else \"\"\n",
    "\n",
    "    name = f\"{int(target_size / 1000)}k{longest_affix}\"\n",
    "    print(\"Generated dataset will be stored in file of prefix: \" + name)\n",
    "    file = pipeline.pre_process_corpus(combination.target_size, sampler, name)\n",
    "\n",
    "    print(f\"Generated dataset in file: {file}\")"
   ],
   "id": "a30da8bdb2f2645",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will generate a total of: 2  datasets\n",
      "Generated dataset will be stored in file of prefix: 256k\n",
      "I have a total of 2220 games with reviews. We take 29 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bbef9399d9846709688bff4a34e2bd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 29 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64379 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb11bea422604f8b8087ba1f3f6462b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/256k.preprocessed.csv\n",
      "Generated dataset will be stored in file of prefix: 512k\n",
      "I have a total of 2220 games with reviews. We take 46 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/102120 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eee570cb00034c3e8bf46bf5366b5e2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 46 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/102118 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5545573d05964c528f936c38ba7870dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 46 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/102116 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cf5a78a30f54754abcf14a9b857c051"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/512k.preprocessed.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See how the dataset changed:",
   "id": "d1ac6ed6895a43c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset Definition\n",
    "To train the model we require a way to get elements of our dataset. ```torch``` provides a way to do this by defining a custom ```Dataset``` class. <br>\n",
    "This class and later loaded into a ```DataLoader``` that will provide the batches of data to the model."
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to generate valid inputs for the model we have to give a numerical representation to our data. <br>\n",
    "In order to do so we use a ```WordEmbedding``` model that will give us the dictionary of the recognized words (The embeddings will be generated inside the model). <br>"
   ],
   "id": "97c7e216c29993ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vocab size?\n",
    "In an ideal world the words in our dictionary are mapped 1 to 1.<br>\n",
    "There are two problems with this approach:\n",
    "- It might not be feasible\n",
    "- We might be introducing too much noise for words that occur in few cases\n",
    "\n",
    "Let's still see if it is generally feasible:"
   ],
   "id": "2706cc7b5e030f4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:34.885250Z",
     "start_time": "2024-12-19T16:45:30.929380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.utils import LoadCorpusUtility\n",
    "\n",
    "# Just to see if it is feasible even if we won't be going for all the words at the end.\n",
    "utility = LoadCorpusUtility(min_word_count=0)\n",
    "\n",
    "corpora = [\n",
    "    dict(file=\"../data/processed-dataset/default/64k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/64k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/256k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/256k_longest.preprocessed.csv\"),\n",
    "]"
   ],
   "id": "a3bfc6807da9abe4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T15:55:06.146836Z",
     "start_time": "2024-12-19T15:54:47.649748Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/69200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c63382139dc74bd3abb293c69974597f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/default/64k.preprocessed.csv.\n",
      "We have a total of 23408 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80318 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0964505753f74470a6f44d37b351a2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/64k.preprocessed.csv.\n",
      "We have a total of 27473 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/288651 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "993834e046ec4917811dddfac778ca92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/256k.preprocessed.csv.\n",
      "We have a total of 55288 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/736608 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd879fc23dae4cb590995800e7307e4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/256k_longest.preprocessed.csv.\n",
      "We have a total of 91499 unique words.\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "for corpus in corpora:\n",
    "    dictionary = utility.make_corpus_dictionary(corpus['file'])\n",
    "    corpus[\"full_dictionary\"] = dictionary\n",
    "    print(f\"For corpus file: {corpus['file']}.\\nWe have a total of {len(dictionary)} unique words.\")"
   ],
   "id": "76ec020e47caae34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try for the 64k full dataset:",
   "id": "7e51c7b835149cb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:37:00.520196Z",
     "start_time": "2024-12-19T16:36:59.459694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.embeddings import WordEmbedding\n",
    "\n",
    "emb_model = WordEmbedding(\n",
    "    corpus_loader_utility=utility, embedding_size=128,\n",
    "    target_model_file='../output/256k-full-longest.embeddings.model',\n",
    "    corpus_file=corpora[3]['file'], min_word_count=1\n",
    ")\n",
    "\n",
    "len(emb_model.vocabulary())"
   ],
   "id": "bc279e7d0b233f64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ..\\output\\256k-full-longest.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': '..\\\\output\\\\256k-full-longest.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from ..\\output\\256k-full-longest.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from ..\\output\\256k-full-longest.embeddings.model.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from ..\\output\\256k-full-longest.embeddings.model.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': '..\\\\output\\\\256k-full-longest.embeddings.model', 'datetime': '2024-12-19T17:37:00.515183', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91499"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the longest dataset it just took us 30s. It is feasible on the computational side. <br>\n",
    "We still want to limit the noise of our dataset so we allow only words that occur with a certain frequency threshold. We use, as proposed when the paper was published, a ```min_word_count=5```.\n",
    "\n",
    "> We might want to test different values of min_word_count and see how it affects the model"
   ],
   "id": "3978c0634b637776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T17:33:31.859656Z",
     "start_time": "2025-01-02T17:33:23.203624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.embeddings import WordEmbedding\n",
    "from core.utils import LoadCorpusUtility\n",
    "\n",
    "corpus_loader_utility = LoadCorpusUtility(min_word_count=4)\n",
    "emb_model = WordEmbedding(embedding_size=128, target_path=\"\")\n",
    "emb_model.generate(\n",
    "    corpus=corpus_loader_utility.load_data(\"../data/processed-dataset/full/256k.preprocessed.csv\"),\n",
    "    persist=False\n",
    ")\n",
    "\n",
    "vocabulary = emb_model.vocabulary()"
   ],
   "id": "aa387a7c57d647b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/288429 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93b3e15434044a32bbb1f6bcb92099e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/288429 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6694750e8b2445e79d0c109bd790a992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 68802 words, keeping 5740 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 138597 words, keeping 7578 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 210768 words, keeping 8691 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 284920 words, keeping 9570 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 359676 words, keeping 10262 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 435953 words, keeping 10766 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 511632 words, keeping 11175 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 585652 words, keeping 11490 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 661413 words, keeping 11767 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 737431 words, keeping 11968 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 813902 words, keeping 12116 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 890073 words, keeping 12259 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 966111 words, keeping 12377 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 1043995 words, keeping 12450 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 1124080 words, keeping 12495 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 1200220 words, keeping 12514 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 1276502 words, keeping 12529 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 1352604 words, keeping 12538 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 1423746 words, keeping 12542 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 1495759 words, keeping 12552 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 1571144 words, keeping 12560 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 1647571 words, keeping 12562 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 1718291 words, keeping 12565 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 1789278 words, keeping 12567 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 1861186 words, keeping 12567 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 1938131 words, keeping 12570 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 2014344 words, keeping 12571 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 2089041 words, keeping 12572 word types\n",
      "INFO:gensim.models.word2vec:collected 12577 word types from a corpus of 2155570 raw words and 288429 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 12577 unique words (100.00% of original 12577, drops 0)', 'datetime': '2025-01-02T18:33:27.799571', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2155570 word corpus (100.00% of original 2155570, drops 0)', 'datetime': '2025-01-02T18:33:27.801064', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 12577 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 52 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1785594.114482151 word corpus (82.8%% of prior 2155570)', 'datetime': '2025-01-02T18:33:27.836591', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 12577 words and 128 dimensions: 19167348 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-02T18:33:27.900454', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 8 workers on 12577 vocabulary and 128 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-01-02T18:33:27.900454', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 216 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 2155570 raw words (1785542 effective words) took 0.7s, 2519564 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 216 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 2155570 raw words (1785469 effective words) took 0.8s, 2270575 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 216 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 2155570 raw words (1785702 effective words) took 0.8s, 2238232 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 216 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 2155570 raw words (1785163 effective words) took 0.8s, 2281705 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 216 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 2155570 raw words (1785360 effective words) took 0.8s, 2268677 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 10777850 raw words (8927236 effective words) took 3.9s, 2273116 effective words/s', 'datetime': '2025-01-02T18:33:31.828617', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12577, vector_size=128, alpha=0.025>', 'datetime': '2025-01-02T18:33:31.828617', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T17:33:33.694276Z",
     "start_time": "2025-01-02T17:33:33.689616Z"
    }
   },
   "cell_type": "code",
   "source": "len(vocabulary)  # Is it a small vocabulary?",
   "id": "797f235b0c7449af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12577"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositiveNegativeCommentGeneratorDataset\n",
    "Gives a sample and also returns some negative samples for contrastive learning. <br>\n"
   ],
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:38.592733Z",
     "start_time": "2024-12-19T16:45:37.566640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(corpora[1]['file'], vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file: ../data/processed-dataset/full/64k.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80318 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "722b38a15ac54bbe80628464acfbfff9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  206 . The limit is set to 256 tokens.\n",
      "Padding sequences to length (256).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:40.406056Z",
     "start_time": "2024-12-19T16:45:40.402536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lazy_dataloader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "id": "6dab4b179d654a7a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:46:09.725041Z",
     "start_time": "2024-12-19T16:46:09.717526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 11  # A random index to show content and \n",
    "print(\n",
    "    f\"Sentence at index {i} original text is: `{ds.get_text_sentence(i)}`\\n \"\n",
    "    f\"It's numeric representation:\\n {ds[i][0][0]}\"\n",
    ")"
   ],
   "id": "8da3fa5e231920ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 11 original text is: `play twice Grune`\n",
      " It's numeric representation:\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2 516   1]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sequence length truncation\n",
    "The model will be trained on sequences of fixed length. <br>\n",
    "The chosen length must be reasonable, we can't just pad everything out for the same of it. <br>\n",
    "\n",
    "We want that the top 95% of the reviews are not truncated. <br>"
   ],
   "id": "2ac5649fbecf63c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T17:34:58.716921Z",
     "start_time": "2025-01-02T17:34:55.474237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(\n",
    "    \"../data/processed-dataset/full/256k.preprocessed.csv\", vocabulary, 10, max_seq_length=80\n",
    ")"
   ],
   "id": "e65999b17f3b8d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file: ../data/processed-dataset/full/256k.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/288429 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50b1dbc92434dc38474f4277d78a12b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  262 . The limit is set to 80 tokens.\n",
      "We loose information on 65 points.This is 0.022535875380076207% of the dataset.\n",
      "Padding sequences to length (80).\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We shall keep 80 as max_seq_length as: 0.022535875380076207% of the dataset records have loss of information.\n",
    "# This will speed up the learning process."
   ],
   "id": "aa5a53d5b26d559c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
