{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T15:36:13.086082Z",
     "start_time": "2024-11-29T15:36:13.084104Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "random_state = 281997"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Subsample the data\n",
    "We should limit the number of reviews, how many? Let's look at some case studies:\n",
    "\n",
    "- Amazon Product Reviews\n",
    "Size: Varies by category, but subsets of 5,000 to 20,000 reviews are common.\n",
    "- Yelp Dataset\n",
    "Size: Typically, 8,000 to 15,000 reviews are used in research for unsupervised aspect extraction.\n",
    "- TripAdvisor Reviews\n",
    "Size: Around 5,000 to 10,000 reviews in unsupervised experiments.\n",
    "\n",
    "For unsupervised learning, 5,000â€“10,000 reviews is a reasonable starting point for recognizing 6 aspects. More reviews may improve diversity and robustness but come with increased computational costs.\n",
    "\n",
    "\n"
   ],
   "id": "3320e5f9231d0f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:36:15.151906Z",
     "start_time": "2024-11-29T15:36:14.899758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_file = \"../data/corpus.csv\"\n",
    "sampled_corpus_file = \"../data/corpus.sampled.csv\""
   ],
   "id": "2a3c38e49670b1a9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "og_data = pd.read_csv(corpus_file)\n",
    "reviews_per_game = int(64000 / len(og_data.groupby([\"game_id\"]).count())) + 1\n",
    "\n",
    "print(f\"I have a total of {len(og_data.groupby([\"game_id\"]).count())} games with reviews. \"\n",
    "      f\"We want to be ~64k reviews so we take {reviews_per_game} reviews per game.\")"
   ],
   "id": "1bcc4826493661a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We start by using ~64k reviews (More robustness). This is before pre-processing which might reduce the total number of reviews later.\n",
    "(\n",
    "    og_data.groupby(\"game_id\", group_keys=False)[og_data.columns]\n",
    "    .apply(lambda x: x.sample(min(len(x), reviews_per_game), random_state=random_state))\n",
    "    .to_csv(sampled_corpus_file, index=False)\n",
    ")"
   ],
   "id": "706f590b584adbab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check distribution of games",
   "id": "2b9f595bf8a9ce64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(sampled_corpus_file)\n",
    "data"
   ],
   "id": "6e664d235ddec991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.groupby([\"game_id\"]).count()\n",
    "# Each of our games has the same representation then others. The \"reviews\" should be balanced across all games.\n",
    "# We can now proceed to pre-process the data."
   ],
   "id": "43f74533b8eaa2d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The downloaded information from the BGG API might not be informative, faulty or bloated with useless information. <br>\n",
    "In order to avoid this we apply some pre-processing steps in order to filter out information we don't need, that may be entire records or some of the \n",
    "text inside a line.\n",
    "\n",
    "During the process we already make the tokenization and stemming of the text using the ```spacy```\n"
   ],
   "id": "dd77baf7691d9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Some parts of torch that are used by Spacy are deprecated, we can ignore them \n",
    "# (The new 3.8 Spacy has some little issues, so we keep it like it is for now)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "id": "64a698bac2ddfbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Spacy\n",
    "To download the model and use it with spacy:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "id": "ee94d5b53e9a4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "a27a63bb78a7956c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PreProcessingService\n",
    "Class that holds the process to clean the text and produce a stemmed corpus. <br/> This will then be persisted in a file to avoid re-processing the same data."
   ],
   "id": "1e98af23ba814cc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import PreProcessingService\n",
    "\n",
    "ps = PreProcessingService()"
   ],
   "id": "8d6e62a1a3aada51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "demo_text = \"This is a demo text. Isn't Root just an amazing game? I love it!\"",
   "id": "2a4b2f815ea5d707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BGG noise removal\n",
    "BGG comments can carry metadata such as images and some pseudo-html tags. <br>\n",
    "To avoid processing those we simply remove them applying two regexes:"
   ],
   "id": "d1e697b1e80ef34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# As defined in the PreProcessingService\n",
    "clean_tags_regex = r\"(?i)\\[(?P<tag>[A-Z]+)\\].*?\\[/\\1\\]\"\n",
    "keep_tag_content_regex = r\"\\[(?P<tag>[a-z]+)(=[^\\]]+)?\\](.*?)\\[/\\1\\]\""
   ],
   "id": "d8e102ce5c62c959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ps.clean_text(\"This is a test for processing [IMG]https://cf.geekdo-static.com/mbs/mb_5855_0.gif[/IMG] as content\")",
   "id": "844da63647167857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ps.clean_text(\"This is a test for processing [b=323]bold[/b] as content\")",
   "id": "4f22f395bed61768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Language detection\n",
    "While it of course would be amazing to have a model with multiple languages support, we are focusing on English. <br>\n",
    "To filter out foreign languages we use the ```langdetect``` library."
   ],
   "id": "13ce78a96095cb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fast_langdetect import detect\n",
    "\n",
    "german_sentence = \"Naja, ich finde die Siedler von Catan immer noch besser\"\n",
    "print(f\"For the demo sentence: \\\"{demo_text}\\\" we detected: {detect(demo_text)['lang']}\")\n",
    "print(f\"For the demo sentence: \\\"{german_sentence}\\\" we detected: {detect(german_sentence)['lang']}\")"
   ],
   "id": "c83a85ac0f4dc4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and lemmatization\n",
    "Using ```spacy``` we tokenize the text and then we lemmatize it. <br>"
   ],
   "id": "9d09de9d83f7803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ps._make_text_lemmas(demo_text)  # (Should be considered private)",
   "id": "dd3379b4e9a69b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove too narrow texts\n",
    "Comments (reviews) that are too short might not be informative. <br>\n",
    "We already remove stopwords and punctuation, so we can filter out comments that are too short but we better set a reasonable threshold (not too high). This step is done by the PreProcessingService aswell."
   ],
   "id": "2f3b1f7ff4e3d626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ps.pre_process(demo_text)",
   "id": "bd43f68840b97cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch Process",
   "id": "eb8ea8a516f6e333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:36:22.471733Z",
     "start_time": "2024-11-29T15:36:22.469661Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_corpus_file: str = \"../data/corpus.preprocessed.csv\"",
   "id": "4c9247f72399ebab",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import pre_process_corpus\n",
    "\n",
    "pre_process_corpus(sampled_corpus_file, preprocessed_corpus_file, False)"
   ],
   "id": "8b468bf30543713d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See how the dataset changed:",
   "id": "d1ac6ed6895a43c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(pd.read_csv(preprocessed_corpus_file))  # We lost 14k reviews but it is okay! (I expect to lose more)",
   "id": "edda0efd2ab7a66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset Definition\n",
    "To train the model we require a way to get elements of our dataset. ```torch``` provides a way to do this by defining a custom ```Dataset``` class. <br>\n",
    "This class and later loaded into a ```DataLoader``` that will provide the batches of data to the model."
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to generate valid inputs for the model we have to give a numerical representation to our data. <br>\n",
    "In order to do so we use a ```WordEmbedding``` model that will give us the dictionary of the recognized words (The embeddings will be generated inside the model). <br>"
   ],
   "id": "97c7e216c29993ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:36:24.644575Z",
     "start_time": "2024-11-29T15:36:24.641879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_vocab_size = 16000\n",
    "embedding_size = 128\n",
    "target_embedding_model_file = \"./../data/word-embeddings.model\""
   ],
   "id": "6c98540db8a486b6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:36:28.699066Z",
     "start_time": "2024-11-29T15:36:25.309419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import core.utils as utils\n",
    "import core.embeddings as embeddings\n",
    "\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    utils.LoadCorpusUtility(), max_vocab_size=max_vocab_size, embedding_size=embedding_size,\n",
    "    target_model_file=target_embedding_model_file, corpus_file=preprocessed_corpus_file\n",
    ")"
   ],
   "id": "f3768d3e3077cd5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:38:14.044990Z",
     "start_time": "2024-11-29T15:37:22.815108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We require a vocabulary to map the words to indexes\n",
    "embeddings_model.load_model()\n",
    "embeddings_model.get_vocab()\n",
    "\n",
    "vocabulary = embeddings_model.model.wv.key_to_index"
   ],
   "id": "423dafa9a3a9c36b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/swifter/swifter.py:87: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50462 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "910eec1cd9854a619af4873ebc5fdcb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/swifter/swifter.py:87: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50462 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "000f20100eb94ef9bc8d184c9c5c41ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 196688 words, keeping 8896 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 426738 words, keeping 11170 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 646351 words, keeping 12215 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 880823 words, keeping 12729 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1120282 words, keeping 12946 word types\n",
      "INFO:gensim.models.word2vec:collected 12954 word types from a corpus of 1132772 raw words and 50462 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 12954 unique words (100.00% of original 12954, drops 0)', 'datetime': '2024-11-29T16:38:09.179087', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1132772 word corpus (100.00% of original 1132772, drops 0)', 'datetime': '2024-11-29T16:38:09.180134', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 12954 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 47 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 959249.5488920466 word corpus (84.7%% of prior 1132772)', 'datetime': '2024-11-29T16:38:09.276398', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 12954 words and 128 dimensions: 19741896 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-29T16:38:09.374765', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training core with 8 workers on 12954 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-11-29T16:38:09.375402', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 1132772 raw words (958943 effective words) took 0.9s, 1072379 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 17 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 1132772 raw words (958999 effective words) took 0.9s, 1016106 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 1132772 raw words (959366 effective words) took 0.8s, 1156268 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 1132772 raw words (959207 effective words) took 0.9s, 1026357 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 16 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 1132772 raw words (959125 effective words) took 1.0s, 998787 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 5663860 raw words (4795640 effective words) took 4.6s, 1034979 effective words/s', 'datetime': '2024-11-29T16:38:14.009462', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12954, vector_size=128, alpha=0.025>', 'datetime': '2024-11-29T16:38:14.009920', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './../data/word-embeddings.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-11-29T16:38:14.025268', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './../data/word-embeddings.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./../data/word-embeddings.model\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositiveNegativeCommentGeneratorDataset\n",
    "Gives a sample and also returns some negative samples for contrastive learning. <br>\n"
   ],
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:38:23.590499Z",
     "start_time": "2024-11-29T15:38:20.375066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(\"./../data/corpus.preprocessed.csv\", vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ./../data/corpus.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50461 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f2e0f29da444965b7f51b522f514de2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 136 points.This is 0.2695150710449654% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1235  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:38:25.361432Z",
     "start_time": "2024-11-29T15:38:25.358729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lazy_dataloader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "id": "6dab4b179d654a7a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:38:26.539327Z",
     "start_time": "2024-11-29T15:38:26.534377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 11  # A random index to show content and \n",
    "print(f\"Sentence at index {i} original text is: `{ds.get_text_item(i)}` (Look at [comments] property for the stripped down version)\\n \"\n",
    "      f\"It's numeric representation:\\n {ds[i][0][0]}\")"
   ],
   "id": "8da3fa5e231920ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 11 original text is: `Fun, but a bit complex for my taste.` (Look at [comments] property for the stripped down version)\n",
      " It's numeric representation:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 3001 1534  101  253 9886\n",
      "  179  173   56  213]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sequence length truncation\n",
    "The model will be trained on sequences of fixed length. <br>\n",
    "The chosen length must be reasonable, we can't just pad everything out for the same of it. <br>\n",
    "\n",
    "We want that the top 95% of the reviews are not truncated. <br>"
   ],
   "id": "2ac5649fbecf63c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We have 137 of the 50461 total reviews that are bigger than 256 tokens.\n",
    "# This is less than 1% of the total reviews. We can truncate."
   ],
   "id": "563f83c8769b8a6d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
