{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:27.733746Z",
     "start_time": "2024-12-19T16:45:27.730073Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "random_state = 281997"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Subsample the data\n",
    "We should limit the number of reviews, how many? Let's look at some case studies:\n",
    "\n",
    "- Amazon Product Reviews\n",
    "Size: Varies by category, but subsets of 5,000 to 20,000 reviews are common.\n",
    "- Yelp Dataset\n",
    "Size: Typically, 8,000 to 15,000 reviews are used in research for unsupervised aspect extraction.\n",
    "- TripAdvisor Reviews\n",
    "Size: Around 5,000 to 10,000 reviews in unsupervised experiments.\n",
    "\n",
    "For unsupervised learning, 5,000â€“10,000 reviews is a reasonable starting point for recognizing 6 aspects. <br>\n",
    "More reviews may improve diversity and robustness but come with increased computational costs.\n",
    "\n",
    "\n"
   ],
   "id": "3320e5f9231d0f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:47:13.569030Z",
     "start_time": "2024-12-17T14:47:13.566295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\""
   ],
   "id": "a22206e9f1313532",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special Scenario: Kickstarter\n",
    "Many reviews on BGG reference the Kickstarter campaigns of the games. <br>\n",
    "Most of these reviews are not informative and are not useful for training the model."
   ],
   "id": "6335025923ce881c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:18:16.928672Z",
     "start_time": "2024-12-16T14:18:11.717452Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(corpus_file)"
   ],
   "id": "d6928c901ad983df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How Many comments contain Kickstarter?\n",
    "Let's measure it! And while at it check how many of these are short comments:"
   ],
   "id": "266cadd4c010a7bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:35:53.640722Z",
     "start_time": "2024-12-16T14:35:50.492164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kickstarter_subset = dataset[dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "print(\n",
    "    f\"The subset is {len(kickstarter_subset) / len(dataset) * 100}% of \"\n",
    "    f\"the original with a total of {len(kickstarter_subset)} comments.\"\n",
    ")"
   ],
   "id": "20c396e5b00ccef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subset is 1.619698530100178% of the original with a total of 34600 comments.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:36:21.755560Z",
     "start_time": "2024-12-16T14:36:21.694972Z"
    }
   },
   "cell_type": "code",
   "source": "kickstarter_counts = (kickstarter_subset[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()",
   "id": "9e87ed48892647b8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:04.489192Z",
     "start_time": "2024-12-16T14:36:58.495575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = dataset[~dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "counts = (ds[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()"
   ],
   "id": "ccd9591d38d03896",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:06.128123Z",
     "start_time": "2024-12-16T14:37:06.125028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"We loose a total of: {kickstarter_counts.get(True) / (kickstarter_counts.get(True) + counts.get(True)) * 100}% possible extractions from the dataset. \\nLess than 1.1, we can just ignore Kickstarter comments\"\n",
    ")"
   ],
   "id": "d054bb1a376db041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We loose a total of: 1.0276979373944586% possible extractions from the dataset. \n",
      "Less than 1.1, we can just ignore Kickstarter comments\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:38:03.843450Z",
     "start_time": "2024-12-16T14:37:57.295257Z"
    }
   },
   "cell_type": "code",
   "source": "ds.to_csv(\"../data/corpus.csv\", index=False)",
   "id": "618d06513fce2184",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How many times are game titles referenced in the corpus? Let's see!\n",
   "id": "511e19578cae31b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.398981400Z",
     "start_time": "2024-12-14T16:50:09.661053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "# As we use regex pattern to check we might have some problems with some game names.\n",
    "# These include cases like: [kosmopoli:t], [redacted], or **, or ???. They are only 9 games.\n",
    "# This should not change the results of our inspections too dramatically especially because they are not as popular as other games.\n",
    "game_names = game_names.drop([3011, 6800, 13330, 14764, 19280, 20312, 21764, 21796, 25651])\n",
    "\n",
    "print(f\"We have a total of {len(game_names)} games.\")\n",
    "match_string = \"|\".join(game_names)"
   ],
   "id": "5a061b08726d4845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 25890 games.\n"
     ]
    }
   ],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.400980900Z",
     "start_time": "2024-12-14T16:50:25.255940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We do match case for those unfortunate cases where game names are actually common use terms like Risk, Get Lucky etc...\n",
    "# I prefer to underestimate than overestimate in this case as it seems the wisest of the two approaches.\n",
    "game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]"
   ],
   "id": "c5c56b170b318d52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacop\\AppData\\Local\\Temp\\ipykernel_24384\\3484882493.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.402979900Z",
     "start_time": "2024-12-14T23:50:35.789468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"A total {len(dataset) - len(game_named_subset)} games have no reference to game names. This is {len(game_named_subset) / len(dataset) * 100}%\")  #These many games contain game references reviews."
   ],
   "id": "bc0015f502636892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total 864815 games have no reference to game names. This is 59.51619698530101%\n"
     ]
    }
   ],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A total of 864815 reference game names at least once in the comment. <br> Replacing those with the \\<GAME_NAME> token might be beneficial to reduce noise in the data",
   "id": "3f91cf6b225bff4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The downloaded information from the BGG API might not be informative, faulty or bloated with useless information. <br>\n",
    "In order to avoid this we apply some pre-processing steps in order to filter out information we don't need, that may be entire records or some of the \n",
    "text inside a line.\n",
    "\n",
    "During the process we already make the tokenization and stemming of the text using the ```spacy```\n"
   ],
   "id": "dd77baf7691d9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Some parts of torch that are used by Spacy are deprecated, we can ignore them \n",
    "# (The new 3.8 Spacy has some little issues, so we keep it like it is for now)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "id": "64a698bac2ddfbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Spacy\n",
    "To download the model and use it with spacy:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "id": "ee94d5b53e9a4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Best compromise between accuracy and speed\n",
    "model = spacy.load(\"en_core_web_md\")"
   ],
   "id": "a27a63bb78a7956c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PreProcessingService\n",
    "Class that holds the process to clean the text and produce a stemmed corpus. <br/> This will then be persisted in a file to avoid re-processing the same data."
   ],
   "id": "1e98af23ba814cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:07:40.544219Z",
     "start_time": "2024-12-16T22:07:21.159935Z"
    }
   },
   "cell_type": "code",
   "source": "from core.pre_processing import CleanTextRule",
   "id": "8d6e62a1a3aada51",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.407980200Z",
     "start_time": "2024-12-09T14:34:22.946931Z"
    }
   },
   "cell_type": "code",
   "source": "demo_text = \"This is a demo text. Isn't Root just an amazing game? I love it!\"",
   "id": "2a4b2f815ea5d707",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BGG noise removal\n",
    "BGG comments can carry metadata such as images and some pseudo-html tags. <br>\n",
    "To avoid processing those we simply remove them applying two regexes:"
   ],
   "id": "d1e697b1e80ef34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.408982Z",
     "start_time": "2024-12-09T14:35:14.660540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As defined in the PreProcessingService\n",
    "clean_tags_regex = r\"(?i)\\[(?P<tag>[A-Z]+)\\].*?\\[/\\1\\]\"\n",
    "keep_tag_content_regex = r\"(?i)\\[(?P<tag>[a-z]+)(=[^\\]]+)?\\](.*?)\\[/\\1\\]\""
   ],
   "id": "d8e102ce5c62c959",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CleanTextRule(clean_tags_regex).process(\n",
    "    \"This is a test for processing [IMG]https://cf.geekdo-static.com/mbs/mb_5855_0.gif[/IMG] as content\"\n",
    ")"
   ],
   "id": "e85f6b511ed108c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "CleanTextRule(keep_tag_content_regex, r'\\3').process(\"This is a test for processing [b=323]bold[/b] as content\")",
   "id": "4f22f395bed61768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Language detection\n",
    "While it of course would be amazing to have a model with multiple languages support, we are focusing on English. <br>\n",
    "To filter out foreign languages we use the ```langdetect``` library."
   ],
   "id": "13ce78a96095cb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fast_langdetect import detect\n",
    "\n",
    "german_sentence = \"Naja, ich finde die Siedler von Catan immer noch besser\"\n",
    "print(f\"For the demo sentence: \\\"{demo_text}\\\" we detected: {detect(demo_text)['lang']}\")\n",
    "print(f\"For the demo sentence: \\\"{german_sentence}\\\" we detected: {detect(german_sentence)['lang']}\")"
   ],
   "id": "c83a85ac0f4dc4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.415328100Z",
     "start_time": "2024-12-14T23:54:30.156163Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir hatten heute viel spass\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 288,
   "source": [
    "from pre_processing import FilterLanguageRule\n",
    "\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"Wir hatten heute viel spass\"))\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"We had lots of fun today\"))"
   ],
   "id": "2585199cc9cea657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split reviews in sentences\n",
    "This should help us generate shorter input data and also giving more granularity to the reviews making aspect extraction simpler. <br>\n",
    "An approach like this was taken by different academic studies such as:\n",
    "- Improving unsupervised neural aspect extraction for online discussions using out-of-domain classification (https://arxiv.org/abs/2006.09766)\n",
    "- Embarrassingly Simple Unsupervised Aspect Extraction (https://arxiv.org/abs/2004.13580)"
   ],
   "id": "c146ffa86b845a9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T18:48:32.519548Z",
     "start_time": "2024-12-17T18:48:32.422124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import SplitSentencesRule\n",
    "\n",
    "SplitSentencesRule().process(\n",
    "    \"Brilliant!  Fits right into my wheelhouse all around and game weight.\"\n",
    "    \"This also has some of the most and best player interaction I've experienced in a game!\"\n",
    ")"
   ],
   "id": "23c6059c22517fa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brilliant!',\n",
       " 'Fits right into my wheelhouse all around and game weight.',\n",
       " \"This also has some of the most and best player interaction I've experienced in a game!\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and lemmatization\n",
    "Using ```spacy``` we tokenize the text and then we lemmatize it. <br>"
   ],
   "id": "9d09de9d83f7803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextRule\n",
    "\n",
    "LemmatizeTextRule().process(demo_text)  # (Should be considered private)"
   ],
   "id": "dd3379b4e9a69b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove too narrow texts\n",
    "Comments (reviews) that are too short might not be informative. <br>\n",
    "We already remove stopwords and punctuation, so we can filter out comments that are too short but we better set a reasonable threshold (not too high). This step is done by the PreProcessingService aswell."
   ],
   "id": "2f3b1f7ff4e3d626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pre_processing import ShortTextFilterRule\n",
    "\n",
    "ShortTextFilterRule(4).process(['this', 'is', 'short'])"
   ],
   "id": "bd43f68840b97cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Remove Dates\n",
    "I believe dates can be good information but not if too specific. Thus, we replace the actual dates with a custom <DATE> token using ```DateMatcherReplacementRule```. <br>\n",
    "This allows us to maintain the information but reduce the granularity of it."
   ],
   "id": "8878f029fc1eebfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:54:10.911294Z",
     "start_time": "2024-12-15T12:54:09.655199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextWithMatcherRules, DateMatcherReplacementRule, GameNamesMatcherReplacementRule\n",
    "import spacy\n",
    "\n",
    "text = \"Rating previous Gloomhaven to February 2017: 8.1 Rating previous to Oct 2017: 9.45 Rating previous to June 2018: 7.72. 10/10/2023\"\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "LemmatizeTextWithMatcherRules(nlp, rules=[DateMatcherReplacementRule(nlp.vocab), ]).process(text)"
   ],
   "id": "697c316a53bbce74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rate',\n",
       " 'previous',\n",
       " '<GAME_NAME>',\n",
       " '<DATE>',\n",
       " '8.1',\n",
       " 'rating',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '9.45',\n",
       " 'rate',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '7.72',\n",
       " '<DATE>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Delete duplicated rows\n",
    "There might, and there are, duplicate rows in our dataset. These are filtered out by the ```PreProcessingService``` after each step of processing. <br>\n",
    "It subsets on the original comment and game_id."
   ],
   "id": "c32d531102eac81d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch Process",
   "id": "eb8ea8a516f6e333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:58:11.819122Z",
     "start_time": "2024-12-17T19:58:11.442269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\"\n",
    "# Our known game names.\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']"
   ],
   "id": "651e810f292f936b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:58:12.607274Z",
     "start_time": "2024-12-17T19:58:12.598273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specially tailored possible cases\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "print(len(game_names))"
   ],
   "id": "eb92bc2ea9925397",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25901\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This pre-processing might not be perfect BUT it is good enough and probably a step in the right direction. <br>\n",
    "A complete model or well thought way to recognize board games is desirable but a long task on its own."
   ],
   "id": "b9b685dbf42c8041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:59:33.958259Z",
     "start_time": "2024-12-17T19:58:13.906547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import swifter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # We use small as we don't need anything over the top.\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()"
   ],
   "id": "21975e4afdf3ecb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25901 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bf935d2c22741f0aae9954ab60f085a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T19:59:37.287219Z",
     "start_time": "2024-12-17T19:59:33.977205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "default_pipeline = PreProcessingService.default_pipeline(\"../data/processed-dataset/default\")\n",
    "full_pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")"
   ],
   "id": "9c4393244a12e2c1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will create these datasets:\n",
    "- ```default_pipeline```: 64k, 64k-longest, 256k\n",
    "- ```full_pipeline```: 256k and 256k-longest\n",
    "\n",
    "This is under the assumption that the more data yeild better models."
   ],
   "id": "5b39a6cfdbfe8e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T20:00:03.336113Z",
     "start_time": "2024-12-17T19:59:37.291224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset_sampler import ConsumingDatasetSampler, BggDatasetRandomBalancedSampler, BggDatasetLongestSampler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetGeneration:\n",
    "    pipeline: PreProcessingService\n",
    "    target_size: int\n",
    "    sampler: ConsumingDatasetSampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        # For a rapid unpacking of the object\n",
    "        return iter((self.pipeline, self.target_size, self.sampler))\n",
    "\n",
    "\n",
    "combinations: [DatasetGeneration] = [\n",
    "    DatasetGeneration(default_pipeline, 64000, BggDatasetRandomBalancedSampler(16000, corpus_file, random_state)),\n",
    "    DatasetGeneration(full_pipeline, 64000, BggDatasetRandomBalancedSampler(16000, corpus_file, random_state)),\n",
    "    DatasetGeneration(full_pipeline, 64000, BggDatasetLongestSampler(16000, corpus_file, random_state)),\n",
    "    # We also try with more data. For now, we suppose that the full_pipeline with longer comments yield better results\n",
    "    # so we preprocess this. Tests on previous datasets lead to our choice for the bigger dataset composition.\n",
    "    DatasetGeneration(full_pipeline, 256000, BggDatasetRandomBalancedSampler(64000, corpus_file, random_state)),\n",
    "    DatasetGeneration(full_pipeline, 256000, BggDatasetLongestSampler(64000, corpus_file, random_state)),\n",
    "]"
   ],
   "id": "887cccead51b9810",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T22:20:21.077991Z",
     "start_time": "2024-12-17T20:00:03.344966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('We will generate a total of:', len(combinations), ' datasets')\n",
    "for combination in combinations:\n",
    "    pipeline, target_size, sampler = combination\n",
    "    longest_affix = \"_longest\" if type(sampler) is BggDatasetLongestSampler else \"\"\n",
    "\n",
    "    name = f\"{int(target_size / 1000)}k{longest_affix}\"\n",
    "    print(\"Generated dataset will be stored in file of prefix: \" + name)\n",
    "    file = pipeline.pre_process_corpus(combination.target_size, sampler, name)\n",
    "\n",
    "    print(f\"Generated dataset in file: {file}\")"
   ],
   "id": "a30da8bdb2f2645",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will generate a total of: 5  datasets\n",
      "Generated dataset will be stored in file of prefix: 64k\n",
      "I have a total of 2220 games with reviews. We take 8 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99a596fb052241678937553593c1755c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 8 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abfec918b770496e85853ba16cf8cc6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/default/64k.preprocessed.csv\n",
      "Generated dataset will be stored in file of prefix: 64k\n",
      "I have a total of 2220 games with reviews. We take 8 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38a0573792754b22b20d157f899786f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 8 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17759 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec4df4d515b45c696b7d05d98639242"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/64k.preprocessed.csv\n",
      "Generated dataset will be stored in file of prefix: 64k_longest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:286: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch[\"original_text\"] = batch[\"comments\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64d1d8712a3e4f49b88bd600386abb71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch[\"comments\"] = batch[\"comments\"].swifter.apply(self.pre_process)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/64k_longest.preprocessed.csv\n",
      "Generated dataset will be stored in file of prefix: 256k\n",
      "I have a total of 2220 games with reviews. We take 29 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6735ee19f9f042218de044c7bb6fb731"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a total of 2220 games with reviews. We take 29 reviews per game.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64378 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96444077be7c497cae2960c25ccbb61f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/256k.preprocessed.csv\n",
      "Generated dataset will be stored in file of prefix: 256k_longest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:286: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch[\"original_text\"] = batch[\"comments\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a591afd699a6438f9692347216f516df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch[\"comments\"] = batch[\"comments\"].swifter.apply(self.pre_process)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset in file: ../data/processed-dataset/full/256k_longest.preprocessed.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See how the dataset changed:",
   "id": "d1ac6ed6895a43c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset Definition\n",
    "To train the model we require a way to get elements of our dataset. ```torch``` provides a way to do this by defining a custom ```Dataset``` class. <br>\n",
    "This class and later loaded into a ```DataLoader``` that will provide the batches of data to the model."
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to generate valid inputs for the model we have to give a numerical representation to our data. <br>\n",
    "In order to do so we use a ```WordEmbedding``` model that will give us the dictionary of the recognized words (The embeddings will be generated inside the model). <br>"
   ],
   "id": "97c7e216c29993ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vocab size?\n",
    "In an ideal world the words in our dictionary are mapped 1 to 1.<br>\n",
    "There are two problems with this approach:\n",
    "- It might not be feasible\n",
    "- We might be introducing too much noise for words that occur in few cases\n",
    "\n",
    "Let's still see if it is generally feasible:"
   ],
   "id": "2706cc7b5e030f4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:34.885250Z",
     "start_time": "2024-12-19T16:45:30.929380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.utils import LoadCorpusUtility\n",
    "\n",
    "# Just to see if it is feasible even if we won't be going for all the words at the end.\n",
    "utility = LoadCorpusUtility(min_word_count=0)\n",
    "\n",
    "corpora = [\n",
    "    dict(file=\"../data/processed-dataset/default/64k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/64k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/256k.preprocessed.csv\"),\n",
    "    dict(file=\"../data/processed-dataset/full/256k_longest.preprocessed.csv\"),\n",
    "]"
   ],
   "id": "a3bfc6807da9abe4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T15:55:06.146836Z",
     "start_time": "2024-12-19T15:54:47.649748Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/69200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c63382139dc74bd3abb293c69974597f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/default/64k.preprocessed.csv.\n",
      "We have a total of 23408 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80318 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0964505753f74470a6f44d37b351a2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/64k.preprocessed.csv.\n",
      "We have a total of 27473 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/288651 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "993834e046ec4917811dddfac778ca92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/256k.preprocessed.csv.\n",
      "We have a total of 55288 unique words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/736608 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd879fc23dae4cb590995800e7307e4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For corpus file: ../data/processed-dataset/full/256k_longest.preprocessed.csv.\n",
      "We have a total of 91499 unique words.\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "\n",
    "for corpus in corpora:\n",
    "    dictionary = utility.make_corpus_dictionary(corpus['file'])\n",
    "    corpus[\"full_dictionary\"] = dictionary\n",
    "    print(f\"For corpus file: {corpus['file']}.\\nWe have a total of {len(dictionary)} unique words.\")"
   ],
   "id": "76ec020e47caae34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try for the 64k full dataset:",
   "id": "7e51c7b835149cb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:37:00.520196Z",
     "start_time": "2024-12-19T16:36:59.459694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.embeddings import WordEmbedding\n",
    "\n",
    "emb_model = WordEmbedding(\n",
    "    corpus_loader_utility=utility, embedding_size=128,\n",
    "    target_model_file='../output/256k-full-longest.embeddings.model',\n",
    "    corpus_file=corpora[3]['file'], min_word_count=1\n",
    ")\n",
    "\n",
    "len(emb_model.get_vocab())"
   ],
   "id": "bc279e7d0b233f64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ..\\output\\256k-full-longest.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': '..\\\\output\\\\256k-full-longest.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from ..\\output\\256k-full-longest.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from ..\\output\\256k-full-longest.embeddings.model.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from ..\\output\\256k-full-longest.embeddings.model.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': '..\\\\output\\\\256k-full-longest.embeddings.model', 'datetime': '2024-12-19T17:37:00.515183', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91499"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the longest dataset it just took us 30s. It is feasible on the computational side. <br>\n",
    "We still want to limit the noise of our dataset so we allow only words that occur with a certain frequency threshold. We use, as proposed when the paper was published, a ```min_word_count=5```.\n",
    "\n",
    "> We might want to test different values of min_word_count and see how it affects the model"
   ],
   "id": "3978c0634b637776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:35.731683Z",
     "start_time": "2024-12-19T16:45:35.164724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.embeddings import WordEmbedding\n",
    "\n",
    "emb_model = WordEmbedding(\n",
    "    corpus_loader_utility=LoadCorpusUtility(min_word_count=4), embedding_size=128,\n",
    "    target_model_file='../output/64k-full.embeddings.model',\n",
    "    corpus_file=corpora[1]['file'], min_word_count=1\n",
    ")\n",
    "\n",
    "vocabulary = emb_model.get_vocab()"
   ],
   "id": "aa387a7c57d647b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ..\\output\\64k-full.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': '..\\\\output\\\\64k-full.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from ..\\output\\64k-full.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': '..\\\\output\\\\64k-full.embeddings.model', 'datetime': '2024-12-19T17:45:35.729095', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:30:01.249587Z",
     "start_time": "2024-12-19T16:30:01.244771Z"
    }
   },
   "cell_type": "code",
   "source": "len(vocabulary)  # Is it a small vocabulary?",
   "id": "797f235b0c7449af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6790"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositiveNegativeCommentGeneratorDataset\n",
    "Gives a sample and also returns some negative samples for contrastive learning. <br>\n"
   ],
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:38.592733Z",
     "start_time": "2024-12-19T16:45:37.566640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(corpora[1]['file'], vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file: ../data/processed-dataset/full/64k.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80318 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "722b38a15ac54bbe80628464acfbfff9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  206 . The limit is set to 256 tokens.\n",
      "Padding sequences to length (256).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:45:40.406056Z",
     "start_time": "2024-12-19T16:45:40.402536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lazy_dataloader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "id": "6dab4b179d654a7a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:46:09.725041Z",
     "start_time": "2024-12-19T16:46:09.717526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 11  # A random index to show content and \n",
    "print(\n",
    "    f\"Sentence at index {i} original text is: `{ds.get_text_sentence(i)}`\\n \"\n",
    "    f\"It's numeric representation:\\n {ds[i][0][0]}\"\n",
    ")"
   ],
   "id": "8da3fa5e231920ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 11 original text is: `play twice Grune`\n",
      " It's numeric representation:\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2 516   1]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sequence length truncation\n",
    "The model will be trained on sequences of fixed length. <br>\n",
    "The chosen length must be reasonable, we can't just pad everything out for the same of it. <br>\n",
    "\n",
    "We want that the top 95% of the reviews are not truncated. <br>"
   ],
   "id": "2ac5649fbecf63c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We have 137 of the 50461 total reviews that are bigger than 256 tokens.\n",
    "# This is less than 1% of the total reviews. We can truncate."
   ],
   "id": "563f83c8769b8a6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:49:02.913425Z",
     "start_time": "2024-12-19T16:49:01.893919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "# We can go for little sequences to reduce overhead of the model. Most of our sentences are quite short.\n",
    "# On this dataset (64k-full 20 is enough).\n",
    "ds = PositiveNegativeCommentGeneratorDataset(corpora[1]['file'], vocabulary, 10, max_seq_length=20)"
   ],
   "id": "e65999b17f3b8d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file: ../data/processed-dataset/full/64k.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80318 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab04b909b7cb4634a5e6381f5aab8a00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  206 . The limit is set to 20 tokens.\n",
      "We loose information on 1496 points.This is 1.8625961801837696% of the dataset.\n",
      "Padding sequences to length (206).\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# todo Salva parametri di \"studio\" in file .ini per ogni dataset confg.",
   "id": "aa5a53d5b26d559c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
