{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T12:55:26.109817Z",
     "start_time": "2024-12-15T12:55:26.105855Z"
    }
   },
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "random_state = 281997"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:49.892730Z",
     "start_time": "2024-12-15T12:41:49.889301Z"
    }
   },
   "cell_type": "code",
   "source": "# todo rewrite everyhting down here. Sampling is now part of processing starting a corpus file",
   "id": "a3ee9e064f92c300",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Subsample the data\n",
    "We should limit the number of reviews, how many? Let's look at some case studies:\n",
    "\n",
    "- Amazon Product Reviews\n",
    "Size: Varies by category, but subsets of 5,000 to 20,000 reviews are common.\n",
    "- Yelp Dataset\n",
    "Size: Typically, 8,000 to 15,000 reviews are used in research for unsupervised aspect extraction.\n",
    "- TripAdvisor Reviews\n",
    "Size: Around 5,000 to 10,000 reviews in unsupervised experiments.\n",
    "\n",
    "For unsupervised learning, 5,000â€“10,000 reviews is a reasonable starting point for recognizing 6 aspects. <br>\n",
    "More reviews may improve diversity and robustness but come with increased computational costs.\n",
    "\n",
    "\n"
   ],
   "id": "3320e5f9231d0f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:18:08.530814Z",
     "start_time": "2024-12-16T14:18:08.526812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\""
   ],
   "id": "a22206e9f1313532",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special Scenario: Kickstarter\n",
    "Many reviews on BGG reference the Kickstarter campaigns of the games. <br>\n",
    "Most of these reviews are not informative and are not useful for training the model."
   ],
   "id": "6335025923ce881c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:18:16.928672Z",
     "start_time": "2024-12-16T14:18:11.717452Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(corpus_file)"
   ],
   "id": "d6928c901ad983df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How Many comments contain Kickstarter?\n",
    "Let's measure it! And while at it check how many of these are short comments:"
   ],
   "id": "266cadd4c010a7bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:35:53.640722Z",
     "start_time": "2024-12-16T14:35:50.492164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kickstarter_subset = dataset[dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "print(\n",
    "    f\"The subset is {len(kickstarter_subset) / len(dataset) * 100}% of \"\n",
    "    f\"the original with a total of {len(kickstarter_subset)} comments.\"\n",
    ")"
   ],
   "id": "20c396e5b00ccef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subset is 1.619698530100178% of the original with a total of 34600 comments.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:36:21.755560Z",
     "start_time": "2024-12-16T14:36:21.694972Z"
    }
   },
   "cell_type": "code",
   "source": "kickstarter_counts = (kickstarter_subset[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()",
   "id": "9e87ed48892647b8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:04.489192Z",
     "start_time": "2024-12-16T14:36:58.495575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = dataset[~dataset[\"comments\"].str.contains(\"kickstarter|kickstarted|kickstart\", case=False)]\n",
    "counts = (ds[\"comments\"].apply(lambda x: len(x.split(\" \")) > 15)).value_counts()"
   ],
   "id": "ccd9591d38d03896",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:37:06.128123Z",
     "start_time": "2024-12-16T14:37:06.125028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"We loose a total of: {kickstarter_counts.get(True) / (kickstarter_counts.get(True) + counts.get(True)) * 100}% possible extractions from the dataset. \\nLess than 1.1, we can just ignore Kickstarter comments\"\n",
    ")"
   ],
   "id": "d054bb1a376db041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We loose a total of: 1.0276979373944586% possible extractions from the dataset. \n",
      "Less than 1.1, we can just ignore Kickstarter comments\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:38:03.843450Z",
     "start_time": "2024-12-16T14:37:57.295257Z"
    }
   },
   "cell_type": "code",
   "source": "ds.to_csv(\"../data/corpus.csv\", index=False)",
   "id": "618d06513fce2184",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How many times are game titles referenced in the corpus? Let's see!\n",
   "id": "511e19578cae31b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.398981400Z",
     "start_time": "2024-12-14T16:50:09.661053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "# As we use regex pattern to check we might have some problems with some game names.\n",
    "# These include cases like: [kosmopoli:t], [redacted], or **, or ???. They are only 9 games.\n",
    "# This should not change the results of our inspections too dramatically especially because they are not as popular as other games.\n",
    "game_names = game_names.drop([3011, 6800, 13330, 14764, 19280, 20312, 21764, 21796, 25651])\n",
    "\n",
    "print(f\"We have a total of {len(game_names)} games.\")\n",
    "match_string = \"|\".join(game_names)"
   ],
   "id": "5a061b08726d4845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 25890 games.\n"
     ]
    }
   ],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.400980900Z",
     "start_time": "2024-12-14T16:50:25.255940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We do match case for those unfortunate cases where game names are actually common use terms like Risk, Get Lucky etc...\n",
    "# I prefer to underestimate than overestimate in this case as it seems the wisest of the two approaches.\n",
    "game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]"
   ],
   "id": "c5c56b170b318d52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacop\\AppData\\Local\\Temp\\ipykernel_24384\\3484882493.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  game_named_subset = dataset[dataset[\"comments\"].str.contains(match_string)]\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.402979900Z",
     "start_time": "2024-12-14T23:50:35.789468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"A total {len(dataset) - len(game_named_subset)} games have no reference to game names. This is {len(game_named_subset) / len(dataset) * 100}%\")  #These many games contain game references reviews."
   ],
   "id": "bc0015f502636892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total 864815 games have no reference to game names. This is 59.51619698530101%\n"
     ]
    }
   ],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A total of 864815 reference game names at least once in the comment. <br> Replacing those with the \\<GAME_NAME> token might be beneficial to reduce noise in the data",
   "id": "3f91cf6b225bff4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The downloaded information from the BGG API might not be informative, faulty or bloated with useless information. <br>\n",
    "In order to avoid this we apply some pre-processing steps in order to filter out information we don't need, that may be entire records or some of the \n",
    "text inside a line.\n",
    "\n",
    "During the process we already make the tokenization and stemming of the text using the ```spacy```\n"
   ],
   "id": "dd77baf7691d9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Some parts of torch that are used by Spacy are deprecated, we can ignore them \n",
    "# (The new 3.8 Spacy has some little issues, so we keep it like it is for now)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "id": "64a698bac2ddfbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Spacy\n",
    "To download the model and use it with spacy:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "id": "ee94d5b53e9a4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Best compromise between accuracy and speed\n",
    "model = spacy.load(\"en_core_web_md\")"
   ],
   "id": "a27a63bb78a7956c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PreProcessingService\n",
    "Class that holds the process to clean the text and produce a stemmed corpus. <br/> This will then be persisted in a file to avoid re-processing the same data."
   ],
   "id": "1e98af23ba814cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:07:40.544219Z",
     "start_time": "2024-12-16T22:07:21.159935Z"
    }
   },
   "cell_type": "code",
   "source": "from core.pre_processing import CleanTextRule",
   "id": "8d6e62a1a3aada51",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.407980200Z",
     "start_time": "2024-12-09T14:34:22.946931Z"
    }
   },
   "cell_type": "code",
   "source": "demo_text = \"This is a demo text. Isn't Root just an amazing game? I love it!\"",
   "id": "2a4b2f815ea5d707",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BGG noise removal\n",
    "BGG comments can carry metadata such as images and some pseudo-html tags. <br>\n",
    "To avoid processing those we simply remove them applying two regexes:"
   ],
   "id": "d1e697b1e80ef34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.408982Z",
     "start_time": "2024-12-09T14:35:14.660540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As defined in the PreProcessingService\n",
    "clean_tags_regex = r\"(?i)\\[(?P<tag>[A-Z]+)\\].*?\\[/\\1\\]\"\n",
    "keep_tag_content_regex = r\"(?i)\\[(?P<tag>[a-z]+)(=[^\\]]+)?\\](.*?)\\[/\\1\\]\""
   ],
   "id": "d8e102ce5c62c959",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CleanTextRule(clean_tags_regex).process(\n",
    "    \"This is a test for processing [IMG]https://cf.geekdo-static.com/mbs/mb_5855_0.gif[/IMG] as content\"\n",
    ")"
   ],
   "id": "e85f6b511ed108c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "CleanTextRule(keep_tag_content_regex, r'\\3').process(\"This is a test for processing [b=323]bold[/b] as content\")",
   "id": "4f22f395bed61768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Language detection\n",
    "While it of course would be amazing to have a model with multiple languages support, we are focusing on English. <br>\n",
    "To filter out foreign languages we use the ```langdetect``` library."
   ],
   "id": "13ce78a96095cb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fast_langdetect import detect\n",
    "\n",
    "german_sentence = \"Naja, ich finde die Siedler von Catan immer noch besser\"\n",
    "print(f\"For the demo sentence: \\\"{demo_text}\\\" we detected: {detect(demo_text)['lang']}\")\n",
    "print(f\"For the demo sentence: \\\"{german_sentence}\\\" we detected: {detect(german_sentence)['lang']}\")"
   ],
   "id": "c83a85ac0f4dc4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:41:52.415328100Z",
     "start_time": "2024-12-14T23:54:30.156163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pre_processing import FilterLanguageRule\n",
    "\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"Wir hatten heute viel spass\"))\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"We had lots of fun today\"))"
   ],
   "id": "2585199cc9cea657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir hatten heute viel spass\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 288
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and lemmatization\n",
    "Using ```spacy``` we tokenize the text and then we lemmatize it. <br>"
   ],
   "id": "9d09de9d83f7803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextRule\n",
    "\n",
    "LemmatizeTextRule().process(demo_text)  # (Should be considered private)"
   ],
   "id": "dd3379b4e9a69b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove too narrow texts\n",
    "Comments (reviews) that are too short might not be informative. <br>\n",
    "We already remove stopwords and punctuation, so we can filter out comments that are too short but we better set a reasonable threshold (not too high). This step is done by the PreProcessingService aswell."
   ],
   "id": "2f3b1f7ff4e3d626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pre_processing import ShortTextFilterRule\n",
    "\n",
    "ShortTextFilterRule(4).process(['this', 'is', 'short'])"
   ],
   "id": "bd43f68840b97cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Remove Dates\n",
    "I believe dates can be good information but not if too specific. Thus, we replace the actual dates with a custom <DATE> token using ```DateMatcherReplacementRule```. <br>\n",
    "This allows us to maintain the information but reduce the granularity of it."
   ],
   "id": "8878f029fc1eebfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:54:10.911294Z",
     "start_time": "2024-12-15T12:54:09.655199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextWithMatcherRules, DateMatcherReplacementRule, GameNamesMatcherReplacementRule\n",
    "import spacy\n",
    "\n",
    "text = \"Rating previous Gloomhaven to February 2017: 8.1 Rating previous to Oct 2017: 9.45 Rating previous to June 2018: 7.72. 10/10/2023\"\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "LemmatizeTextWithMatcherRules(nlp, rules=[DateMatcherReplacementRule(nlp.vocab), ]).process(text)"
   ],
   "id": "697c316a53bbce74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rate',\n",
       " 'previous',\n",
       " '<GAME_NAME>',\n",
       " '<DATE>',\n",
       " '8.1',\n",
       " 'rating',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '9.45',\n",
       " 'rate',\n",
       " 'previous',\n",
       " '<DATE>',\n",
       " '7.72',\n",
       " '<DATE>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Delete duplicated rows\n",
    "There might, and there are, duplicate rows in our dataset. These are filtered out by the ```PreProcessingService``` after each step of processing. <br>\n",
    "It subsets on the original comment and game_id."
   ],
   "id": "c32d531102eac81d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch Process",
   "id": "eb8ea8a516f6e333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:55:35.003419Z",
     "start_time": "2024-12-15T12:55:34.636628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\"\n",
    "# Our known game names.\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']"
   ],
   "id": "651e810f292f936b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:55:42.687997Z",
     "start_time": "2024-12-15T12:55:42.683648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specially tailored possible cases\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "print(len(game_names))"
   ],
   "id": "eb92bc2ea9925397",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This pre-processing might not be perfect BUT it is good enough and probably a step in the right direction. <br>\n",
    "A complete model or well thought way to recognize board games is desirable but a long task on its own."
   ],
   "id": "b9b685dbf42c8041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T12:57:09.805971Z",
     "start_time": "2024-12-15T12:55:50.930458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import swifter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # We use small as we don't need anything over the top.\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()"
   ],
   "id": "21975e4afdf3ecb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25901 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1a45f960e3b4937bf1dc67271d46b6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:28:55.668589Z",
     "start_time": "2024-12-16T23:28:51.794384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "default_pipeline = PreProcessingService.default_pipeline(\"../data/processed-dataset/default\")\n",
    "full_pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")"
   ],
   "id": "9c4393244a12e2c1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_game_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpre_processing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreProcessingService\n\u001B[0;32m      3\u001B[0m default_pipeline \u001B[38;5;241m=\u001B[39m PreProcessingService\u001B[38;5;241m.\u001B[39mdefault_pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/processed-dataset/default\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m full_pipeline \u001B[38;5;241m=\u001B[39m PreProcessingService\u001B[38;5;241m.\u001B[39mfull_pipeline(\u001B[43mdocument_game_names\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/processed-dataset/full\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'document_game_names' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will create these datasets:\n",
    "- ```default_pipeline```: 64k, 64k-longest, 256k\n",
    "- ```full_pipeline```: 256k and 256k-longest\n",
    "\n",
    "This is under the assumption that the more data yeild better models."
   ],
   "id": "5b39a6cfdbfe8e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T23:28:40.072934Z",
     "start_time": "2024-12-16T23:28:40.048407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset_sampler import ConsumingDatasetSampler, BggDatasetRandomBalancedSampler, BggDatasetLongestSampler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DatasetGeneration:\n",
    "    pipeline: PreProcessingService\n",
    "    target_size: int\n",
    "    sampler: ConsumingDatasetSampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        # For a rapid unpacking of the object\n",
    "        return iter((self.pipeline, self.target_size, self.sampler))\n",
    "\n",
    "\n",
    "combinations: [DatasetGeneration] = [\n",
    "    DatasetGeneration(default_pipeline, 64000, BggDatasetRandomBalancedSampler(16000, corpus_file, random_state)),\n",
    "    DatasetGeneration(default_pipeline, 64000, BggDatasetLongestSampler(16000, corpus_file, random_state)),\n",
    "    DatasetGeneration(default_pipeline, 256000, BggDatasetRandomBalancedSampler(64000, corpus_file, random_state)),\n",
    "    DatasetGeneration(full_pipeline, 256000, BggDatasetRandomBalancedSampler(64000, corpus_file, random_state)),\n",
    "    DatasetGeneration(full_pipeline, 256000, BggDatasetLongestSampler(64000, corpus_file, random_state)),\n",
    "]"
   ],
   "id": "887cccead51b9810",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreProcessingService' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_sampler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConsumingDatasetSampler, BggDatasetRandomBalancedSampler, BggDatasetLongestSampler\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataclasses\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataclass\n\u001B[0;32m      4\u001B[0m \u001B[38;5;129;43m@dataclass\u001B[39;49m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43;01mDatasetGeneration\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mPreProcessingService\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\n",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m, in \u001B[0;36mDatasetGeneration\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;129m@dataclass\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDatasetGeneration\u001B[39;00m:\n\u001B[1;32m----> 6\u001B[0m     pipeline: \u001B[43mPreProcessingService\u001B[49m\n\u001B[0;32m      7\u001B[0m     target_size: \u001B[38;5;28mint\u001B[39m\n\u001B[0;32m      8\u001B[0m     sampler: ConsumingDatasetSampler\n",
      "\u001B[1;31mNameError\u001B[0m: name 'PreProcessingService' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for combination in combinations:\n",
    "    pipeline, target_size, sampler = combination\n",
    "    print(f\"Processing the {int(target_size / 1000)}k datasets:\")\n",
    "    longest_affix = \"_longest\" if sampler is BggDatasetLongestSampler else \"\"\n",
    "    name = f\"{int(target_size / 1000)}k{longest_affix}\"\n",
    "    file = pipeline.pre_process_corpus(combination.target_size, sampler, name)\n",
    "    print(f\"Generated dataset in file: {file}\")"
   ],
   "id": "a30da8bdb2f2645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See how the dataset changed:",
   "id": "d1ac6ed6895a43c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset Definition\n",
    "To train the model we require a way to get elements of our dataset. ```torch``` provides a way to do this by defining a custom ```Dataset``` class. <br>\n",
    "This class and later loaded into a ```DataLoader``` that will provide the batches of data to the model."
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to generate valid inputs for the model we have to give a numerical representation to our data. <br>\n",
    "In order to do so we use a ```WordEmbedding``` model that will give us the dictionary of the recognized words (The embeddings will be generated inside the model). <br>"
   ],
   "id": "97c7e216c29993ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_vocab_size = 16000\n",
    "embedding_size = 128\n",
    "target_embedding_model_file = \"./../data/word-embeddings.model\""
   ],
   "id": "6c98540db8a486b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import core.utils as utils\n",
    "import core.embeddings as embeddings\n",
    "\n",
    "# We just show how to use them\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    utils.LoadCorpusUtility(), max_vocab_size=max_vocab_size, embedding_size=embedding_size,\n",
    "    target_model_file=target_embedding_model_file, corpus_file=\"../data/processed-dataset/default/k8.preprocessed.csv\"\n",
    ")"
   ],
   "id": "f3768d3e3077cd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We require a vocabulary to map the words to indexes\n",
    "embeddings_model.load_model()\n",
    "embeddings_model.get_vocab()\n",
    "\n",
    "vocabulary = embeddings_model.model.wv.key_to_index"
   ],
   "id": "423dafa9a3a9c36b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositiveNegativeCommentGeneratorDataset\n",
    "Gives a sample and also returns some negative samples for contrastive learning. <br>\n"
   ],
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(\"./../data/corpus.preprocessed.csv\", vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lazy_dataloader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "id": "6dab4b179d654a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "i = 11  # A random index to show content and \n",
    "print(\n",
    "    f\"Sentence at index {i} original text is: `{ds.get_text_item(i)}` (Look at [comments] property for the stripped down version)\\n \"\n",
    "    f\"It's numeric representation:\\n {ds[i][0][0]}\"\n",
    ")"
   ],
   "id": "8da3fa5e231920ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sequence length truncation\n",
    "The model will be trained on sequences of fixed length. <br>\n",
    "The chosen length must be reasonable, we can't just pad everything out for the same of it. <br>\n",
    "\n",
    "We want that the top 95% of the reviews are not truncated. <br>"
   ],
   "id": "2ac5649fbecf63c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We have 137 of the 50461 total reviews that are bigger than 256 tokens.\n",
    "# This is less than 1% of the total reviews. We can truncate."
   ],
   "id": "563f83c8769b8a6d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
