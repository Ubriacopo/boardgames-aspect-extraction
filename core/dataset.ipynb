{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T13:59:27.792281Z",
     "start_time": "2024-12-10T13:59:27.787779Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "random_state = 281997"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Subsample the data\n",
    "We should limit the number of reviews, how many? Let's look at some case studies:\n",
    "\n",
    "- Amazon Product Reviews\n",
    "Size: Varies by category, but subsets of 5,000 to 20,000 reviews are common.\n",
    "- Yelp Dataset\n",
    "Size: Typically, 8,000 to 15,000 reviews are used in research for unsupervised aspect extraction.\n",
    "- TripAdvisor Reviews\n",
    "Size: Around 5,000 to 10,000 reviews in unsupervised experiments.\n",
    "\n",
    "For unsupervised learning, 5,000â€“10,000 reviews is a reasonable starting point for recognizing 6 aspects. More reviews may improve diversity and robustness but come with increased computational costs.\n",
    "\n",
    "\n"
   ],
   "id": "3320e5f9231d0f54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We decided to subsample in 5 different sizes: [16k, 32k, 64k, 128k, 256k] <br>\n",
    "Training will be done on all the datasets and we will see how the model behaves with more data. (If we are actually underfitting)"
   ],
   "id": "67707f4ce6798eef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:10:57.724358Z",
     "start_time": "2024-12-08T13:10:57.721944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File of our corpus:\n",
    "corpus_file = \"../data/corpus.csv\""
   ],
   "id": "a22206e9f1313532",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Random sampling\n",
    "We randomly take reviews, without taking into account anything special. <br>"
   ],
   "id": "19bd52310e181ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset_sampler import BggDatasetRandomBalancedSampler\n",
    "\n",
    "target_sizes = [16000, 64000, 256000]\n",
    "for size in target_sizes:\n",
    "    sampler = (BggDatasetRandomBalancedSampler(size, output_dir=\"../data/sampled-dataset\", random_state=random_state))\n",
    "    sampler.make_sample_of_data(corpus_file, f\"corpus.sampled.{int(size / 1000)}k.csv\")"
   ],
   "id": "b80e45a69c2de03f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:21:18.364050Z",
     "start_time": "2024-12-08T13:21:18.360677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_corpora = dict(\n",
    "    k16=\"../data/sampled-dataset/corpus.sampled.16k.csv\",\n",
    "    k64=\"../data/sampled-dataset/corpus.sampled.64k.csv\",\n",
    "    k256=\"../data/sampled-dataset/corpus.sampled.256k.csv\"\n",
    ")"
   ],
   "id": "30d1ddaa879ffe37",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Longest reviews sampling",
   "id": "68a3dc468105e97e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset_sampler import BggDatasetLongestSampler\n",
    "\n",
    "target_sizes = [16000, 64000, 256000]  # 4 steps distance\n",
    "for size in target_sizes:\n",
    "    sampler = (BggDatasetLongestSampler(size, output_dir=\"../data/sampled-dataset\", random_state=random_state))\n",
    "    sampler.make_sample_of_data(corpus_file, f\"corpus.longest-sampled.{int(size / 1000)}k.csv\")"
   ],
   "id": "7543d5106ea22d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:21:19.746454Z",
     "start_time": "2024-12-08T13:21:19.742455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_corpora[\"k16_longest\"] = \"../data/sampled-dataset/corpus.longest-sampled.16k.csv\"\n",
    "generated_corpora[\"k64_longest\"] = \"../data/sampled-dataset/corpus.longest-sampled.64k.csv\"\n",
    "generated_corpora[\"k256_longest\"] = \"../data/sampled-dataset/corpus.longest-sampled.256k.csv\""
   ],
   "id": "c8eb3bfa4fdb0093",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check distribution of games",
   "id": "2b9f595bf8a9ce64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:59:41.489932Z",
     "start_time": "2024-12-10T13:59:31.123082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy import displacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "id": "ced71d3015ee1b85",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = nlp(\"Root is a fantastic boardgame. It's almost as good as settlers! Do you want to play some risk?\")\n",
    "displacy.render(c)"
   ],
   "id": "7c3ec191cf0c980e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:02:15.557781Z",
     "start_time": "2024-12-10T14:00:12.567522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r = nlp(\"+fs +\")\n",
    "r[0]"
   ],
   "id": "28d25f7bde84f72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special Scenario: Kickstarter\n",
    "Many reviews on BGG reference the Kickstarter campaigns of the games. <br>\n",
    "Most of these reviews are not informative and are not useful for training the model. <br>\n",
    "\n",
    "For reviews containing ```Kickstarter``` we apply the following Heuristic:\n",
    "- If the review is short (<15 words) we remove it.\n",
    "- If it is longer we keep it."
   ],
   "id": "6335025923ce881c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Removal of 'Kickstarter' reviews\n",
    "from pre_processing import PreProcessingService, KickstarterRemovalRule\n",
    "\n",
    "ps = PreProcessingService.kickstarter_filter_pipeline(\"\")"
   ],
   "id": "b88b953d3ec3a3d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = \"Kickstarter is a great platform to launch games. The longer my review is the more likely we are going to keep it. Extraordinary.\"\n",
    "ps.pre_process(test)"
   ],
   "id": "aac3eedff2476466",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have a special pipeline for the Kickstarter removal. <br>\n",
    "For which we will generate a separate dataset for comparison to see if the quality of the data improves."
   ],
   "id": "1d60628d81e0dc0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The downloaded information from the BGG API might not be informative, faulty or bloated with useless information. <br>\n",
    "In order to avoid this we apply some pre-processing steps in order to filter out information we don't need, that may be entire records or some of the \n",
    "text inside a line.\n",
    "\n",
    "During the process we already make the tokenization and stemming of the text using the ```spacy```\n"
   ],
   "id": "dd77baf7691d9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Some parts of torch that are used by Spacy are deprecated, we can ignore them \n",
    "# (The new 3.8 Spacy has some little issues, so we keep it like it is for now)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "id": "64a698bac2ddfbaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Spacy\n",
    "To download the model and use it with spacy:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "id": "ee94d5b53e9a4b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load(\"en_core_web_md\")"
   ],
   "id": "a27a63bb78a7956c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PreProcessingService\n",
    "Class that holds the process to clean the text and produce a stemmed corpus. <br/> This will then be persisted in a file to avoid re-processing the same data."
   ],
   "id": "1e98af23ba814cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T14:34:20.202483Z",
     "start_time": "2024-12-09T14:34:20.196056Z"
    }
   },
   "cell_type": "code",
   "source": "from core.pre_processing import CleanTextRule",
   "id": "8d6e62a1a3aada51",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T14:34:22.950222Z",
     "start_time": "2024-12-09T14:34:22.946931Z"
    }
   },
   "cell_type": "code",
   "source": "demo_text = \"This is a demo text. Isn't Root just an amazing game? I love it!\"",
   "id": "2a4b2f815ea5d707",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BGG noise removal\n",
    "BGG comments can carry metadata such as images and some pseudo-html tags. <br>\n",
    "To avoid processing those we simply remove them applying two regexes:"
   ],
   "id": "d1e697b1e80ef34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T14:35:14.663514Z",
     "start_time": "2024-12-09T14:35:14.660540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As defined in the PreProcessingService\n",
    "clean_tags_regex = r\"(?i)\\[(?P<tag>[A-Z]+)\\].*?\\[/\\1\\]\"\n",
    "keep_tag_content_regex = r\"(?i)\\[(?P<tag>[a-z]+)(=[^\\]]+)?\\](.*?)\\[/\\1\\]\""
   ],
   "id": "d8e102ce5c62c959",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CleanTextRule(clean_tags_regex).process(\n",
    "    \"This is a test for processing [IMG]https://cf.geekdo-static.com/mbs/mb_5855_0.gif[/IMG] as content\")"
   ],
   "id": "e85f6b511ed108c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "CleanTextRule(keep_tag_content_regex, r'\\3').process(\"This is a test for processing [b=323]bold[/b] as content\")",
   "id": "4f22f395bed61768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another thing we found in the comments: :F::O::R::E::V::E::R::blank::K::E::E::P::E::R:",
   "id": "dd5365fa95a92667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T14:39:17.548164Z",
     "start_time": "2024-12-09T14:39:17.543331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# todo reprocess. Queste regole non le avevo all'inizio. In piÃº riduci i ds sono troppi. Non mi metto ad allenare per tutti\n",
    "CleanTextRule(\":F::O::R::E::V::E::R::blank::K::E::E::P::E::R:\").process(\n",
    "    \":F::O::R::E::V::E::R::blank::K::E::E::P::E::R: this is a test \"\n",
    ")\n",
    "CleanTextRule(\"f::o::r::e::v::e::r::blank::k::e::e::p::e::r\")"
   ],
   "id": "697ac01012281834",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test for processing [b=323]bold[/b] as content'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Language detection\n",
    "While it of course would be amazing to have a model with multiple languages support, we are focusing on English. <br>\n",
    "To filter out foreign languages we use the ```langdetect``` library."
   ],
   "id": "13ce78a96095cb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fast_langdetect import detect\n",
    "\n",
    "german_sentence = \"Naja, ich finde die Siedler von Catan immer noch besser\"\n",
    "print(f\"For the demo sentence: \\\"{demo_text}\\\" we detected: {detect(demo_text)['lang']}\")\n",
    "print(f\"For the demo sentence: \\\"{german_sentence}\\\" we detected: {detect(german_sentence)['lang']}\")"
   ],
   "id": "c83a85ac0f4dc4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import FilterLanguageRule\n",
    "\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"Wir hatten viel spass heute\"))\n",
    "print(FilterLanguageRule([\"it\", \"de\"]).process(\"We had lots of fun today\"))"
   ],
   "id": "2585199cc9cea657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and lemmatization\n",
    "Using ```spacy``` we tokenize the text and then we lemmatize it. <br>"
   ],
   "id": "9d09de9d83f7803"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import LemmatizeTextRule\n",
    "\n",
    "LemmatizeTextRule().process(demo_text)  # (Should be considered private)"
   ],
   "id": "dd3379b4e9a69b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove too narrow texts\n",
    "Comments (reviews) that are too short might not be informative. <br>\n",
    "We already remove stopwords and punctuation, so we can filter out comments that are too short but we better set a reasonable threshold (not too high). This step is done by the PreProcessingService aswell."
   ],
   "id": "2f3b1f7ff4e3d626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pre_processing import ShortTextFilterRule\n",
    "\n",
    "ShortTextFilterRule(4).process(['this', 'is', 'short'])"
   ],
   "id": "bd43f68840b97cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Batch Process",
   "id": "eb8ea8a516f6e333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:26:26.899521Z",
     "start_time": "2024-12-08T13:26:26.818107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "# Our known game names.\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']"
   ],
   "id": "651e810f292f936b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:26:27.800667Z",
     "start_time": "2024-12-08T13:26:27.794412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specially tailored possible cases\n",
    "pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)"
   ],
   "id": "eb92bc2ea9925397",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        Brass: Birmingham\n",
       "1                Pandemic Legacy: Season 1\n",
       "2                               Gloomhaven\n",
       "3                                 Ark Nova\n",
       "4        Twilight Imperium: Fourth Edition\n",
       "                       ...                \n",
       "25896                    What Do You Meme?\n",
       "25897                      Flushin' Frenzy\n",
       "25898                              Electro\n",
       "25899                                Quick\n",
       "25900                                Catan\n",
       "Length: 25901, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This pre-processing might not be perfect BUT it is good enough and probably a step in the right direction. <br>\n",
    "A complete model or well thought way to recognize board games is desirable but a long task on its own."
   ],
   "id": "b9b685dbf42c8041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:26:29.576347Z",
     "start_time": "2024-12-08T13:26:29.573678Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(game_names))",
   "id": "4de2cae5912d7b78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25899\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:27:51.879663Z",
     "start_time": "2024-12-08T13:26:32.045070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import swifter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # We use small as we don't need anything over the top.\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()"
   ],
   "id": "21975e4afdf3ecb0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25899 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dde269c61ad4d86b4a5d748c019db8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:27:59.936645Z",
     "start_time": "2024-12-08T13:27:55.184955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipelines = [\n",
    "    PreProcessingService.default_pipeline(\"../data/processed-dataset/default\"),\n",
    "    PreProcessingService.game_name_less_pipeline(document_game_names, \"../data/processed-dataset/game-name-filtered\"),\n",
    "    PreProcessingService.kickstarter_filter_pipeline_without_game_names(\n",
    "        document_game_names, \"../data/processed-dataset/kickstarter-filtered-game-name-filtered\"\n",
    "    ),\n",
    "    PreProcessingService.kickstarter_filter_pipeline_without_game_names_and_numbers(\n",
    "        document_game_names, \"../data/processed-dataset/kickstarter-filtered-game-name-filtered-no-numbers\"\n",
    "    )\n",
    "]"
   ],
   "id": "9c4393244a12e2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating game names tokenized representationL: (25899)\n",
      "Done generating... cb ready for use!\n",
      "Generating game names tokenized representationL: (25899)\n",
      "Done generating... cb ready for use!\n",
      "Generating game names tokenized representationL: (25899)\n",
      "Done generating... cb ready for use!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:10:46.623880Z",
     "start_time": "2024-12-08T13:28:03.677907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_corpora = dict()\n",
    "\n",
    "for key in generated_corpora:\n",
    "    # Generate the 4 datasets we desire.\n",
    "    print(f\"Processing the {key} datasets:\")\n",
    "    for pipeline in pipelines:\n",
    "        print(f\"Started pipeline with pipe:\\n {list(map(lambda x: x.__class__.__name__, pipeline.pipeline))}\")\n",
    "        generated_file = pipeline.pre_process_corpus(generated_corpora[key], key)\n",
    "        processed_corpora[f\"{key}_{pipeline.name.replace(\"pipeline\", \"\")}\"] = generated_file"
   ],
   "id": "c3b73028dc23d690",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the k16 datasets:\n",
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "602ea73f0091458fb0aeee27cfe2bcbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17141b169414487e8d51bb70cacfc5f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62236726b44748c1bf9f208b859f7d6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutNumbersRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/17760 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b3d19227adf4a169dfba7c89ae17a75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the k64 datasets:\n",
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "444accd6a0bf40e98eaf9b6c55c0cc9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b05a6d9d38ed44b8b17d707c85dd4ac6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e51c90cc38742a6b7af33fdf9be22e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutNumbersRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/64380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c3d5be8117e44609d642e9885c23652"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the k256 datasets:\n",
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/257520 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "620281ae2a324d45bd0b2ca8a389392d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/257520 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e009e9a86814994a4fe2e7b17219da9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/257520 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13cf5da1ecd34f8584ec2d40850cd0e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutNumbersRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/257520 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcff8e36c1f840b2bc6ea68803720c87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the k16_longest datasets:\n",
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c0dd3c36afc4ca7b1ec88468e3201ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\swifter\\swifter.py:314\u001B[0m, in \u001B[0;36mSeriesAccessor.apply\u001B[1;34m(self, func, convert_dtype, args, **kwds)\u001B[0m\n\u001B[0;32m    312\u001B[0m     sample_df \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39mapply(func, convert_dtype\u001B[38;5;241m=\u001B[39mconvert_dtype, args\u001B[38;5;241m=\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_apply(\n\u001B[1;32m--> 314\u001B[0m         np\u001B[38;5;241m.\u001B[39marray_equal(sample_df, tmp_df) \u001B[38;5;241m&\u001B[39m (\u001B[38;5;28mhasattr\u001B[39m(tmp_df, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m&\u001B[39m (sample_df\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m \u001B[43mtmp_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m),\n\u001B[0;32m    315\u001B[0m         error_message\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVectorized function sample doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match pandas apply sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pipeline \u001B[38;5;129;01min\u001B[39;00m pipelines:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarted pipeline with pipe:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m\u001B[38;5;250m \u001B[39mx:\u001B[38;5;250m \u001B[39mx\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\u001B[38;5;250m \u001B[39mpipeline\u001B[38;5;241m.\u001B[39mpipeline))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m     generated_file \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpre_process_corpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_corpora\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     processed_corpora[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpipeline\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m generated_file\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:322\u001B[0m, in \u001B[0;36mPreProcessingService.pre_process_corpus\u001B[1;34m(self, resource_file_path, name, override)\u001B[0m\n\u001B[0;32m    319\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(resource_file_path)\n\u001B[0;32m    321\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moriginal_text\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomments\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 322\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomments\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcomments\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mswifter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpre_process\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    324\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[0;32m    325\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.preprocessed.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\swifter\\swifter.py:329\u001B[0m, in \u001B[0;36mSeriesAccessor.apply\u001B[1;34m(self, func, convert_dtype, args, **kwds)\u001B[0m\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parallel_apply(func, convert_dtype, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    328\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# use pandas\u001B[39;00m\n\u001B[1;32m--> 329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pandas_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\swifter\\swifter.py:235\u001B[0m, in \u001B[0;36mSeriesAccessor._pandas_apply\u001B[1;34m(self, df, func, convert_dtype, *args, **kwds)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_bar:\n\u001B[0;32m    234\u001B[0m     tqdm\u001B[38;5;241m.\u001B[39mpandas(desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_bar_desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPandas Apply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogress_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\u001B[38;5;241m.\u001B[39mapply(func, convert_dtype\u001B[38;5;241m=\u001B[39mconvert_dtype, args\u001B[38;5;241m=\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\tqdm\\std.py:917\u001B[0m, in \u001B[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001B[1;34m(df, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;66;03m# Apply the provided function (in **kwargs)\u001B[39;00m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001B[39;00m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_function\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    919\u001B[0m     t\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\tqdm\\std.py:912\u001B[0m, in \u001B[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    907\u001B[0m     \u001B[38;5;66;03m# update tbar correctly\u001B[39;00m\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001B[39;00m\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;66;03m# on the first column/row to decide whether it can\u001B[39;00m\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001B[39;00m\n\u001B[0;32m    911\u001B[0m     t\u001B[38;5;241m.\u001B[39mupdate(n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m<\u001B[39m t\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 912\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:301\u001B[0m, in \u001B[0;36mPreProcessingService.pre_process\u001B[1;34m(self, entry)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpre_process\u001B[39m(\u001B[38;5;28mself\u001B[39m, entry: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 301\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrule\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextensive_logging\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mentry\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    305\u001B[0m         logging\u001B[38;5;241m.\u001B[39merror(e)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:301\u001B[0m, in \u001B[0;36mPreProcessingService.pre_process.<locals>.<lambda>\u001B[1;34m(t, rule)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpre_process\u001B[39m(\u001B[38;5;28mself\u001B[39m, entry: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 301\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m t, rule: \u001B[43mrule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextensive_logging\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpipeline, entry)\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    305\u001B[0m         logging\u001B[38;5;241m.\u001B[39merror(e)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\pre_processing.py:131\u001B[0m, in \u001B[0;36mLemmatizeTextRule.process\u001B[1;34m(self, entry, extensive_logging)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m entry \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;66;03m# todo look at shape of this list. It should be a list of strings.\u001B[39;00m\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess(e) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m entry]\n\u001B[1;32m--> 131\u001B[0m text_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m text_tokens \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_invalid_token(token)]\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\spacy\\language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:23:18.123689Z",
     "start_time": "2024-12-08T16:17:00.886693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Not that tricky but we know that longest take the longest sequence words. I can only elaborate the 256k as\n",
    "# others are subset of that one. So I do it like that:\n",
    "for pipeline in pipelines:\n",
    "    print(f\"Started pipeline with pipe:\\n {list(map(lambda x: x.__class__.__name__, pipeline.pipeline))}\")\n",
    "    generated_file = pipeline.pre_process_corpus(generated_corpora[\"k256_longest\"], \"k256_longest\")\n",
    "    processed_corpora[f\"k256_longest_{pipeline.name.replace(\"pipeline\", \"\")}\"] = generated_file\n",
    "\n",
    "    for i in [16000, 64000]:\n",
    "        pd.read_csv(processed_corpora[f\"k256_longest_{pipeline.name.replace(\"pipeline\", \"\")}\"])[0:i].to_csv(\n",
    "            f\"{pipeline.target_path}/k{int(i / 1000)}_longest.preprocessed.csv\")"
   ],
   "id": "89f76d7753b6dce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/256000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b7749cb788c4891a8d804a1341e0b8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/256000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be8876140bc0481b8b028812e31f791d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutGameNamesRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/256000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e269243733040d6bfc8cca3034edeee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pipeline with pipe:\n",
      " ['CleanTextRule', 'CleanTextRule', 'KickstarterRemovalRule', 'FilterLanguageRule', 'LemmatizeTextWithoutNumbersRule', 'ShortTextFilterRule', 'ListToTextRegenerationRule']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/256000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eda31b0a28d849ddb8753e3c75c6dd6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"We generated: {processed_corpora}\")",
   "id": "16660e694b8541e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See how the dataset changed:",
   "id": "d1ac6ed6895a43c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"We lost a total of {len(pd.read_csv(generated_corpora[\"k8\"])) - len(pd.read_csv(\"../data/processed-dataset/default/8k.csv\"))} reviews\")"
   ],
   "id": "edda0efd2ab7a66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Delete duplicated rows",
   "id": "c32d531102eac81d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:37:43.534792Z",
     "start_time": "2024-12-09T15:36:45.600661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "folders = [\n",
    "    \"../data/processed-dataset/default\",\n",
    "    \"../data/processed-dataset/game-name-filtered\",\n",
    "    \"../data/processed-dataset/kickstarter-filtered-game-name-filtered\",\n",
    "    \"../data/processed-dataset/kickstarter-filtered-game-name-filtered-no-numbers\",\n",
    "]\n",
    "\n",
    "generated_docs = [\n",
    "    \"k16.preprocessed.csv\",\n",
    "    \"k16_longest.preprocessed.csv\",\n",
    "    \"k64.preprocessed.csv\",\n",
    "    \"k64_longest.preprocessed.csv\",\n",
    "    \"k256.preprocessed.csv\",\n",
    "    \"k256_longest.preprocessed.csv\",\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"\\nCleaning folder {folder}\")\n",
    "    for doc in generated_docs:\n",
    "        file_path = f\"{folder}/{doc}\"\n",
    "        data = pd.read_csv(file_path)\n",
    "        data.drop_duplicates(subset=[\"original_text\", \"game_id\"]).to_csv(file_path, mode=\"w\", header=True, index=False)\n",
    "        print(f\"Cleaned {doc}!\")"
   ],
   "id": "377fc2668caaf38d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning folder ../data/processed-dataset/default\n",
      "Cleaned k16.preprocessed.csv!\n",
      "Cleaned k16_longest.preprocessed.csv!\n",
      "Cleaned k64.preprocessed.csv!\n",
      "Cleaned k64_longest.preprocessed.csv!\n",
      "Cleaned k256.preprocessed.csv!\n",
      "Cleaned k256_longest.preprocessed.csv!\n",
      "\n",
      "Cleaning folder ../data/processed-dataset/game-name-filtered\n",
      "Cleaned k16.preprocessed.csv!\n",
      "Cleaned k16_longest.preprocessed.csv!\n",
      "Cleaned k64.preprocessed.csv!\n",
      "Cleaned k64_longest.preprocessed.csv!\n",
      "Cleaned k256.preprocessed.csv!\n",
      "Cleaned k256_longest.preprocessed.csv!\n",
      "\n",
      "Cleaning folder ../data/processed-dataset/kickstarter-filtered-game-name-filtered\n",
      "Cleaned k16.preprocessed.csv!\n",
      "Cleaned k16_longest.preprocessed.csv!\n",
      "Cleaned k64.preprocessed.csv!\n",
      "Cleaned k64_longest.preprocessed.csv!\n",
      "Cleaned k256.preprocessed.csv!\n",
      "Cleaned k256_longest.preprocessed.csv!\n",
      "\n",
      "Cleaning folder ../data/processed-dataset/kickstarter-filtered-game-name-filtered-no-numbers\n",
      "Cleaned k16.preprocessed.csv!\n",
      "Cleaned k16_longest.preprocessed.csv!\n",
      "Cleaned k64.preprocessed.csv!\n",
      "Cleaned k64_longest.preprocessed.csv!\n",
      "Cleaned k256.preprocessed.csv!\n",
      "Cleaned k256_longest.preprocessed.csv!\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset Definition\n",
    "To train the model we require a way to get elements of our dataset. ```torch``` provides a way to do this by defining a custom ```Dataset``` class. <br>\n",
    "This class and later loaded into a ```DataLoader``` that will provide the batches of data to the model."
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to generate valid inputs for the model we have to give a numerical representation to our data. <br>\n",
    "In order to do so we use a ```WordEmbedding``` model that will give us the dictionary of the recognized words (The embeddings will be generated inside the model). <br>"
   ],
   "id": "97c7e216c29993ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_vocab_size = 16000\n",
    "embedding_size = 128\n",
    "target_embedding_model_file = \"./../data/word-embeddings.model\""
   ],
   "id": "6c98540db8a486b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import core.utils as utils\n",
    "import core.embeddings as embeddings\n",
    "\n",
    "# We just show how to use them\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    utils.LoadCorpusUtility(), max_vocab_size=max_vocab_size, embedding_size=embedding_size,\n",
    "    target_model_file=target_embedding_model_file, corpus_file=\"../data/processed-dataset/default/k8.preprocessed.csv\"\n",
    ")"
   ],
   "id": "f3768d3e3077cd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We require a vocabulary to map the words to indexes\n",
    "embeddings_model.load_model()\n",
    "embeddings_model.get_vocab()\n",
    "\n",
    "vocabulary = embeddings_model.model.wv.key_to_index"
   ],
   "id": "423dafa9a3a9c36b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositiveNegativeCommentGeneratorDataset\n",
    "Gives a sample and also returns some negative samples for contrastive learning. <br>\n"
   ],
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(\"./../data/corpus.preprocessed.csv\", vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lazy_dataloader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "id": "6dab4b179d654a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "i = 11  # A random index to show content and \n",
    "print(\n",
    "    f\"Sentence at index {i} original text is: `{ds.get_text_item(i)}` (Look at [comments] property for the stripped down version)\\n \"\n",
    "    f\"It's numeric representation:\\n {ds[i][0][0]}\"\n",
    ")"
   ],
   "id": "8da3fa5e231920ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sequence length truncation\n",
    "The model will be trained on sequences of fixed length. <br>\n",
    "The chosen length must be reasonable, we can't just pad everything out for the same of it. <br>\n",
    "\n",
    "We want that the top 95% of the reviews are not truncated. <br>"
   ],
   "id": "2ac5649fbecf63c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We have 137 of the 50461 total reviews that are bigger than 256 tokens.\n",
    "# This is less than 1% of the total reviews. We can truncate."
   ],
   "id": "563f83c8769b8a6d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
