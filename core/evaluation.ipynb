{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:16.096252Z",
     "start_time": "2024-12-05T14:57:16.094215Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a24a0908495f4bc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kickstarter Less Dataset (256k)\n",
   "id": "d52c676c461ef740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.255056Z",
     "start_time": "2024-12-05T14:57:16.122454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train import AbaeModelConfiguration\n",
    "from core.train import AbaeModelManager\n",
    "\n",
    "config = AbaeModelConfiguration(\n",
    "    corpus_file=\"../data/corpus.preprocessed.kickstarter_removed.256k.csv\",\n",
    "    model_name=\"abae.kickstarter_removed.256k\", aspect_size=16, max_vocab_size=40000,\n",
    ")\n",
    "\n",
    "manager = AbaeModelManager(config)"
   ],
   "id": "4af0ba69460037d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "INFO:gensim.utils:loading Word2Vec object from output/abae.kickstarter_removed.256k.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'output/abae.kickstarter_removed.256k.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from output/abae.kickstarter_removed.256k.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'output/abae.kickstarter_removed.256k.embeddings.model', 'datetime': '2024-12-05T15:57:19.253506', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.317412Z",
     "start_time": "2024-12-05T14:57:19.255868Z"
    }
   },
   "cell_type": "code",
   "source": "inference_model = manager.prepare_evaluation_model()",
   "id": "a615c99f728276ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.325926Z",
     "start_time": "2024-12-05T14:57:19.318053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "word_emb = inference_model.get_layer('word_embedding').get_weights()[0]\n",
    "word_emb = torch.from_numpy(word_emb)\n",
    "\n",
    "word_emb.shape"
   ],
   "id": "5319f9c68c632e39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25055, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.328501Z",
     "start_time": "2024-12-05T14:57:19.326851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aspect_embeddings = inference_model.get_layer('weighted_aspect_emb').W\n",
    "vocab_inv = manager.embedding_model.model.wv.index_to_key"
   ],
   "id": "b729faf80b9d3b09",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.333789Z",
     "start_time": "2024-12-05T14:57:19.329026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.evaluation import normalize_embedding_matrix\n",
    "\n",
    "word_emb = normalize_embedding_matrix(word_emb)\n",
    "aspect_embeddings = normalize_embedding_matrix(aspect_embeddings)"
   ],
   "id": "44b761048a98c802",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:19.369551Z",
     "start_time": "2024-12-05T14:57:19.334374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.evaluation import extract_top_k_words_of_aspect\n",
    "\n",
    "aspects_top_k_words = [extract_top_k_words_of_aspect(a, word_emb, vocab_inv) for a in aspect_embeddings]"
   ],
   "id": "6467c03e5c3b055d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  successor (0.739248514175415)\n",
      "Word:  caylus (0.7310448288917542)\n",
      "Word:  tigris (0.7292113304138184)\n",
      "Word:  vein (0.7269777655601501)\n",
      "Word:  concordia (0.72653728723526)\n",
      "Word:  nova (0.7137109041213989)\n",
      "Word:  brass (0.7124441862106323)\n",
      "Word:  patchwork (0.7122059464454651)\n",
      "Word:  goa (0.7093117237091064)\n",
      "Word:  lorenzo (0.7025164365768433)\n",
      "Word:  predecessor (0.6919888257980347)\n",
      "Word:  scythe (0.6902366876602173)\n",
      "Word:  carcassone (0.6799775958061218)\n",
      "Word:  splendor (0.6772584915161133)\n",
      "Word:  istanbul (0.677189826965332)\n",
      "Word:  steroid (0.6756918430328369)\n",
      "Word:  keyflower (0.674812376499176)\n",
      "Word:  reimplementation (0.6687012910842896)\n",
      "Word:  sequel (0.6677713394165039)\n",
      "Word:  uwe (0.6667392253875732)\n",
      "Word:  rosenberg (0.6656197309494019)\n",
      "Word:  caverna (0.6651198863983154)\n",
      "Word:  orleans (0.6585699915885925)\n",
      "Word:  og (0.6574374437332153)\n",
      "Word:  birmingham (0.6533355116844177)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  illustration (0.8313093185424805)\n",
      "Word:  colorful (0.8279768228530884)\n",
      "Word:  sturdy (0.8033429384231567)\n",
      "Word:  functional (0.7723512649536133)\n",
      "Word:  thick (0.766491711139679)\n",
      "Word:  presentation (0.7586276531219482)\n",
      "Word:  vibrant (0.7327176332473755)\n",
      "Word:  bright (0.732177734375)\n",
      "Word:  artwork (0.7297290563583374)\n",
      "Word:  chunky (0.727603554725647)\n",
      "Word:  adorable (0.7207968831062317)\n",
      "Word:  colourful (0.7113686203956604)\n",
      "Word:  visually (0.7086130976676941)\n",
      "Word:  aesthetically (0.699967622756958)\n",
      "Word:  gorgeous (0.6986837387084961)\n",
      "Word:  illustrate (0.6852495670318604)\n",
      "Word:  packaging (0.68285071849823)\n",
      "Word:  art (0.68193519115448)\n",
      "Word:  plastic (0.6817364692687988)\n",
      "Word:  overproduce (0.6784065365791321)\n",
      "Word:  cardboard (0.6783788204193115)\n",
      "Word:  durable (0.6769279837608337)\n",
      "Word:  cartoony (0.6737772226333618)\n",
      "Word:  visual (0.6726703643798828)\n",
      "Word:  aesthetic (0.6711934804916382)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  government (0.6537377834320068)\n",
      "Word:  pound (0.6390856504440308)\n",
      "Word:  slave (0.6383839845657349)\n",
      "Word:  takeover (0.6376343965530396)\n",
      "Word:  indirectly (0.6372005939483643)\n",
      "Word:  rice (0.6238287091255188)\n",
      "Word:  russia (0.6230992674827576)\n",
      "Word:  revolt (0.6208956241607666)\n",
      "Word:  carriage (0.6183581352233887)\n",
      "Word:  airborne (0.6181811690330505)\n",
      "Word:  patriot (0.6179948449134827)\n",
      "Word:  peasant (0.6166446805000305)\n",
      "Word:  chief (0.6156402826309204)\n",
      "Word:  ore (0.6151480674743652)\n",
      "Word:  missile (0.6138738989830017)\n",
      "Word:  blockade (0.6133998036384583)\n",
      "Word:  craftsman (0.6116085052490234)\n",
      "Word:  aurbit (0.6110320687294006)\n",
      "Word:  politician (0.6109344959259033)\n",
      "Word:  gas (0.6108483076095581)\n",
      "Word:  envoy (0.6107466220855713)\n",
      "Word:  council (0.6103895902633667)\n",
      "Word:  threaten (0.6101714968681335)\n",
      "Word:  taxation (0.6101093292236328)\n",
      "Word:  surrender (0.6099389791488647)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  playmat (0.773808479309082)\n",
      "Word:  bundle (0.7345288395881653)\n",
      "Word:  exclusive (0.7040220499038696)\n",
      "Word:  backer (0.7020175457000732)\n",
      "Word:  promos (0.6994059085845947)\n",
      "Word:  ks (0.6845317482948303)\n",
      "Word:  addon (0.6824579834938049)\n",
      "Word:  preordere (0.6769403219223022)\n",
      "Word:  kit (0.6731395125389099)\n",
      "Word:  incl (0.6719321012496948)\n",
      "Word:  ebay (0.671379804611206)\n",
      "Word:  retail (0.6695592403411865)\n",
      "Word:  separately (0.6644646525382996)\n",
      "Word:  -plus (0.6613636612892151)\n",
      "Word:  kickstarte (0.6577873826026917)\n",
      "Word:  tin (0.657631516456604)\n",
      "Word:  nis (0.6559828519821167)\n",
      "Word:  anniversary (0.6512113213539124)\n",
      "Word:  endbringer (0.6507132053375244)\n",
      "Word:  gametrayz (0.6502020359039307)\n",
      "Word:  extension (0.6497981548309326)\n",
      "Word:  10th (0.6492010951042175)\n",
      "Word:  printing (0.649055004119873)\n",
      "Word:  os3 (0.6486407518386841)\n",
      "Word:  accessory (0.6482079029083252)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  theming (0.6056110858917236)\n",
      "Word:  artwork- (0.5323971509933472)\n",
      "Word:  mechanisms- (0.5203343033790588)\n",
      "Word:  cyberpunk (0.5156961679458618)\n",
      "Word:  components- (0.4977618455886841)\n",
      "Word:  ip (0.4850092828273773)\n",
      "Word:  theme (0.4791092872619629)\n",
      "Word:  sibbling (0.46791285276412964)\n",
      "Word:  storytelling (0.4626336395740509)\n",
      "Word:  microbadge=34746 (0.4447489380836487)\n",
      "Word:  sol (0.43384629487991333)\n",
      "Word:  eurogame (0.4333735704421997)\n",
      "Word:  presentation (0.4269278049468994)\n",
      "Word:  pfister (0.4253562390804291)\n",
      "Word:  humour (0.42316609621047974)\n",
      "Word:  laukat (0.4227566123008728)\n",
      "Word:  workerplacement (0.42118364572525024)\n",
      "Word:  seamlessly (0.4145762324333191)\n",
      "Word:  framework (0.4099147915840149)\n",
      "Word:  abstracty (0.40589118003845215)\n",
      "Word:  kramer (0.4003620147705078)\n",
      "Word:  star::star::star::nostar::nostar (0.3958247900009155)\n",
      "Word:  ambiance (0.393182635307312)\n",
      "Word:  tere (0.39174628257751465)\n",
      "Word:  pam (0.3894035220146179)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  tedious (0.7040528059005737)\n",
      "Word:  uninteresting (0.6661335229873657)\n",
      "Word:  pointless (0.6614547967910767)\n",
      "Word:  ultimately (0.6479504108428955)\n",
      "Word:  slog (0.645614743232727)\n",
      "Word:  procedural (0.6411250829696655)\n",
      "Word:  unsatisfying (0.6237403154373169)\n",
      "Word:  unforgiving (0.6145417094230652)\n",
      "Word:  dull (0.6091749668121338)\n",
      "Word:  arbitrary (0.6034442186355591)\n",
      "Word:  harsh (0.5990813374519348)\n",
      "Word:  liking (0.5989380478858948)\n",
      "Word:  painful (0.5880246758460999)\n",
      "Word:  disappointing (0.5827527642250061)\n",
      "Word:  shallow (0.5826346278190613)\n",
      "Word:  convoluted (0.5797157883644104)\n",
      "Word:  fiddly (0.577906608581543)\n",
      "Word:  terribly (0.5776752233505249)\n",
      "Word:  overlong (0.576109766960144)\n",
      "Word:  suffer (0.5747424960136414)\n",
      "Word:  calculation (0.572914183139801)\n",
      "Word:  mathy (0.5720897316932678)\n",
      "Word:  dry (0.5710248947143555)\n",
      "Word:  plod (0.569502592086792)\n",
      "Word:  rote (0.5691955089569092)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  engaging (0.718542218208313)\n",
      "Word:  puzzly (0.706798255443573)\n",
      "Word:  thoughtful (0.6699002981185913)\n",
      "Word:  intriguing (0.6347543001174927)\n",
      "Word:  satisfy (0.6342340707778931)\n",
      "Word:  fascinating (0.6336718201637268)\n",
      "Word:  delightful (0.6239219307899475)\n",
      "Word:  immersive (0.6169160604476929)\n",
      "Word:  intense (0.6093376278877258)\n",
      "Word:  intricate (0.6018016338348389)\n",
      "Word:  blend (0.5988196134567261)\n",
      "Word:  surprising (0.5976182222366333)\n",
      "Word:  emergent (0.5904867649078369)\n",
      "Word:  innovative (0.5901941061019897)\n",
      "Word:  wonderfully (0.5896986722946167)\n",
      "Word:  challenging (0.589469313621521)\n",
      "Word:  varied (0.5871151685714722)\n",
      "Word:  strategical (0.58623868227005)\n",
      "Word:  compelling (0.5838038921356201)\n",
      "Word:  entertaining (0.5833074450492859)\n",
      "Word:  tactical (0.5817297697067261)\n",
      "Word:  deckbuilding (0.5689189434051514)\n",
      "Word:  thinky (0.5684452056884766)\n",
      "Word:  creative (0.5681754350662231)\n",
      "Word:  fluid (0.564597487449646)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  2012 (0.8957140445709229)\n",
      "Word:  2011 (0.8950109481811523)\n",
      "Word:  2009 (0.8904664516448975)\n",
      "Word:  2007 (0.8823026418685913)\n",
      "Word:  2008 (0.8765960931777954)\n",
      "Word:  january (0.8718916177749634)\n",
      "Word:  2010 (0.8690791726112366)\n",
      "Word:  2016 (0.8681637048721313)\n",
      "Word:  2024 (0.8656596541404724)\n",
      "Word:  jan (0.864478349685669)\n",
      "Word:  aug (0.8642047643661499)\n",
      "Word:  sept (0.8639707565307617)\n",
      "Word:  august (0.8623989820480347)\n",
      "Word:  2014 (0.8601714372634888)\n",
      "Word:  feb (0.8594971895217896)\n",
      "Word:  23 (0.8586921691894531)\n",
      "Word:  july (0.8586184978485107)\n",
      "Word:  september (0.8568543791770935)\n",
      "Word:  december (0.8528299927711487)\n",
      "Word:  2017 (0.852756679058075)\n",
      "Word:  nov (0.8527511358261108)\n",
      "Word:  05 (0.8521738052368164)\n",
      "Word:  2013 (0.8497843742370605)\n",
      "Word:  03 (0.849012553691864)\n",
      "Word:  april (0.8485146164894104)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  5p (0.7386441826820374)\n",
      "Word:  6p (0.731103777885437)\n",
      "Word:  2.5 (0.7261812686920166)\n",
      "Word:  2,3 (0.7075560092926025)\n",
      "Word:  1–4 (0.6965786218643188)\n",
      "Word:  120 (0.6900813579559326)\n",
      "Word:  minimum (0.6892633438110352)\n",
      "Word:  4 (0.6830364465713501)\n",
      "Word:  3.5 (0.6827676296234131)\n",
      "Word:  5 (0.6705047488212585)\n",
      "Word:  hrs (0.6703524589538574)\n",
      "Word:  p (0.6677823066711426)\n",
      "Word:  2 (0.663541316986084)\n",
      "Word:  3–5 (0.6587302088737488)\n",
      "Word:  | (0.6584082841873169)\n",
      "Word:  2–4 (0.657484233379364)\n",
      "Word:  jugadores (0.649742066860199)\n",
      "Word:  180 (0.649402379989624)\n",
      "Word:  150min (0.6487183570861816)\n",
      "Word:  pl (0.646895170211792)\n",
      "Word:  4p (0.6420287489891052)\n",
      "Word:  3.6 (0.6390177011489868)\n",
      "Word:  2–5 (0.6338261365890503)\n",
      "Word:  hr (0.6335310935974121)\n",
      "Word:  60min (0.6305173635482788)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  haba (0.6249263286590576)\n",
      "Word:  de (0.6214011907577515)\n",
      "Word:  g (0.6091286540031433)\n",
      "Word:  tabula (0.6086817979812622)\n",
      "Word:  r (0.598611056804657)\n",
      "Word:  duchess (0.5939170122146606)\n",
      "Word:  desde (0.5936490893363953)\n",
      "Word:  46 (0.59280925989151)\n",
      "Word:  kitty (0.5900701284408569)\n",
      "Word:  100pk (0.5896608829498291)\n",
      "Word:  der (0.5892238616943359)\n",
      "Word:  halloween (0.5882335901260376)\n",
      "Word:  mdg-7104 (0.5865020155906677)\n",
      "Word:  harley (0.5863572955131531)\n",
      "Word:  beer (0.5862793922424316)\n",
      "Word:  dan (0.5862497687339783)\n",
      "Word:  james (0.5823870897293091)\n",
      "Word:  van (0.5805909633636475)\n",
      "Word:  deposito (0.5796600580215454)\n",
      "Word:  goat (0.5793377757072449)\n",
      "Word:  en (0.5786583423614502)\n",
      "Word:  eulani (0.57796311378479)\n",
      "Word:  friends (0.577806830406189)\n",
      "Word:  bolton (0.5775043964385986)\n",
      "Word:  matt (0.5764032602310181)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  learn (0.8976331949234009)\n",
      "Word:  teach (0.8746163845062256)\n",
      "Word:  grasp (0.8320752382278442)\n",
      "Word:  explain (0.8127039670944214)\n",
      "Word:  teaching (0.7579811811447144)\n",
      "Word:  understand (0.7480484247207642)\n",
      "Word:  grok (0.7227581143379211)\n",
      "Word:  digest (0.7048222422599792)\n",
      "Word:  internalize (0.6550203561782837)\n",
      "Word:  learning (0.6286235451698303)\n",
      "Word:  hang (0.6033831834793091)\n",
      "Word:  newbie (0.601612389087677)\n",
      "Word:  intimidate (0.593424916267395)\n",
      "Word:  comprehend (0.5811084508895874)\n",
      "Word:  internalise (0.5792596340179443)\n",
      "Word:  beginner (0.5762982368469238)\n",
      "Word:  intuitive (0.5757943391799927)\n",
      "Word:  explanation (0.5755511522293091)\n",
      "Word:  relatively (0.5605951547622681)\n",
      "Word:  newcomer (0.5288904309272766)\n",
      "Word:  accessible (0.5200917720794678)\n",
      "Word:  overlook (0.505193829536438)\n",
      "Word:  comfortable (0.5009444952011108)\n",
      "Word:  parse (0.5006829500198364)\n",
      "Word:  dense (0.5001760721206665)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  hut (0.7790427207946777)\n",
      "Word:  crop (0.7767229080200195)\n",
      "Word:  district (0.7736755609512329)\n",
      "Word:  contract (0.7651357650756836)\n",
      "Word:  currency (0.7630130052566528)\n",
      "Word:  palace (0.7591584920883179)\n",
      "Word:  plant (0.7552641034126282)\n",
      "Word:  harvest (0.7329472899436951)\n",
      "Word:  disc (0.7323476672172546)\n",
      "Word:  tent (0.7311029434204102)\n",
      "Word:  valuable (0.7309062480926514)\n",
      "Word:  income (0.7275106310844421)\n",
      "Word:  mining (0.7260743379592896)\n",
      "Word:  caravan (0.7228035926818848)\n",
      "Word:  boat (0.7216872572898865)\n",
      "Word:  river (0.720855712890625)\n",
      "Word:  adjacent (0.7204995155334473)\n",
      "Word:  pawn (0.7203629612922668)\n",
      "Word:  farm (0.7190582752227783)\n",
      "Word:  claim (0.7181892395019531)\n",
      "Word:  citizen (0.7168853282928467)\n",
      "Word:  brick (0.7140960693359375)\n",
      "Word:  coal (0.7103909850120544)\n",
      "Word:  assistant (0.7098140716552734)\n",
      "Word:  temple (0.7047557830810547)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  manual (0.8068215847015381)\n",
      "Word:  instruction (0.7947537899017334)\n",
      "Word:  faq (0.7892134189605713)\n",
      "Word:  booklet (0.7840953469276428)\n",
      "Word:  index (0.7768779993057251)\n",
      "Word:  rulebook (0.7751756906509399)\n",
      "Word:  reference (0.7750811576843262)\n",
      "Word:  typo (0.7634031772613525)\n",
      "Word:  clarification (0.7510777711868286)\n",
      "Word:  paragraph (0.7408995628356934)\n",
      "Word:  wording (0.7395057678222656)\n",
      "Word:  forum (0.7378411293029785)\n",
      "Word:  glossary (0.7334436774253845)\n",
      "Word:  consult (0.7199385762214661)\n",
      "Word:  unclear (0.7195554971694946)\n",
      "Word:  error (0.7186257243156433)\n",
      "Word:  thorough (0.7073087096214294)\n",
      "Word:  translation (0.7001204490661621)\n",
      "Word:  clarify (0.6999341249465942)\n",
      "Word:  page (0.6986279487609863)\n",
      "Word:  errata (0.6983932256698608)\n",
      "Word:  translate (0.697641134262085)\n",
      "Word:  refer (0.6863702535629272)\n",
      "Word:  playbook (0.6856905221939087)\n",
      "Word:  file (0.6823931336402893)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  eliminate (0.7285047769546509)\n",
      "Word:  initiative (0.6980311870574951)\n",
      "Word:  randomly (0.695290207862854)\n",
      "Word:  outcome (0.6767880916595459)\n",
      "Word:  effectively (0.6764273643493652)\n",
      "Word:  discard (0.675682008266449)\n",
      "Word:  determine (0.6727367639541626)\n",
      "Word:  effect (0.6726858615875244)\n",
      "Word:  prevent (0.6725395917892456)\n",
      "Word:  potentially (0.6654207706451416)\n",
      "Word:  beneficial (0.6639978885650635)\n",
      "Word:  simultaneously (0.6603031158447266)\n",
      "Word:  directly (0.6594130396842957)\n",
      "Word:  occur (0.655897855758667)\n",
      "Word:  ensure (0.6555535793304443)\n",
      "Word:  select (0.6545717716217041)\n",
      "Word:  attack (0.654458224773407)\n",
      "Word:  fate (0.6526445150375366)\n",
      "Word:  force (0.6525661945343018)\n",
      "Word:  secretly (0.6504300832748413)\n",
      "Word:  strength (0.6496902108192444)\n",
      "Word:  powerful (0.6494775414466858)\n",
      "Word:  bid (0.6479315161705017)\n",
      "Word:  switch (0.646094024181366)\n",
      "Word:  devastating (0.6444013118743896)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  shadow (0.7913479208946228)\n",
      "Word:  legend (0.7901990413665771)\n",
      "Word:  warhammer (0.7698392271995544)\n",
      "Word:  invasion (0.7605602741241455)\n",
      "Word:  mansion (0.7588604688644409)\n",
      "Word:  vampire (0.752719521522522)\n",
      "Word:  wrath (0.7486203908920288)\n",
      "Word:  brimstone (0.7413413524627686)\n",
      "Word:  demon (0.7389450073242188)\n",
      "Word:  mythic (0.7384161353111267)\n",
      "Word:  firefly (0.7376453280448914)\n",
      "Word:  heroic (0.7284348607063293)\n",
      "Word:  runebound (0.7236553430557251)\n",
      "Word:  greek (0.7224932312965393)\n",
      "Word:  rogue (0.719290018081665)\n",
      "Word:  crossover (0.7178816795349121)\n",
      "Word:  sinister (0.716095507144928)\n",
      "Word:  blade (0.7154049277305603)\n",
      "Word:  mythos (0.7147831916809082)\n",
      "Word:  champion (0.7147499322891235)\n",
      "Word:  40k (0.7140669822692871)\n",
      "Word:  pathfinder (0.7129002213478088)\n",
      "Word:  orc (0.7116289734840393)\n",
      "Word:  andor (0.7091148495674133)\n",
      "Word:  undead (0.7083783149719238)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  request (0.6947983503341675)\n",
      "Word:  wife (0.685591459274292)\n",
      "Word:  occasion (0.6790377497673035)\n",
      "Word:  anymore (0.6783736348152161)\n",
      "Word:  friend (0.6666617393493652)\n",
      "Word:  regularly (0.6568692922592163)\n",
      "Word:  bored (0.6476596593856812)\n",
      "Word:  happily (0.6414668560028076)\n",
      "Word:  folk (0.6311484575271606)\n",
      "Word:  convince (0.6278557777404785)\n",
      "Word:  husband (0.6261847019195557)\n",
      "Word:  partner (0.6122449636459351)\n",
      "Word:  anytime (0.6119512915611267)\n",
      "Word:  ask (0.6043254137039185)\n",
      "Word:  everybody (0.6041883230209351)\n",
      "Word:  week (0.5980612635612488)\n",
      "Word:  willing (0.5961793661117554)\n",
      "Word:  weekend (0.5940553545951843)\n",
      "Word:  evening (0.5936676263809204)\n",
      "Word:  imagine (0.5907056331634521)\n",
      "Word:  month (0.5856772661209106)\n",
      "Word:  son (0.5819655060768127)\n",
      "Word:  tired (0.574928879737854)\n",
      "Word:  buddy (0.5713748931884766)\n",
      "Word:  group (0.570905864238739)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![title](./../resources/umass.png)\n",
   "id": "c35ff70d0f15833d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset generation",
   "id": "b6bbdc3d8587ddd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:57:28.711667Z",
     "start_time": "2024-12-05T14:57:19.370228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "\n",
    "vocabulary = manager.embedding_model.model.wv.key_to_index\n",
    "ds = dataset.PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=vocabulary, csv_dataset_path=\"../data/corpus.preprocessed.kickstarter_removed.256k.csv\", negative_size=15\n",
    ")"
   ],
   "id": "9455357c1b446cee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ../data/corpus.preprocessed.kickstarter_removed.256k.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/200529 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15ce9194698742dfbd0c98a3ad7527bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 538 points.This is 0.2682903719661495% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1999  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:01:23.682323Z",
     "start_time": "2024-12-05T14:57:32.833366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.evaluation import coherence\n",
    "\n",
    "print(manager.embedding_model.model.wv.index_to_key[13])\n",
    "# todo pass indexwords\n",
    "for i in range(len(aspects_top_k_words)):\n",
    "    i_th_aspect_top_k = aspects_top_k_words[i][0:10]\n",
    "    word_indexes = list(map(lambda x: x[2], i_th_aspect_top_k))\n",
    "    print(word_indexes)\n",
    "    print(coherence(word_indexes, ds))"
   ],
   "id": "41b7a4f894b6ba59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n",
      "[3137, 1547, 2765, 2814, 1952, 2546, 1464, 2151, 4040, 3779]\n",
      "-17.125883244267772\n",
      "[1523, 1609, 3186, 1658, 1554, 1200, 3539, 2581, 145, 2327]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m word_indexes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;241m2\u001B[39m], i_th_aspect_top_k))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(word_indexes)\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mcoherence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_indexes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/core/evaluation.py:45\u001B[0m, in \u001B[0;36mcoherence\u001B[0;34m(top_n_words, corpus)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Exponential complexity. We can optimize this.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m document_co_frequency_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w_i, w_j \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mcombinations(top_n_words, \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m     46\u001B[0m     co_occurrence \u001B[38;5;241m=\u001B[39m document_co_occurrence(w_i, w_j, corpus)\n\u001B[1;32m     47\u001B[0m     document_co_frequency_values[(w_i, w_j)] \u001B[38;5;241m=\u001B[39m co_occurrence\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/core/evaluation.py:65\u001B[0m, in \u001B[0;36mdocument_co_occurrence\u001B[0;34m(a, b, corpus)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocument_frequency\u001B[39m(a: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, corpus: PositiveNegativeCommentGeneratorDataset) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: a \u001B[38;5;129;01min\u001B[39;00m x)\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocument_co_occurrence\u001B[39m(a: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, b: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, corpus: PositiveNegativeCommentGeneratorDataset) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: a \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;129;01mand\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m x)\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/core/evaluation.py:65\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocument_frequency\u001B[39m(a: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, corpus: PositiveNegativeCommentGeneratorDataset) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: a \u001B[38;5;129;01min\u001B[39;00m x)\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocument_co_occurrence\u001B[39m(a: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, b: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m, corpus: PositiveNegativeCommentGeneratorDataset) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: a \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;129;01mand\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m x)\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py:366\u001B[0m, in \u001B[0;36mThreadTracer.__call__\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args \u001B[38;5;241m=\u001B[39m args\n\u001B[1;32m    364\u001B[0m \u001B[38;5;66;03m# ENDIF\u001B[39;00m\n\u001B[0;32m--> 366\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, frame, event, arg):\n\u001B[1;32m    367\u001B[0m \u001B[38;5;250m        \u001B[39m\u001B[38;5;124;03m''' This is the callback used when we enter some context in the debugger.\u001B[39;00m\n\u001B[1;32m    368\u001B[0m \n\u001B[1;32m    369\u001B[0m \u001B[38;5;124;03m        We also decorate the thread we are in with info about the debugging.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;124;03m            This is the global debugger (this method should actually be added as a method to it).\u001B[39;00m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;124;03m        '''\u001B[39;00m\n\u001B[1;32m    379\u001B[0m         \u001B[38;5;66;03m# IFDEF CYTHON\u001B[39;00m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;66;03m# cdef str filename;\u001B[39;00m\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;66;03m# cdef str base;\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    388\u001B[0m         \u001B[38;5;66;03m# ENDIF\u001B[39;00m\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;66;03m# print('ENTER: trace_dispatch', frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name)\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.4. Word2vec Coherence Score\n",
    "[https://www.baeldung.com/cs/topic-modeling-coherence-score#4-word2vec-coherence-score"
   ],
   "id": "4f1ae0bad6256629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd183c0812b5d507",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
