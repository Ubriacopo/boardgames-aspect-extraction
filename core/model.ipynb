{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#https://github.com/alexeyev/abae-pytorch \n",
    "\n",
    "# TODO pulisci questo file\n",
    "# todo  prova con is_alpha in preprocessing e buttiamo via il resto (con sequenze piu lunghe)!\n",
    "#  --> leggi top 1000 di restaruant e vedi split con '' quanto lunghe in media\n",
    "# todo fix regularization sia su matrix che generation di aspects / emb\n",
    "\n",
    "# https://arxiv.org/pdf/1803.09820\n",
    "# https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/"
   ],
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Una considerazione: Potrei sfruttare gli aspetti inferiti da quello che ho per filtrare via dal testo documenti che parlano di \"Piattaforme\", \"Kickstarter\", \"Materiali\" visto che queste le becca con facilita.\n",
    "# Il mio dataset é probabilmente troppo vario al momento. Un filtraggio di questo tipo facilitrerebbe l'apprendimento."
   ],
   "id": "49f165d09275bde5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset. <br>\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] <br>\n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors <br>\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Choice of max_seq size is relative to the ds. 95th percentile (at max 5% loss of information) -> todo valuta",
   "id": "377fe340c9c009f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset import PandasPositiveNegativeNumericTextDataset\n",
    "from core.train import ABAEModelConfiguration, ABAEModelManager\n",
    "import pandas as pd\n",
    "\n",
    "corpus = \"../output/dataset/default-pre-processed/80k.preprocessed.csv\"\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"test_default_adam_new_ds\")\n",
    "config.epochs = 1\n",
    "config.max_sequence_length = 40\n",
    "config.negative_sample_size = 15\n",
    "print(f\"Running on default config:\\n {config}\")\n",
    "\n",
    "# Without any hp tuning we just try and see how it goes.\n",
    "manager = ABAEModelManager(config)\n",
    "train_dataset = PandasPositiveNegativeNumericTextDataset(\n",
    "    dataframe=pd.read_csv(corpus),\n",
    "    vocabulary=manager.embedding_model.vocabulary(),\n",
    "    negative_size=config.negative_sample_size,\n",
    "    max_seq_length=config.max_sequence_length\n",
    ")"
   ],
   "id": "830abaf1d37c6bb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manager.run_train_process(train_dataset, optimizer='adam')",
   "id": "d3278a09ace24e58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "predictions = iteration_model.predict()"
   ],
   "id": "c4e25c02eab60eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "from keras import ops as K\n",
    "\n",
    "iteration_model = manager._t_model\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embeddings').weights[0].value.data\n",
    "aspect_embeddings = K.transpose(iteration_model.get_layer('aspect_embeddings').w)\n",
    "\n",
    "word_emb = torch.nn.functional.normalize(word_emb, p=2, dim=-1)\n",
    "aspect_embeddings = torch.nn.functional.normalize(aspect_embeddings, p=2, dim=-1)\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=50) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(corpus, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "db228e03f66dec96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "65e0b816a06ee9b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"hands_on\")\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=3) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "13e9619f5dead821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! <br>\n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The main goal of ABAE is to extract interpretable and meaningful aspects, which makes coherence the more aligned metric.<br> Reconstruction error might help guide training but doesn’t guarantee that the extracted aspects are semantically useful.",
   "id": "cd4dfe8c62445d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper, HyperparameterTuningManager\n",
    "\n",
    "configurations = 4  # We try 15 different configurations\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "print(f\"Starting procedure. We try a total of {configurations}\")\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "hp_tuning_manager = HyperparameterTuningManager(hp_wrapper, corpus_file, \"./output\")"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hp_tuning_manager(different_configurations=configurations, repeat=3)\n",
    "# The output of this cell is very long therefore I deleted it"
   ],
   "id": "8b31050bf118ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    columns=[\n",
    "        'name', 'mean_loss', 'mean_coherence', 'aspect_size', 'embedding_size', 'epochs', 'batch_size', 'learning_rate',\n",
    "        'decay_rate', 'momentum', 'negative_sample_size'\n",
    "    ]\n",
    ")\n",
    "\n",
    "i = 0\n",
    "# Inspect results we stored for process. todo: Quando finisci salvalo come file da \"tenere\" in repo cosi che chi controlla lo ha gia.\n",
    "for element in Path(\"./output\").iterdir():\n",
    "\n",
    "    # Search only for the tuning process runs.\n",
    "    if element.is_dir() and element.name.startswith(\"tuning\"):\n",
    "        run_result = json.load(open(str(element) + \"/run_results.json\"))\n",
    "        # We want the best configurations.\n",
    "\n",
    "        obj = {\n",
    "            'name': element.name,\n",
    "            'mean_loss': np.mean(run_result[\"evaluation_loss\"]),\n",
    "            'mean_coherence': np.mean(run_result[\"coherence\"])\n",
    "        }\n",
    "        temp = pd.DataFrame(run_result['params'] | obj, index=[i])\n",
    "        dataframe = pd.concat([temp, dataframe])\n",
    "        i += 1"
   ],
   "id": "6f11e2391039f8fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds.text_ds.apply(lambda x: x.split(' '))",
   "id": "83f0afe3f02329d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, normalize, get_aspect_top_k_words\n",
    "import json\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "model_name = 'tuning_7128da5c-04c0-4e91-abf2-462f9646b48d'\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "parameters = json.load(open(f\"./output/{model_name}/run_results.json\"))['params']\n",
    "config = ABAEModelConfiguration(corpus_file=c_file, model_name=model_name, **parameters)\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "\n",
    "# word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "# aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=20) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=10)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "804ef5608c713f42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manager.embedding_model.model.wv",
   "id": "6f3bd8c300970a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(ds)",
   "id": "49c5e904ed44fc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset import SimpleWord2VecEmbeddingsDataset\n",
    "\n",
    "iteration_model = manager.get_inference_model()\n",
    "custom_review = [\n",
    "    'This game is hard to play but fun! Luck also plays a huge role!',\n",
    "    'My brother takes forever to decide what to do',\n",
    "    'Zero luck it is all skill',\n",
    "    'A lot of downtime',\n",
    "    'i have banana',\n",
    "    'plastic inserts'\n",
    "]\n",
    "# todo rebuild a prediciton model senza negative predict. Devo ristrutturare tutto1\n",
    "ds = SimpleWord2VecEmbeddingsDataset(custom_review, manager.embedding_model.model)\n",
    "\n",
    "res = iteration_model.predict(DataLoader(ds, batch_size=32))"
   ],
   "id": "2a96d8734cbb8224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Il modello fa schifo non serve a niente.\n",
    "res[1]"
   ],
   "id": "70e3c77deb50ee48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "64b81ef255245128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "len(ds.dataset.to_list())\n",
    "list(filter(lambda x: x.any() is None or len(x) < 1, ds.dataset.to_list()))"
   ],
   "id": "b3d2de1dcde3bd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Focus on learning rate",
   "id": "cf07a77d637e025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We fix other params and now focus entirely on lr.\n",
    "# We have already a \"promising\" range defined.\n",
    "# We look in that space so we redefine lr on ABAERandomHyperparametersSelectionWrapper"
   ],
   "id": "c6f003172b40c1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See if the Hp tuning really improved upon our results:\n",
    "We used SGD anda learned its parameters under the assumption that we would do better. <br>\n",
    "Let's see if it really is the case, or we just wasted time.\n",
    "\n",
    "For comparison we use Adam that has the advantage of being robust enough without parameter scouting."
   ],
   "id": "9b8089270d722ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#todo",
   "id": "84bcef4b87f44595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test accuracy on small test sample we filled out\n",
    "\n",
    "### Test set definition"
   ],
   "id": "fcf1124990b1034f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "# We take around 1k records that were not seen yet from the model and label them by hand.\n",
    "dataset = pd.read_csv(\"../data/corpus.csv\")\n",
    "# Handle game names. Or not? I don't need the full pipeline I guess. todo\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()\n",
    "\n",
    "pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")\n",
    "\n",
    "# Extract 1k from dataset that are not in 200k\n",
    "train_ds = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "\n",
    "# Take top 2k. (We will select some good ones and reduce the number to 1k)\n",
    "test_set = dataset[~dataset[\"comments\"].isin(train_ds[\"comments\"])]"
   ],
   "id": "ae3809bf35d257e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to use labels:",
   "id": "f792cdc1d37508dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = {\n",
    "    '0': \"Luck/Alea\",\n",
    "    '1': 'Bookkeeping',\n",
    "    '2': 'Downtime',\n",
    "    '3': 'Interaction',\n",
    "    '4': 'Bash',\n",
    "    '5': 'Complicated/Complex',  # I could watch weight to see if there is a ratio relation.\n",
    "    '6': 'Misc'\n",
    "}"
   ],
   "id": "b87ccaeccafba997",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
