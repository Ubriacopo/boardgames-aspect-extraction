{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO pulisci questo file\n",
    "# https://arxiv.org/pdf/1803.09820\n",
    "# https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/"
   ],
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset. <br>\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] <br>\n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors <br>\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "from core.train import AbaeModelManager, AbaeModelConfiguration\n",
    "\n",
    "corpus = \"../output/dataset/pre-processed/200k.preprocessed.csv\"\n",
    "config = AbaeModelConfiguration(corpus_file=corpus, model_name=f\"hands_on\")\n",
    "\n",
    "print(f\"Running on default config:\\n {config}\")\n",
    "\n",
    "# Without any hp tuning we just try and see how it goes.\n",
    "manager = AbaeModelManager(config)\n",
    "train_dataset = PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=manager.embedding_model.vocabulary(),\n",
    "    csv_dataset_path=config.corpus_file, negative_size=config.negative_sample_size\n",
    ")\n",
    "\n",
    "manager.run_train_process(train_dataset)"
   ],
   "id": "ae6aa4303bfa4f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2459/3284\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m3:47\u001B[0m 276ms/step - loss: 7.1087 - max_margin_loss: 7.1019"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! <br>\n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The main goal of ABAE is to extract interpretable and meaningful aspects, which makes coherence the more aligned metric.<br> Reconstruction error might help guide training but doesn’t guarantee that the extracted aspects are semantically useful.",
   "id": "cd4dfe8c62445d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper, HyperparameterTuningManager\n",
    "\n",
    "configurations = 15  # We try 15 different configurations\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "print(f\"Starting procedure. We try a total of {configurations}\")\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "hp_tuning_manager = HyperparameterTuningManager(hp_wrapper, corpus_file, \"./output\")"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hp_tuning_manager(different_configurations=configurations, repeat=3)",
   "id": "8b31050bf118ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Focus on learning rate",
   "id": "cf07a77d637e025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We fix other params and now focus entirely on lr.\n",
    "# We have already a \"promising\" range defined.\n",
    "# We look in that space so we redefine lr on ABAERandomHyperparametersSelectionWrapper"
   ],
   "id": "c6f003172b40c1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See if the Hp tuning really improved upon our results:\n",
    "We used SGD anda learned its parameters under the assumption that we would do better. <br>\n",
    "Let's see if it really is the case, or we just wasted time.\n",
    "\n",
    "For comparison we use Adam that has the advantage of being robust enough without parameter scouting."
   ],
   "id": "9b8089270d722ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#todo",
   "id": "84bcef4b87f44595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb595c23e3f6a586",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test accuracy on small test sample we filled out\n",
    "\n",
    "### Test set definition"
   ],
   "id": "fcf1124990b1034f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "from core.pre_processing import DatasetGeneration\n",
    "\n",
    "# We take around 1k records that were not seen yet from the model and label them by hand.\n",
    "dataset = pd.read_csv(\"../data/corpus.csv\")\n",
    "pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")\n",
    "\n",
    "# Extract 1k from dataset that are not in 200k\n",
    "train_ds = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "\n",
    "# Take top 2k. (We will select some good ones and reduce the number to 1k)\n",
    "test_set = dataset[~dataset[\"comments\"].isin(train_ds[\"comments\"])]"
   ],
   "id": "ae3809bf35d257e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to use labels:",
   "id": "f792cdc1d37508dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "labels = {\n",
    "    '0': \"Luck/Alea\",\n",
    "    '1': 'Bookkeeping',\n",
    "    '2': 'Downtime',\n",
    "    '3': 'Interaction',\n",
    "    '4': 'Bash',\n",
    "    '5': 'Complicated/Complex',  # I could watch weight to see if there is a ratio relation.\n",
    "    '6': 'Misc'\n",
    "}"
   ],
   "id": "b530929833f7c79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
