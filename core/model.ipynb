{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO pulisci questo file\n",
    "# https://arxiv.org/pdf/1803.09820\n",
    "# https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/"
   ],
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:16:18.741707Z",
     "start_time": "2025-01-11T14:16:18.738140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " # Without any hp tuning we just try and see how it goes.",
   "id": "ae6aa4303bfa4f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset.\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] \n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! What size to search is a good question and should be studied (Which I may be doing later). \n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The main goal of ABAE is to extract interpretable and meaningful aspects, which makes coherence the more aligned metric. Reconstruction error might help guide training but doesn’t guarantee that the extracted aspects are semantically useful.",
   "id": "cd4dfe8c62445d96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:19:12.438421Z",
     "start_time": "2025-01-11T14:16:20.999162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from core.evaluation import normalize, get_aspect_top_k_words, coherence_per_aspect\n",
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper\n",
    "from core.train import AbaeModelManager, AbaeModelConfiguration\n",
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Starting process\")\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "configurations = 15  # We try 15 different configurations\n",
    "\n",
    "seen_configurations = set()\n",
    "seen_configurations.add(frozenset({'aspect_size': 17, 'embedding_size': 150, 'epochs': 20, 'batch_size': 64}.items()))\n",
    "\n",
    "scores = list()\n",
    "\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "for i in range(configurations):\n",
    "    uuid = uuid4()\n",
    "    parameters = next(hp_wrapper)\n",
    "    while seen_configurations.__contains__(frozenset(parameters.items())):\n",
    "        print(f\"We already worked on configuration: {parameters}\")\n",
    "        parameters = next(hp_wrapper)  # In case we fetch the same config more than once.\n",
    "    print(f\"Working on configuration: {parameters}\")\n",
    "    seen_configurations.add(frozenset(parameters.items()))\n",
    "\n",
    "    # Train process\n",
    "    config = AbaeModelConfiguration(corpus_file=corpus_file, model_name=f\"tuning_{uuid}\", **parameters)\n",
    "    manager = AbaeModelManager(config)\n",
    "\n",
    "    # The dataset generation depends on the embedding model\n",
    "    ds = PositiveNegativeCommentGeneratorDataset(\n",
    "        vocabulary=manager.embedding_model.vocabulary(),\n",
    "        csv_dataset_path=config.corpus_file, negative_size=15\n",
    "    )\n",
    "\n",
    "    train, validation = torch.utils.data.random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "    print(\"Training process in progress..\")\n",
    "    results, iteration_model = manager.run_train_process(train)\n",
    "\n",
    "    print(results)\n",
    "    # Evaluate the model\n",
    "    # We evaluate on the relative coherence between topics.\n",
    "    print(\"Evaluating model\")\n",
    "    word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "\n",
    "    aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "    print(f\"Word embeddings shape: {word_emb.shape}\")\n",
    "    inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "    aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=50) for a in aspect_embeddings]\n",
    "\n",
    "    aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]\n",
    "\n",
    "\n",
    "    coherence, coherence_model = coherence_per_aspect(aspect_words, ds.text_ds.loc[validation.indices], 10)\n",
    "    scores.append(dict(coherence=coherence_model.get_coherence(), parameters=parameters))\n",
    "\n",
    "# End done.\n",
    "print(scores)"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process\n",
      "Working on configuration: {'aspect_size': 18, 'embedding_size': 300, 'epochs': 1, 'batch_size': 64, 'learning_rate': 0.001, 'decay_rate': 0.9450000000000001, 'momentum': 0.97, 'negative_sample_size': 15}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9d4b096a808478fb53cab3fed23eb3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2644293914fc44e99282352a8f9db380"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 119085 words, keeping 4857 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 239721 words, keeping 5207 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 359702 words, keeping 5263 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 477300 words, keeping 5274 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 598508 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 721182 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 838524 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 957006 words, keeping 5280 word types\n",
      "INFO:gensim.models.word2vec:collected 5280 word types from a corpus of 1003308 raw words and 83855 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 5280 unique words (100.00% of original 5280, drops 0)', 'datetime': '2025-01-11T15:16:28.428204', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1003308 word corpus (100.00% of original 1003308, drops 0)', 'datetime': '2025-01-11T15:16:28.429200', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 5280 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 46 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 802044.7673554413 word corpus (79.9%% of prior 1003308)', 'datetime': '2025-01-11T15:16:28.448064', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:estimated required memory for 5280 words and 300 dimensions: 15312000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-11T15:16:28.478121', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 8 workers on 5280 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-01-11T15:16:28.479122', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 1003308 raw words (801713 effective words) took 0.5s, 1661515 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 1003308 raw words (801810 effective words) took 0.5s, 1652025 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 1003308 raw words (801900 effective words) took 0.6s, 1411928 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 1003308 raw words (802118 effective words) took 0.5s, 1515579 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 1003308 raw words (802179 effective words) took 0.6s, 1419752 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 5016540 raw words (4009720 effective words) took 2.7s, 1498103 effective words/s', 'datetime': '2025-01-11T15:16:31.155987', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=5280, vector_size=300, alpha=0.025>', 'datetime': '2025-01-11T15:16:31.155987', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './output/tuning_9176f1dd-7b94-479a-86d1-4e95547f7044/embeddings.keras', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-01-11T15:16:31.156985', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './output/tuning_9176f1dd-7b94-479a-86d1-4e95547f7044/embeddings.keras', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./output/tuning_9176f1dd-7b94-479a-86d1-4e95547f7044/embeddings.keras\n",
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n",
      "Loading dataset from file: ../output/dataset/pre-processed/tuning.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "194a8e62863a4e1db8c9754841b3b935"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  274 . The limit is set to 80 tokens.\n",
      "We loose information on 54 points.This is 0.06439687555900066% of the dataset.\n",
      "Padding sequences to length (80).\n",
      "Training process in progress..\n",
      "\u001B[1m1048/1049\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 131ms/step - loss: 13.9122 - max_margin_loss: 13.8530"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1049/1049\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m137s\u001B[0m 131ms/step - loss: 13.9098 - max_margin_loss: 13.8507\n",
      "<keras.src.callbacks.history.History object at 0x0000027C36BCEA20>\n",
      "Evaluating model\n",
      "Word embeddings shape: torch.Size([5280, 300])\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  distribute (0.7574232816696167)\n",
      "Word:  correspond (0.7515112161636353)\n",
      "Word:  ideia (0.7118360996246338)\n",
      "Word:  ascend (0.7110495567321777)\n",
      "Word:  cylinder (0.7081038951873779)\n",
      "Word:  adjacent (0.6909685134887695)\n",
      "Word:  randomize (0.6881609559059143)\n",
      "Word:  coloured (0.6871917247772217)\n",
      "Word:  select (0.6845979690551758)\n",
      "Word:  assistant (0.6817619800567627)\n",
      "Word:  landscape (0.6817315816879272)\n",
      "Word:  associate (0.67988121509552)\n",
      "Word:  guild (0.6798262000083923)\n",
      "Word:  retrieve (0.677538275718689)\n",
      "Word:  designate (0.6736943125724792)\n",
      "Word:  assign (0.6718295812606812)\n",
      "Word:  airship (0.6717966198921204)\n",
      "Word:  slot (0.6717431545257568)\n",
      "Word:  occupation (0.6701945066452026)\n",
      "Word:  allocate (0.6636679172515869)\n",
      "Word:  secretly (0.6630603671073914)\n",
      "Word:  activate (0.6590392589569092)\n",
      "Word:  district (0.6587041020393372)\n",
      "Word:  respective (0.6578375697135925)\n",
      "Word:  rotate (0.6562579870223999)\n",
      "Word:  seed (0.6562346816062927)\n",
      "Word:  numbered (0.6558438539505005)\n",
      "Word:  colonize (0.6550326943397522)\n",
      "Word:  pyramid (0.653866708278656)\n",
      "Word:  colored (0.6532232761383057)\n",
      "Word:  refill (0.6526806354522705)\n",
      "Word:  divide (0.6516693830490112)\n",
      "Word:  helper (0.6513776779174805)\n",
      "Word:  prerequisite (0.6508045196533203)\n",
      "Word:  reverse (0.6507132053375244)\n",
      "Word:  displace (0.648627519607544)\n",
      "Word:  display (0.6483882069587708)\n",
      "Word:  enclose (0.6458096504211426)\n",
      "Word:  permit (0.6439582109451294)\n",
      "Word:  banner (0.6407308578491211)\n",
      "Word:  diamond (0.6404630541801453)\n",
      "Word:  grant (0.6401856541633606)\n",
      "Word:  pawn (0.6391627192497253)\n",
      "Word:  hut (0.6370986104011536)\n",
      "Word:  vertical (0.6363958120346069)\n",
      "Word:  align (0.6351702809333801)\n",
      "Word:  access (0.6348199844360352)\n",
      "Word:  milestone (0.6347272992134094)\n",
      "Word:  visible (0.6343240141868591)\n",
      "Word:  dino (0.633501410484314)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  exp (0.572293758392334)\n",
      "Word:  ii (0.5634350776672363)\n",
      "Word:  expansions (0.5459674000740051)\n",
      "Word:  promos (0.5451341867446899)\n",
      "Word:  sleeved (0.5428765416145325)\n",
      "Word:  ● (0.5414063930511475)\n",
      "Word:  2x (0.5380837917327881)\n",
      "Word:  1x (0.5371832251548767)\n",
      "Word:  includes (0.5340771675109863)\n",
      "Word:  elves (0.5322446227073669)\n",
      "Word:  pledge (0.5311183929443359)\n",
      "Word:  mmp (0.5293718576431274)\n",
      "Word:  extras (0.5265591144561768)\n",
      "Word:  premium (0.5263853669166565)\n",
      "Word:  daelore (0.5253918766975403)\n",
      "Word:  painted (0.5212626457214355)\n",
      "Word:  dwarves (0.5209168195724487)\n",
      "Word:  ✓ (0.5167518258094788)\n",
      "Word:  gamegenic (0.5129326581954956)\n",
      "Word:  m./lavar (0.5114893913269043)\n",
      "Word:  nis (0.5096960663795471)\n",
      "Word:  bruce (0.5083787441253662)\n",
      "Word:  tokens (0.5052860975265503)\n",
      "Word:  upgraded (0.5032464861869812)\n",
      "Word:  accessory (0.502415657043457)\n",
      "Word:  shadows (0.500592827796936)\n",
      "Word:  prime (0.4994962215423584)\n",
      "Word:  flames (0.4993736147880554)\n",
      "Word:  men (0.49861940741539)\n",
      "Word:  bundle (0.4972803592681885)\n",
      "Word:  goals (0.4963032603263855)\n",
      "Word:  swamp (0.4955748915672302)\n",
      "Word:  ea (0.49360713362693787)\n",
      "Word:  le (0.49127644300460815)\n",
      "Word:  kit (0.4891344904899597)\n",
      "Word:  valley (0.48836082220077515)\n",
      "Word:  arcane (0.4857334792613983)\n",
      "Word:  elf (0.484561562538147)\n",
      "Word:  sleeves (0.48314088582992554)\n",
      "Word:  l (0.47876882553100586)\n",
      "Word:  matte (0.4787120819091797)\n",
      "Word:  w/ (0.4778132438659668)\n",
      "Word:  lee (0.47521623969078064)\n",
      "Word:  kings (0.47499939799308777)\n",
      "Word:  summer (0.47312551736831665)\n",
      "Word:  crossover (0.47288551926612854)\n",
      "Word:  playmat (0.470394492149353)\n",
      "Word:  franc (0.47025179862976074)\n",
      "Word:  neoprene (0.4701967239379883)\n",
      "Word:  63.5x88 (0.4690549075603485)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  shallow (0.7006508708000183)\n",
      "Word:  involved (0.6937653422355652)\n",
      "Word:  simplistic (0.6761400699615479)\n",
      "Word:  needlessly (0.6624394059181213)\n",
      "Word:  pleasantly (0.6472042798995972)\n",
      "Word:  opaque (0.6467126607894897)\n",
      "Word:  smoothly (0.6436067819595337)\n",
      "Word:  burner (0.641859769821167)\n",
      "Word:  overly (0.6401004791259766)\n",
      "Word:  obtuse (0.6346855163574219)\n",
      "Word:  surprisingly (0.6307271718978882)\n",
      "Word:  deceptively (0.6303049325942993)\n",
      "Word:  dense (0.6294069886207581)\n",
      "Word:  game (0.6223522424697876)\n",
      "Word:  ruleset (0.6195528507232666)\n",
      "Word:  overwrought (0.6195157170295715)\n",
      "Word:  unintuitive (0.6178824305534363)\n",
      "Word:  convoluted (0.6174805760383606)\n",
      "Word:  procedural (0.6157309412956238)\n",
      "Word:  elegant (0.6108782291412354)\n",
      "Word:  snappy (0.6088464856147766)\n",
      "Word:  amazingly (0.6036877036094666)\n",
      "Word:  themeless (0.6025003790855408)\n",
      "Word:  outstay (0.6014936566352844)\n",
      "Word:  daunting (0.6006429195404053)\n",
      "Word:  logical (0.5988865494728088)\n",
      "Word:  underwhelming (0.5983936786651611)\n",
      "Word:  intuitive (0.5976254940032959)\n",
      "Word:  mildly (0.5967795848846436)\n",
      "Word:  breezy (0.5958049297332764)\n",
      "Word:  unnecessarily (0.5943107008934021)\n",
      "Word:  tad (0.5938329696655273)\n",
      "Word:  bloated (0.5931901335716248)\n",
      "Word:  intriguing (0.5916005373001099)\n",
      "Word:  awkward (0.5893009901046753)\n",
      "Word:  cardplay (0.589017391204834)\n",
      "Word:  paced (0.586525559425354)\n",
      "Word:  affair (0.5846185684204102)\n",
      "Word:  delightful (0.5820063352584839)\n",
      "Word:  overcomplicated (0.5816836357116699)\n",
      "Word:  meaty (0.5815052390098572)\n",
      "Word:  messy (0.5812950134277344)\n",
      "Word:  lacking (0.5805266499519348)\n",
      "Word:  worried (0.5795466899871826)\n",
      "Word:  partly (0.5795101523399353)\n",
      "Word:  spreadsheet (0.5780944228172302)\n",
      "Word:  mathy (0.5777550935745239)\n",
      "Word:  unforgiving (0.5774984359741211)\n",
      "Word:  liking (0.577241063117981)\n",
      "Word:  strangely (0.5745790600776672)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  game (0.4773712158203125)\n",
      "Word:  engaging (0.3677656352519989)\n",
      "Word:  perspective (0.36608293652534485)\n",
      "Word:  blend (0.3646060526371002)\n",
      "Word:  fascinating (0.36335015296936035)\n",
      "Word:  delightful (0.35959404706954956)\n",
      "Word:  abstraction (0.35518360137939453)\n",
      "Word:  elegant (0.35503822565078735)\n",
      "Word:  emergent (0.35500097274780273)\n",
      "Word:  intriguing (0.34614983201026917)\n",
      "Word:  compelling (0.34601759910583496)\n",
      "Word:  wonderfully (0.34505096077919006)\n",
      "Word:  shallow (0.34260281920433044)\n",
      "Word:  eurogame (0.3386828303337097)\n",
      "Word:  nonetheless (0.33820605278015137)\n",
      "Word:  surprisingly (0.3349446654319763)\n",
      "Word:  polished (0.3334221839904785)\n",
      "Word:  elegantly (0.33110103011131287)\n",
      "Word:  ish (0.3282961845397949)\n",
      "Word:  narrative (0.32466715574264526)\n",
      "Word:  mechanical (0.32336148619651794)\n",
      "Word:  quirky (0.32090550661087036)\n",
      "Word:  themeless (0.32085564732551575)\n",
      "Word:  humor (0.3203854560852051)\n",
      "Word:  exceptionally (0.3178063631057739)\n",
      "Word:  simplistic (0.3163674771785736)\n",
      "Word:  immersive (0.31439682841300964)\n",
      "Word:  integrate (0.3141395151615143)\n",
      "Word:  genuinely (0.3125903606414795)\n",
      "Word:  brilliantly (0.31224149465560913)\n",
      "Word:  approachable (0.3111828863620758)\n",
      "Word:  elegance (0.3107835352420807)\n",
      "Word:  typical (0.30826398730278015)\n",
      "Word:  solid (0.30818241834640503)\n",
      "Word:  rich (0.3080267906188965)\n",
      "Word:  pleasantly (0.30166095495224)\n",
      "Word:  truly (0.3014013171195984)\n",
      "Word:  burner (0.301341712474823)\n",
      "Word:  presentation (0.30053645372390747)\n",
      "Word:  mechanically (0.3004279136657715)\n",
      "Word:  surprising (0.30032774806022644)\n",
      "Word:  refreshing (0.2991625666618347)\n",
      "Word:  scope (0.2987253665924072)\n",
      "Word:  crunchy (0.2981790006160736)\n",
      "Word:  gamey (0.29779160022735596)\n",
      "Word:  amazingly (0.2976565957069397)\n",
      "Word:  appealing (0.2973082363605499)\n",
      "Word:  simplicity (0.29688531160354614)\n",
      "Word:  wargame (0.29544103145599365)\n",
      "Word:  atmosphere (0.2951655387878418)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  unsatisfying (0.6033220291137695)\n",
      "Word:  frustrate (0.5822135210037231)\n",
      "Word:  game (0.5788781642913818)\n",
      "Word:  sour (0.5682976245880127)\n",
      "Word:  hose (0.5657349824905396)\n",
      "Word:  kingmaker (0.56471848487854)\n",
      "Word:  frustration (0.5585653185844421)\n",
      "Word:  unlucky (0.5579196214675903)\n",
      "Word:  outcome (0.55613112449646)\n",
      "Word:  unforgiving (0.5512720346450806)\n",
      "Word:  dumb (0.547441303730011)\n",
      "Word:  tendency (0.5433059334754944)\n",
      "Word:  kingmake (0.535000741481781)\n",
      "Word:  kingmaking (0.5346455574035645)\n",
      "Word:  swing (0.5328209400177002)\n",
      "Word:  tend (0.5294521450996399)\n",
      "Word:  purely (0.5212024450302124)\n",
      "Word:  eliminate (0.5194000005722046)\n",
      "Word:  spoil (0.5187367796897888)\n",
      "Word:  decisive (0.5131754279136658)\n",
      "Word:  runaway (0.5131080150604248)\n",
      "Word:  crucial (0.5123249292373657)\n",
      "Word:  inevitably (0.5114467144012451)\n",
      "Word:  prone (0.5085833072662354)\n",
      "Word:  paralysis (0.5084990859031677)\n",
      "Word:  bash (0.5075713992118835)\n",
      "Word:  devolve (0.5058497190475464)\n",
      "Word:  imbalance (0.504094123840332)\n",
      "Word:  unfair (0.49958091974258423)\n",
      "Word:  concerned (0.49794119596481323)\n",
      "Word:  ending (0.49771010875701904)\n",
      "Word:  contention (0.49662327766418457)\n",
      "Word:  merely (0.4958744943141937)\n",
      "Word:  analyze (0.4952048063278198)\n",
      "Word:  unpredictable (0.49218669533729553)\n",
      "Word:  groan (0.49020153284072876)\n",
      "Word:  frequently (0.48941653966903687)\n",
      "Word:  swingy (0.48786240816116333)\n",
      "Word:  screw (0.4850512146949768)\n",
      "Word:  fortune (0.4821380078792572)\n",
      "Word:  impact (0.4788917005062103)\n",
      "Word:  calculate (0.47747084498405457)\n",
      "Word:  mouth (0.47704553604125977)\n",
      "Word:  screwage (0.4764243960380554)\n",
      "Word:  consistently (0.47537922859191895)\n",
      "Word:  overcome (0.4736749827861786)\n",
      "Word:  drag (0.4728660583496094)\n",
      "Word:  anticlimactic (0.4707971215248108)\n",
      "Word:  predict (0.4706340432167053)\n",
      "Word:  accept (0.47049933671951294)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  manual (0.7669262886047363)\n",
      "Word:  instruction (0.7556840181350708)\n",
      "Word:  refer (0.7543908357620239)\n",
      "Word:  clarification (0.7518853545188904)\n",
      "Word:  unclear (0.728613018989563)\n",
      "Word:  faq (0.7232728004455566)\n",
      "Word:  vague (0.7208477258682251)\n",
      "Word:  reference (0.7106091976165771)\n",
      "Word:  description (0.7062393426895142)\n",
      "Word:  ambiguity (0.7023082971572876)\n",
      "Word:  index (0.6929852962493896)\n",
      "Word:  clarify (0.6856635808944702)\n",
      "Word:  wording (0.6824520826339722)\n",
      "Word:  typo (0.6814799308776855)\n",
      "Word:  file (0.6813913583755493)\n",
      "Word:  page (0.6769843697547913)\n",
      "Word:  glossary (0.676905632019043)\n",
      "Word:  translation (0.669908344745636)\n",
      "Word:  obscure (0.6672416925430298)\n",
      "Word:  confuse (0.6640616655349731)\n",
      "Word:  guide (0.64886075258255)\n",
      "Word:  consult (0.6411166191101074)\n",
      "Word:  symbology (0.6327361464500427)\n",
      "Word:  rulebook (0.6324336528778076)\n",
      "Word:  download (0.6264336705207825)\n",
      "Word:  read (0.6258617639541626)\n",
      "Word:  picture (0.6230963468551636)\n",
      "Word:  booklet (0.6230252385139465)\n",
      "Word:  errata (0.6182821989059448)\n",
      "Word:  sentence (0.6177085638046265)\n",
      "Word:  cheat (0.6164044141769409)\n",
      "Word:  awful (0.6160566806793213)\n",
      "Word:  interpret (0.6120431423187256)\n",
      "Word:  exception (0.6104176640510559)\n",
      "Word:  tutorial (0.6098426580429077)\n",
      "Word:  daunting (0.6036313772201538)\n",
      "Word:  decipher (0.603369951248169)\n",
      "Word:  interpretation (0.6033658385276794)\n",
      "Word:  ruling (0.6028182506561279)\n",
      "Word:  describe (0.5945345163345337)\n",
      "Word:  ambiguous (0.5916777849197388)\n",
      "Word:  scan (0.5907790064811707)\n",
      "Word:  definition (0.5891242027282715)\n",
      "Word:  riddle (0.5884565114974976)\n",
      "Word:  rewrite (0.5851098299026489)\n",
      "Word:  helpful (0.5845254063606262)\n",
      "Word:  sheet (0.5827034711837769)\n",
      "Word:  creator (0.582369327545166)\n",
      "Word:  reminder (0.5822089910507202)\n",
      "Word:  suggestion (0.5812253952026367)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  game (0.5921691656112671)\n",
      "Word:  buddy (0.45572859048843384)\n",
      "Word:  occasion (0.4421935975551605)\n",
      "Word:  convince (0.43732571601867676)\n",
      "Word:  halfway (0.4334877133369446)\n",
      "Word:  regularly (0.43213438987731934)\n",
      "Word:  thoroughly (0.42915692925453186)\n",
      "Word:  frustrate (0.42751508951187134)\n",
      "Word:  click (0.4256739914417267)\n",
      "Word:  consistently (0.4244070053100586)\n",
      "Word:  evening (0.4218379855155945)\n",
      "Word:  bored (0.4171876907348633)\n",
      "Word:  spoil (0.41489464044570923)\n",
      "Word:  realize (0.4090158939361572)\n",
      "Word:  surprised (0.40660881996154785)\n",
      "Word:  happy (0.4056186378002167)\n",
      "Word:  husband (0.40101611614227295)\n",
      "Word:  happily (0.40090346336364746)\n",
      "Word:  hook (0.3996933698654175)\n",
      "Word:  folk (0.3975977301597595)\n",
      "Word:  dumb (0.392930269241333)\n",
      "Word:  laugh (0.390802800655365)\n",
      "Word:  pleasantly (0.3896390199661255)\n",
      "Word:  hardcore (0.38790905475616455)\n",
      "Word:  sour (0.3875300884246826)\n",
      "Word:  bore (0.3874833285808563)\n",
      "Word:  nail (0.3867807686328888)\n",
      "Word:  partner (0.38639140129089355)\n",
      "Word:  merit (0.38422349095344543)\n",
      "Word:  entertain (0.3823121190071106)\n",
      "Word:  girlfriend (0.3798455595970154)\n",
      "Word:  willing (0.3774564862251282)\n",
      "Word:  excite (0.37644729018211365)\n",
      "Word:  rarely (0.3742343783378601)\n",
      "Word:  tired (0.37217065691947937)\n",
      "Word:  hilarious (0.3718510866165161)\n",
      "Word:  stressful (0.3703867793083191)\n",
      "Word:  excited (0.3677303194999695)\n",
      "Word:  worried (0.36525100469589233)\n",
      "Word:  anymore (0.3646891415119171)\n",
      "Word:  anytime (0.3642377257347107)\n",
      "Word:  unsatisfying (0.3638290762901306)\n",
      "Word:  got (0.3625745475292206)\n",
      "Word:  realise (0.36197227239608765)\n",
      "Word:  wife (0.36163923144340515)\n",
      "Word:  everybody (0.3604484796524048)\n",
      "Word:  gaming (0.3596789836883545)\n",
      "Word:  stress (0.35882481932640076)\n",
      "Word:  hobby (0.3584592938423157)\n",
      "Word:  competitively (0.3578355312347412)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  game (0.5415220260620117)\n",
      "Word:  rub (0.41540825366973877)\n",
      "Word:  like (0.4131646156311035)\n",
      "Word:  horse (0.4076867699623108)\n",
      "Word:  guesser (0.4066649079322815)\n",
      "Word:  nail (0.4043753147125244)\n",
      "Word:  bet (0.40345633029937744)\n",
      "Word:  opposite (0.39998358488082886)\n",
      "Word:  instantly (0.3984445035457611)\n",
      "Word:  spreadsheet (0.39619237184524536)\n",
      "Word:  desperate (0.39571526646614075)\n",
      "Word:  flat (0.3935650587081909)\n",
      "Word:  shut (0.39112210273742676)\n",
      "Word:  weird (0.39038214087486267)\n",
      "Word:  deduce (0.38991889357566833)\n",
      "Word:  stupid (0.3820075988769531)\n",
      "Word:  mess (0.38044843077659607)\n",
      "Word:  bizarre (0.3797985017299652)\n",
      "Word:  giver (0.37908804416656494)\n",
      "Word:  steer (0.37898463010787964)\n",
      "Word:  fragile (0.37728753685951233)\n",
      "Word:  struggle (0.3753736615180969)\n",
      "Word:  unforgiving (0.37181323766708374)\n",
      "Word:  somebody (0.371113121509552)\n",
      "Word:  click (0.3709513247013092)\n",
      "Word:  contention (0.370883047580719)\n",
      "Word:  mouse (0.3707764148712158)\n",
      "Word:  slog (0.3694848418235779)\n",
      "Word:  overlook (0.36938947439193726)\n",
      "Word:  unable (0.36878564953804016)\n",
      "Word:  impulse (0.36809903383255005)\n",
      "Word:  gangster (0.36771970987319946)\n",
      "Word:  convince (0.36684930324554443)\n",
      "Word:  everybody (0.36640626192092896)\n",
      "Word:  trouble (0.3644838035106659)\n",
      "Word:  care (0.3635779619216919)\n",
      "Word:  pretend (0.3632117211818695)\n",
      "Word:  repeatedly (0.3630257248878479)\n",
      "Word:  halfway (0.3626580834388733)\n",
      "Word:  accurately (0.3624598979949951)\n",
      "Word:  disagree (0.360898494720459)\n",
      "Word:  bankrupt (0.3604124188423157)\n",
      "Word:  frustrate (0.3582306504249573)\n",
      "Word:  unsatisfying (0.3581768870353699)\n",
      "Word:  tendency (0.3563786745071411)\n",
      "Word:  precise (0.3555762767791748)\n",
      "Word:  eye (0.35438287258148193)\n",
      "Word:  kingmaker (0.35356056690216064)\n",
      "Word:  insider (0.35307344794273376)\n",
      "Word:  intentionally (0.35254180431365967)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  hybrid (0.7440182566642761)\n",
      "Word:  blend (0.7092220783233643)\n",
      "Word:  ameritrash (0.6892321109771729)\n",
      "Word:  laying (0.6815437078475952)\n",
      "Word:  eurogame (0.6702263355255127)\n",
      "Word:  reminiscent (0.6642575860023499)\n",
      "Word:  lite (0.6627805829048157)\n",
      "Word:  esque (0.6623148918151855)\n",
      "Word:  mixture (0.6603304147720337)\n",
      "Word:  racing (0.6510967016220093)\n",
      "Word:  puzzly (0.6462447047233582)\n",
      "Word:  disguise (0.6450459957122803)\n",
      "Word:  soulless (0.636613667011261)\n",
      "Word:  mash (0.6362807154655457)\n",
      "Word:  tl;dr (0.6347260475158691)\n",
      "Word:  mancala (0.6278982758522034)\n",
      "Word:  vein (0.6276187896728516)\n",
      "Word:  closed (0.6260274648666382)\n",
      "Word:  refreshing (0.62299644947052)\n",
      "Word:  pseudo (0.6206661462783813)\n",
      "Word:  novel (0.6184481382369995)\n",
      "Word:  drafting (0.6181926727294922)\n",
      "Word:  spacial (0.613356351852417)\n",
      "Word:  mashup (0.6132453680038452)\n",
      "Word:  game (0.6126136779785156)\n",
      "Word:  deckbuilding (0.6094393134117126)\n",
      "Word:  implementation (0.6075987815856934)\n",
      "Word:  programming (0.6053421497344971)\n",
      "Word:  bingo (0.6037846803665161)\n",
      "Word:  yahtzee (0.6012725830078125)\n",
      "Word:  groundbreake (0.6003245115280151)\n",
      "Word:  typical (0.5999057292938232)\n",
      "Word:  domino (0.5998837947845459)\n",
      "Word:  seamlessly (0.599882960319519)\n",
      "Word:  fulfillment (0.5964912176132202)\n",
      "Word:  tetris (0.5950039625167847)\n",
      "Word:  spatial (0.5922860503196716)\n",
      "Word:  rolling (0.5908681154251099)\n",
      "Word:  ish (0.5903549790382385)\n",
      "Word:  akin (0.5892736315727234)\n",
      "Word:  rondel (0.5879426002502441)\n",
      "Word:  lover (0.5874840021133423)\n",
      "Word:  fare (0.5856310725212097)\n",
      "Word:  collecting (0.5853459239006042)\n",
      "Word:  polyomino (0.5824458599090576)\n",
      "Word:  settlers (0.5804218053817749)\n",
      "Word:  stacking (0.5778466463088989)\n",
      "Word:  simplified (0.577023983001709)\n",
      "Word:  laukat (0.575873851776123)\n",
      "Word:  betting (0.5745442509651184)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  opportunistic (0.8117303848266602)\n",
      "Word:  tradeoff (0.7642819881439209)\n",
      "Word:  tactically (0.7567365169525146)\n",
      "Word:  pathway (0.7517833709716797)\n",
      "Word:  angst (0.7486473321914673)\n",
      "Word:  uncertainty (0.7460789084434509)\n",
      "Word:  impactful (0.7424609661102295)\n",
      "Word:  multitude (0.7400436401367188)\n",
      "Word:  lever (0.7395414113998413)\n",
      "Word:  concentrate (0.7375749945640564)\n",
      "Word:  strategize (0.7369427680969238)\n",
      "Word:  subtle (0.7272176742553711)\n",
      "Word:  constrain (0.7239189147949219)\n",
      "Word:  avenue (0.7196985483169556)\n",
      "Word:  inject (0.7187424302101135)\n",
      "Word:  consideration (0.7142328023910522)\n",
      "Word:  cardplay (0.7135878801345825)\n",
      "Word:  instability (0.7113625407218933)\n",
      "Word:  employ (0.7104166150093079)\n",
      "Word:  accomplishment (0.7104020118713379)\n",
      "Word:  thrilling (0.7095569372177124)\n",
      "Word:  emerge (0.7066766023635864)\n",
      "Word:  intricate (0.7055697441101074)\n",
      "Word:  myriad (0.705276370048523)\n",
      "Word:  flexible (0.7049739360809326)\n",
      "Word:  positioning (0.7047736644744873)\n",
      "Word:  maneuvering (0.7041934132575989)\n",
      "Word:  gamey (0.7034808397293091)\n",
      "Word:  input (0.7032713294029236)\n",
      "Word:  juggle (0.7032235860824585)\n",
      "Word:  incentivize (0.7021213173866272)\n",
      "Word:  dilemma (0.7013013958930969)\n",
      "Word:  strategical (0.6990692615509033)\n",
      "Word:  permutation (0.698917031288147)\n",
      "Word:  execution (0.697234570980072)\n",
      "Word:  weigh (0.695624589920044)\n",
      "Word:  interlock (0.6947851181030273)\n",
      "Word:  navigate (0.6941272020339966)\n",
      "Word:  screwage (0.6939493417739868)\n",
      "Word:  hinder (0.6915583610534668)\n",
      "Word:  undermine (0.6912261247634888)\n",
      "Word:  tempo (0.6910613775253296)\n",
      "Word:  arise (0.6887733936309814)\n",
      "Word:  optimization (0.6869663000106812)\n",
      "Word:  sufficient (0.6868752241134644)\n",
      "Word:  optimisation (0.6866395473480225)\n",
      "Word:  meaningfully (0.6866218447685242)\n",
      "Word:  winning (0.6848711967468262)\n",
      "Word:  differentiation (0.6846801042556763)\n",
      "Word:  definite (0.6822516918182373)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  cooperative (0.6950154304504395)\n",
      "Word:  coop (0.6790333986282349)\n",
      "Word:  operative (0.6622250080108643)\n",
      "Word:  semi (0.660285472869873)\n",
      "Word:  traitor (0.6160367727279663)\n",
      "Word:  competitive (0.6037938594818115)\n",
      "Word:  co (0.5896788835525513)\n",
      "Word:  communication (0.5894654989242554)\n",
      "Word:  deckbuilder (0.5825397968292236)\n",
      "Word:  social (0.579716145992279)\n",
      "Word:  taker (0.5752876400947571)\n",
      "Word:  1v1 (0.5683180689811707)\n",
      "Word:  op (0.5666926503181458)\n",
      "Word:  2p (0.5593782663345337)\n",
      "Word:  introduction (0.5533348321914673)\n",
      "Word:  deduction (0.5425180792808533)\n",
      "Word:  intrigue (0.540217399597168)\n",
      "Word:  ish (0.5394406318664551)\n",
      "Word:  partnership (0.5362846851348877)\n",
      "Word:  teamwork (0.5337656140327454)\n",
      "Word:  hidden (0.5259993672370911)\n",
      "Word:  multiplayer (0.522489607334137)\n",
      "Word:  deception (0.5173109769821167)\n",
      "Word:  overlord (0.5163823366165161)\n",
      "Word:  game (0.5163561701774597)\n",
      "Word:  3p (0.5031802654266357)\n",
      "Word:  bluffing (0.5029006004333496)\n",
      "Word:  quarterbacking (0.502562940120697)\n",
      "Word:  puzzly (0.5017064809799194)\n",
      "Word:  sport (0.5011194348335266)\n",
      "Word:  crawler (0.49831926822662354)\n",
      "Word:  deckbuilding (0.49788981676101685)\n",
      "Word:  implementation (0.4977469742298126)\n",
      "Word:  roleplay (0.4972810745239258)\n",
      "Word:  suited (0.4946727752685547)\n",
      "Word:  taking (0.493815541267395)\n",
      "Word:  blend (0.4929700791835785)\n",
      "Word:  lover (0.4927005171775818)\n",
      "Word:  confrontation (0.49138566851615906)\n",
      "Word:  traditional (0.4907187223434448)\n",
      "Word:  cooperatively (0.4902454614639282)\n",
      "Word:  themed (0.4898734986782074)\n",
      "Word:  ops (0.4860568344593048)\n",
      "Word:  asymmetrical (0.48396632075309753)\n",
      "Word:  lotr (0.47987765073776245)\n",
      "Word:  fury (0.4741773009300232)\n",
      "Word:  emulate (0.4720420837402344)\n",
      "Word:  cdg (0.47104090452194214)\n",
      "Word:  detective (0.4695020318031311)\n",
      "Word:  mash (0.4693111777305603)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  game (0.3884214162826538)\n",
      "Word:  adult (0.2237805277109146)\n",
      "Word:  hardcore (0.20524220168590546)\n",
      "Word:  burner (0.1988946795463562)\n",
      "Word:  gaming (0.19088944792747498)\n",
      "Word:  filler (0.1787584275007248)\n",
      "Word:  pleasantly (0.1786329597234726)\n",
      "Word:  entertain (0.1786070615053177)\n",
      "Word:  seasoned (0.175355926156044)\n",
      "Word:  gamer (0.1737576276063919)\n",
      "Word:  thoroughly (0.17011508345603943)\n",
      "Word:  alike (0.16884486377239227)\n",
      "Word:  18xx (0.16823381185531616)\n",
      "Word:  happily (0.16706837713718414)\n",
      "Word:  relax (0.16478590667247772)\n",
      "Word:  play (0.16265574097633362)\n",
      "Word:  hobby (0.16189582645893097)\n",
      "Word:  surprised (0.15739069879055023)\n",
      "Word:  girlfriend (0.1538839340209961)\n",
      "Word:  hearted (0.15311381220817566)\n",
      "Word:  enjoy (0.1527438461780548)\n",
      "Word:  approachable (0.15262150764465332)\n",
      "Word:  overstay (0.152488574385643)\n",
      "Word:  pleasant (0.14810681343078613)\n",
      "Word:  experienced (0.14665542542934418)\n",
      "Word:  introductory (0.14623025059700012)\n",
      "Word:  ish (0.14488649368286133)\n",
      "Word:  smoothly (0.14436355233192444)\n",
      "Word:  evening (0.14232346415519714)\n",
      "Word:  absolutely (0.1403324455022812)\n",
      "Word:  casual (0.14019237458705902)\n",
      "Word:  interested (0.13972529768943787)\n",
      "Word:  experience (0.1378527134656906)\n",
      "Word:  happy (0.1377549022436142)\n",
      "Word:  solid (0.1374197155237198)\n",
      "Word:  crowd (0.13728591799736023)\n",
      "Word:  group (0.13725382089614868)\n",
      "Word:  gateway (0.13614243268966675)\n",
      "Word:  addictive (0.13362504541873932)\n",
      "Word:  willing (0.13329091668128967)\n",
      "Word:  bore (0.1329154074192047)\n",
      "Word:  fun (0.13257591426372528)\n",
      "Word:  mildly (0.13146048784255981)\n",
      "Word:  mood (0.13090670108795166)\n",
      "Word:  deserve (0.1283484846353531)\n",
      "Word:  introduction (0.12821367383003235)\n",
      "Word:  enjoyable (0.12800881266593933)\n",
      "Word:  surprisingly (0.12762098014354706)\n",
      "Word:  appreciate (0.12761515378952026)\n",
      "Word:  opinion (0.12700681388378143)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  game (0.6266300678253174)\n",
      "Word:  entertain (0.5162835121154785)\n",
      "Word:  hearted (0.5033948421478271)\n",
      "Word:  breezy (0.4971920847892761)\n",
      "Word:  casual (0.48994219303131104)\n",
      "Word:  crunchy (0.482837438583374)\n",
      "Word:  meaty (0.4756968319416046)\n",
      "Word:  filler (0.4739440083503723)\n",
      "Word:  thinky (0.47389209270477295)\n",
      "Word:  mildly (0.46824073791503906)\n",
      "Word:  burner (0.46283072233200073)\n",
      "Word:  relax (0.4586026072502136)\n",
      "Word:  seasoned (0.4555357098579407)\n",
      "Word:  adult (0.45524585247039795)\n",
      "Word:  gateway (0.4506400227546692)\n",
      "Word:  pleasant (0.4491428732872009)\n",
      "Word:  overstay (0.4489482045173645)\n",
      "Word:  hardcore (0.4486158788204193)\n",
      "Word:  approachable (0.4470982551574707)\n",
      "Word:  surprisingly (0.4455484449863434)\n",
      "Word:  puzzly (0.4447537660598755)\n",
      "Word:  meatier (0.44357261061668396)\n",
      "Word:  alike (0.44280266761779785)\n",
      "Word:  introductory (0.4397343397140503)\n",
      "Word:  suitable (0.43755561113357544)\n",
      "Word:  lightweight (0.4361969232559204)\n",
      "Word:  outstay (0.4357450008392334)\n",
      "Word:  ish (0.43093836307525635)\n",
      "Word:  pleasantly (0.42658695578575134)\n",
      "Word:  gamer (0.416765421628952)\n",
      "Word:  meat (0.412181556224823)\n",
      "Word:  involved (0.41172897815704346)\n",
      "Word:  relaxing (0.41002821922302246)\n",
      "Word:  pretzel (0.4098290801048279)\n",
      "Word:  shallow (0.409121036529541)\n",
      "Word:  fare (0.40903401374816895)\n",
      "Word:  crowd (0.40888768434524536)\n",
      "Word:  confrontational (0.40875688195228577)\n",
      "Word:  deceptively (0.40628576278686523)\n",
      "Word:  addictive (0.4057789146900177)\n",
      "Word:  entertaining (0.40466636419296265)\n",
      "Word:  experienced (0.40357643365859985)\n",
      "Word:  thinking (0.4005781412124634)\n",
      "Word:  accessible (0.39980027079582214)\n",
      "Word:  mood (0.3967447280883789)\n",
      "Word:  cardplay (0.3936377763748169)\n",
      "Word:  relaxed (0.3898770213127136)\n",
      "Word:  lover (0.38870924711227417)\n",
      "Word:  stressful (0.38816913962364197)\n",
      "Word:  light (0.38789254426956177)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  west (0.2921501398086548)\n",
      "Word:  boardgame (0.27602171897888184)\n",
      "Word:  game (0.2703309953212738)\n",
      "Word:  ah (0.25875163078308105)\n",
      "Word:  tv (0.25391247868537903)\n",
      "Word:  justice (0.24728035926818848)\n",
      "Word:  classic (0.2359119951725006)\n",
      "Word:  warhammer (0.22573654353618622)\n",
      "Word:  school (0.22363696992397308)\n",
      "Word:  modern (0.21775802969932556)\n",
      "Word:  league (0.2152937650680542)\n",
      "Word:  holmes (0.21335837244987488)\n",
      "Word:  movie (0.21052655577659607)\n",
      "Word:  consulting (0.2068158984184265)\n",
      "Word:  d&d (0.20678414404392242)\n",
      "Word:  tabletop (0.2054549902677536)\n",
      "Word:  horror (0.20149242877960205)\n",
      "Word:  john (0.19840148091316223)\n",
      "Word:  rpg (0.19474507868289948)\n",
      "Word:  fi (0.19263598322868347)\n",
      "Word:  wars (0.1901301145553589)\n",
      "Word:  sister (0.18977782130241394)\n",
      "Word:  martin (0.18941783905029297)\n",
      "Word:  matt (0.18751758337020874)\n",
      "Word:  detective (0.1864367127418518)\n",
      "Word:  dad (0.18362584710121155)\n",
      "Word:  stefan (0.18318641185760498)\n",
      "Word:  wallace (0.18228891491889954)\n",
      "Word:  brother (0.18156935274600983)\n",
      "Word:  popular (0.18148480355739594)\n",
      "Word:  italy (0.18106873333454132)\n",
      "Word:  summer (0.17910507321357727)\n",
      "Word:  cthulhu (0.1785825490951538)\n",
      "Word:  franchise (0.1775551736354828)\n",
      "Word:  lotr (0.17740638554096222)\n",
      "Word:  sci (0.17707519233226776)\n",
      "Word:  arkham (0.17558656632900238)\n",
      "Word:  trek (0.17457440495491028)\n",
      "Word:  music (0.1737181544303894)\n",
      "Word:  old (0.17315398156642914)\n",
      "Word:  chronicles (0.17245203256607056)\n",
      "Word:  woman (0.1681571751832962)\n",
      "Word:  american (0.16721679270267487)\n",
      "Word:  des (0.16685880720615387)\n",
      "Word:  flames (0.16530409455299377)\n",
      "Word:  reiner (0.1647256761789322)\n",
      "Word:  clark (0.16399797797203064)\n",
      "Word:  lcg (0.1637038141489029)\n",
      "Word:  hear (0.1624433547258377)\n",
      "Word:  lewis (0.1575048267841339)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  sturdy (0.5306850075721741)\n",
      "Word:  plastic (0.526809811592102)\n",
      "Word:  cardboard (0.5248210430145264)\n",
      "Word:  thick (0.524773120880127)\n",
      "Word:  chunky (0.4962767958641052)\n",
      "Word:  tray (0.4849882125854492)\n",
      "Word:  standee (0.4844663739204407)\n",
      "Word:  3d (0.47651898860931396)\n",
      "Word:  insert (0.4713388979434967)\n",
      "Word:  colorful (0.4707520008087158)\n",
      "Word:  figurine (0.4643242061138153)\n",
      "Word:  oversized (0.45852911472320557)\n",
      "Word:  glossy (0.4568088948726654)\n",
      "Word:  wooden (0.45424848794937134)\n",
      "Word:  inside (0.44801777601242065)\n",
      "Word:  organizer (0.4451325535774231)\n",
      "Word:  wood (0.4450645446777344)\n",
      "Word:  flimsy (0.44017553329467773)\n",
      "Word:  bright (0.43654289841651917)\n",
      "Word:  pencil (0.4361356198787689)\n",
      "Word:  mat (0.4284626245498657)\n",
      "Word:  paint (0.42262381315231323)\n",
      "Word:  storage (0.4203123450279236)\n",
      "Word:  fancy (0.416427880525589)\n",
      "Word:  mount (0.4148292541503906)\n",
      "Word:  illustration (0.4145570695400238)\n",
      "Word:  stunning (0.41137126088142395)\n",
      "Word:  functional (0.4046948552131653)\n",
      "Word:  acrylic (0.4014567732810974)\n",
      "Word:  magnetic (0.39989233016967773)\n",
      "Word:  paper (0.39660555124282837)\n",
      "Word:  printed (0.3947475552558899)\n",
      "Word:  tin (0.389588326215744)\n",
      "Word:  custom (0.3893613815307617)\n",
      "Word:  thin (0.3875080347061157)\n",
      "Word:  lift (0.3864583373069763)\n",
      "Word:  lid (0.385906457901001)\n",
      "Word:  underneath (0.3820796608924866)\n",
      "Word:  cloth (0.3804987668991089)\n",
      "Word:  fold (0.3799651265144348)\n",
      "Word:  s2 (0.37670814990997314)\n",
      "Word:  cute (0.3755684494972229)\n",
      "Word:  minis (0.3755001127719879)\n",
      "Word:  sculpt (0.37357115745544434)\n",
      "Word:  transparent (0.3716597557067871)\n",
      "Word:  photo (0.37098729610443115)\n",
      "Word:  lovely (0.37058717012405396)\n",
      "Word:  dutrait (0.37045636773109436)\n",
      "Word:  neatly (0.36729469895362854)\n",
      "Word:  neoprene (0.36508992314338684)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  x1 (0.8447774648666382)\n",
      "Word:  x3 (0.8430130481719971)\n",
      "Word:  tact (0.8116634488105774)\n",
      "Word:  mechanics (0.8063971996307373)\n",
      "Word:  david (0.800774097442627)\n",
      "Word:  | (0.7889299988746643)\n",
      "Word:  star::star::star::star::star::star::star::star::nostar::nostar (0.7847887277603149)\n",
      "Word:  star::star::star::star::star::star::star::star::star::nostar (0.784450352191925)\n",
      "Word:  x15 (0.7760673761367798)\n",
      "Word:  william (0.7631304860115051)\n",
      "Word:  unattractive (0.7631261348724365)\n",
      "Word:  2–4 (0.7626053094863892)\n",
      "Word:  plays (0.760918915271759)\n",
      "Word:  willingness (0.7522631883621216)\n",
      "Word:  strat (0.7497013211250305)\n",
      "Word:  star::star::star::star::star::star::star::star::star::halfstar (0.7482808232307434)\n",
      "Word:  x5 (0.7413650751113892)\n",
      "Word:  logistics (0.7341721653938293)\n",
      "Word:  ♥ (0.7313054203987122)\n",
      "Word:  aesthetics (0.7309454083442688)\n",
      "Word:  t (0.7260608077049255)\n",
      "Word:  oca (0.7215799689292908)\n",
      "Word:  navir (0.7214893698692322)\n",
      "Word:  comp (0.7179008722305298)\n",
      "Word:  minutes (0.7176938652992249)\n",
      "Word:  players (0.708856463432312)\n",
      "Word:  g (0.7085504531860352)\n",
      "Word:  star::star::star::star::nostar (0.708336353302002)\n",
      "Word:  google (0.7041037678718567)\n",
      "Word:  x2 (0.7016700506210327)\n",
      "Word:  w (0.6975979804992676)\n",
      "Word:  ganadas (0.6954028606414795)\n",
      "Word:  amassgames (0.6942694783210754)\n",
      "Word:  p (0.6940174102783203)\n",
      "Word:  scalability (0.692914605140686)\n",
      "Word:  best (0.6905487775802612)\n",
      "Word:  ease (0.686775267124176)\n",
      "Word:  criteria (0.6827048659324646)\n",
      "Word:  evaluation (0.678814172744751)\n",
      "Word:  d (0.6786755323410034)\n",
      "Word:  grade (0.6781893372535706)\n",
      "Word:  components (0.6777949333190918)\n",
      "Word:  tentative (0.6759155988693237)\n",
      "Word:  rules (0.6751183271408081)\n",
      "Word:  h (0.672966480255127)\n",
      "Word:  eric (0.6711660623550415)\n",
      "Word:  star::star::star::nostar::nostar (0.6680030822753906)\n",
      "Word:  < (0.6677695512771606)\n",
      "Word:  mythic (0.6653373837471008)\n",
      "Word:  ranking (0.6589860320091248)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  artillery (0.7015180587768555)\n",
      "Word:  goblin (0.6927782297134399)\n",
      "Word:  protect (0.6854479312896729)\n",
      "Word:  cavalry (0.6812537908554077)\n",
      "Word:  russia (0.6746402382850647)\n",
      "Word:  retreat (0.6717520952224731)\n",
      "Word:  invader (0.667899489402771)\n",
      "Word:  infantry (0.6656069755554199)\n",
      "Word:  defend (0.6643003225326538)\n",
      "Word:  captain (0.6640777587890625)\n",
      "Word:  horde (0.6633657813072205)\n",
      "Word:  navy (0.6552903652191162)\n",
      "Word:  president (0.6455618143081665)\n",
      "Word:  clash (0.6444470286369324)\n",
      "Word:  defender (0.6386553049087524)\n",
      "Word:  fleet (0.6363096237182617)\n",
      "Word:  tank (0.6339815855026245)\n",
      "Word:  soviet (0.6339150667190552)\n",
      "Word:  invade (0.6315914392471313)\n",
      "Word:  vampire (0.6302281022071838)\n",
      "Word:  hitler (0.6282170414924622)\n",
      "Word:  cave (0.6262387037277222)\n",
      "Word:  guard (0.6231433153152466)\n",
      "Word:  south (0.6224014759063721)\n",
      "Word:  alien (0.6209558844566345)\n",
      "Word:  armor (0.620598554611206)\n",
      "Word:  british (0.6194770336151123)\n",
      "Word:  dangerous (0.6194457411766052)\n",
      "Word:  sword (0.619251012802124)\n",
      "Word:  allies (0.6184663772583008)\n",
      "Word:  arm (0.6155057549476624)\n",
      "Word:  police (0.6131775379180908)\n",
      "Word:  portal (0.6126969456672668)\n",
      "Word:  assault (0.6106688976287842)\n",
      "Word:  sea (0.6105917692184448)\n",
      "Word:  naval (0.6086974740028381)\n",
      "Word:  samurai (0.6086737513542175)\n",
      "Word:  conquer (0.6085652112960815)\n",
      "Word:  soldier (0.6084246635437012)\n",
      "Word:  russians (0.6076776385307312)\n",
      "Word:  corps (0.6071474552154541)\n",
      "Word:  fortress (0.6065139770507812)\n",
      "Word:  shark (0.6059122085571289)\n",
      "Word:  cast (0.6052471399307251)\n",
      "Word:  royal (0.6013787388801575)\n",
      "Word:  cannon (0.5997941493988037)\n",
      "Word:  ward (0.5995759963989258)\n",
      "Word:  agent (0.5994858741760254)\n",
      "Word:  march (0.5948381423950195)\n",
      "Word:  wound (0.594394862651825)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  plantation (0.7400959730148315)\n",
      "Word:  palace (0.734659731388092)\n",
      "Word:  crop (0.7271112203598022)\n",
      "Word:  cattle (0.7182929515838623)\n",
      "Word:  oil (0.7146482467651367)\n",
      "Word:  railway (0.7113857269287109)\n",
      "Word:  harvest (0.7096148729324341)\n",
      "Word:  herd (0.7085524201393127)\n",
      "Word:  village (0.6984598636627197)\n",
      "Word:  erect (0.6982686519622803)\n",
      "Word:  transport (0.6971472501754761)\n",
      "Word:  secure (0.6949064135551453)\n",
      "Word:  infrastructure (0.6944806575775146)\n",
      "Word:  cargo (0.6882384419441223)\n",
      "Word:  monument (0.6870652437210083)\n",
      "Word:  hut (0.6869834661483765)\n",
      "Word:  estate (0.6869058609008789)\n",
      "Word:  statue (0.6864882111549377)\n",
      "Word:  fence (0.6855065822601318)\n",
      "Word:  revenue (0.6835141181945801)\n",
      "Word:  merchant (0.6834568977355957)\n",
      "Word:  farm (0.6826958656311035)\n",
      "Word:  trader (0.6787288188934326)\n",
      "Word:  mining (0.6765819787979126)\n",
      "Word:  port (0.6749503016471863)\n",
      "Word:  amass (0.6741054058074951)\n",
      "Word:  fame (0.6738505363464355)\n",
      "Word:  industry (0.6712881922721863)\n",
      "Word:  float (0.6704230308532715)\n",
      "Word:  colony (0.6695261001586914)\n",
      "Word:  flood (0.6684536933898926)\n",
      "Word:  fish (0.6682376861572266)\n",
      "Word:  export (0.6670218110084534)\n",
      "Word:  airline (0.6650482416152954)\n",
      "Word:  enclose (0.6649942398071289)\n",
      "Word:  dike (0.6648473739624023)\n",
      "Word:  pagoda (0.664513349533081)\n",
      "Word:  colonize (0.6643977761268616)\n",
      "Word:  multiplier (0.6616021394729614)\n",
      "Word:  fulfil (0.6608903408050537)\n",
      "Word:  farmer (0.6606791019439697)\n",
      "Word:  brick (0.6586485505104065)\n",
      "Word:  coal (0.6584929823875427)\n",
      "Word:  district (0.6581224799156189)\n",
      "Word:  stone (0.6580386161804199)\n",
      "Word:  acquisition (0.6578646302223206)\n",
      "Word:  airship (0.6571296453475952)\n",
      "Word:  profitable (0.6567765474319458)\n",
      "Word:  ore (0.6557791233062744)\n",
      "Word:  dam (0.6526886224746704)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on configuration: {'aspect_size': 16, 'embedding_size': 200, 'epochs': 1, 'batch_size': 64, 'learning_rate': 0.001, 'decay_rate': 0.93, 'momentum': 0.9400000000000001, 'negative_sample_size': 15}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e02b120dba894f5282e17b49cb8a846a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c51347ca448d479eac3927fbe941bef9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 119085 words, keeping 4857 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 239721 words, keeping 5207 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 359702 words, keeping 5263 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 477300 words, keeping 5274 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 598508 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 721182 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 838524 words, keeping 5279 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 957006 words, keeping 5280 word types\n",
      "INFO:gensim.models.word2vec:collected 5280 word types from a corpus of 1003308 raw words and 83855 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 5280 unique words (100.00% of original 5280, drops 0)', 'datetime': '2025-01-11T15:18:53.010293', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1003308 word corpus (100.00% of original 1003308, drops 0)', 'datetime': '2025-01-11T15:18:53.010293', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 5280 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 46 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 802044.7673554413 word corpus (79.9%% of prior 1003308)', 'datetime': '2025-01-11T15:18:53.028821', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 5280 words and 200 dimensions: 11088000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-11T15:18:53.062020', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 8 workers on 5280 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-01-11T15:18:53.063019', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 1003308 raw words (801844 effective words) took 0.5s, 1768769 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 1003308 raw words (802440 effective words) took 0.4s, 1902196 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 1003308 raw words (801677 effective words) took 0.5s, 1779396 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 1003308 raw words (802341 effective words) took 0.5s, 1710008 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 101 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 12 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 1003308 raw words (801474 effective words) took 0.4s, 1870532 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 5016540 raw words (4009776 effective words) took 2.3s, 1767372 effective words/s', 'datetime': '2025-01-11T15:18:55.332164', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=5280, vector_size=200, alpha=0.025>', 'datetime': '2025-01-11T15:18:55.332164', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './output/tuning_212f55cb-f6a7-4cac-99d0-41cd39f16592/embeddings.keras', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-01-11T15:18:55.332164', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './output/tuning_212f55cb-f6a7-4cac-99d0-41cd39f16592/embeddings.keras', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./output/tuning_212f55cb-f6a7-4cac-99d0-41cd39f16592/embeddings.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n",
      "Loading dataset from file: ../output/dataset/pre-processed/tuning.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/83855 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "522db3d4ffdf4de399021bbe04e7dd45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  274 . The limit is set to 80 tokens.\n",
      "We loose information on 54 points.This is 0.06439687555900066% of the dataset.\n",
      "Padding sequences to length (80).\n",
      "Training process in progress..\n",
      "\u001B[1m 113/1049\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m1:57\u001B[0m 125ms/step - loss: 15.0733 - max_margin_loss: 15.0049"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 43\u001B[0m\n\u001B[0;32m     41\u001B[0m train, validation \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mrandom_split(ds, [\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.2\u001B[39m], generator\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mGenerator()\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m42\u001B[39m))\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining process in progress..\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 43\u001B[0m results, iteration_model \u001B[38;5;241m=\u001B[39m \u001B[43mmanager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_train_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# We evaluate on the relative coherence between topics.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\train.py:131\u001B[0m, in \u001B[0;36mAbaeModelManager.run_train_process\u001B[1;34m(self, dataset, consider_stored, optimizer)\u001B[0m\n\u001B[0;32m    128\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mdataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    130\u001B[0m \u001B[38;5;66;03m# Now run the training process and return the process history.\u001B[39;00m\n\u001B[1;32m--> 131\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_t_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Every epoch the model is persisted on the FS.\u001B[39;49;00m\n\u001B[0;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[43mModelCheckpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./tmp/ckpt/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.keras\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_margin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_t_model\u001B[38;5;241m.\u001B[39msave(considered_path)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_generator\u001B[38;5;241m.\u001B[39mmake_model(considered_path)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:253\u001B[0m, in \u001B[0;36mTorchTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    252\u001B[0m logs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 253\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Callbacks\u001B[39;49;00m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:132\u001B[0m, in \u001B[0;36mEpochIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps_per_execution):\n\u001B[1;32m--> 132\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m         buffer\u001B[38;5;241m.\u001B[39mappend(data)\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m step, buffer\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\dataset.py:84\u001B[0m, in \u001B[0;36mPositiveNegativeCommentGeneratorDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# For each input sentence, we randomly sample m sentences from our training data as negative samples\u001B[39;00m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;66;03m# Stack to get rid of lists and create nice numpy arrays to be elaborated/\\\u001B[39;00m\n\u001B[0;32m     83\u001B[0m     sample \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mat[index])\n\u001B[1;32m---> 84\u001B[0m     negative_samples \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnegative_size\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [sample, negative_samples], \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6118\u001B[0m, in \u001B[0;36mNDFrame.sample\u001B[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001B[0m\n\u001B[0;32m   6115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   6116\u001B[0m     weights \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39mpreprocess_weights(\u001B[38;5;28mself\u001B[39m, weights, axis)\n\u001B[1;32m-> 6118\u001B[0m sampled_indices \u001B[38;5;241m=\u001B[39m \u001B[43msample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6119\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(sampled_indices, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m   6121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ignore_index:\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\core\\sample.py:152\u001B[0m, in \u001B[0;36msample\u001B[1;34m(obj_len, size, replace, weights, random_state)\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    150\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weights: weights sum to zero\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mastype(\n\u001B[0;32m    153\u001B[0m     np\u001B[38;5;241m.\u001B[39mintp, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    154\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T22:01:49.900630Z",
     "start_time": "2025-01-10T22:01:49.897224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(scores)\n",
    "# [{'coherence': -13.677789412100989, 'parameters': {'aspect_size': 17, 'embedding_size': 150, 'epochs': 20, 'batch_size': 64}}] #1st config\n",
    "\"\"\"\n",
    "[{'coherence': -14.44752463324073,\n",
    "  'parameters': {'aspect_size': 16, 'embedding_size': 150, 'epochs': 15, 'batch_size': 64}},\n",
    " {'coherence': -13.02996122137577,\n",
    "  'parameters': {'aspect_size': 19, 'embedding_size': 200, 'epochs': 15, 'batch_size': 128}},\n",
    " {'coherence': -13.172903114163981,\n",
    "  'parameters': {'aspect_size': 18, 'embedding_size': 200, 'epochs': 10, 'batch_size': 128}},\n",
    " {'coherence': -13.961204280840835,\n",
    "  'parameters': {'aspect_size': 19, 'embedding_size': 200, 'epochs': 20, 'batch_size': 128}},\n",
    " {'coherence': -14.993132489678818,\n",
    "  'parameters': {'aspect_size': 16, 'embedding_size': 150, 'epochs': 5, 'batch_size': 64}},\n",
    " {'coherence': -13.23495325584567,\n",
    "  'parameters': {'aspect_size': 15, 'embedding_size': 100, 'epochs': 15, 'batch_size': 32}},\n",
    " {'coherence': -13.501908255646923,\n",
    "  'parameters': {'aspect_size': 14, 'embedding_size': 100, 'epochs': 15, 'batch_size': 32}}]\n",
    "\"\"\""
   ],
   "id": "875f68d6da388aca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'coherence': -14.44752463324073, 'parameters': {'aspect_size': 16, 'embedding_size': 150, 'epochs': 15, 'batch_size': 64}}, {'coherence': -13.02996122137577, 'parameters': {'aspect_size': 19, 'embedding_size': 200, 'epochs': 15, 'batch_size': 128}}, {'coherence': -13.172903114163981, 'parameters': {'aspect_size': 18, 'embedding_size': 200, 'epochs': 10, 'batch_size': 128}}, {'coherence': -13.961204280840835, 'parameters': {'aspect_size': 19, 'embedding_size': 200, 'epochs': 20, 'batch_size': 128}}, {'coherence': -14.993132489678818, 'parameters': {'aspect_size': 16, 'embedding_size': 150, 'epochs': 5, 'batch_size': 64}}, {'coherence': -13.23495325584567, 'parameters': {'aspect_size': 15, 'embedding_size': 100, 'epochs': 15, 'batch_size': 32}}, {'coherence': -13.501908255646923, 'parameters': {'aspect_size': 14, 'embedding_size': 100, 'epochs': 15, 'batch_size': 32}}]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:25:40.083924Z",
     "start_time": "2025-01-10T17:25:40.081020Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"A\")",
   "id": "84341736155515ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 64k - Default",
   "id": "3cbf621e796d8608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# How to Address Issues (If Any):\n",
    "# Introduce Hard Negatives:\n",
    "# Instead of randomly selecting negative samples, use hard negatives—examples that are more challenging to distinguish from positive pairs. This keeps the max-margin loss informative and prevents the model from converging too quickly.\n",
    "\n",
    "# Regularization:\n",
    "# Apply regularization (e.g., L2 regularization) to prevent overfitting and ensure the model generalizes well.\n",
    "\n",
    "# Early Stopping:\n",
    "# If the loss plateaus and aspect quality is satisfactory, consider using early stopping to avoid unnecessary training."
   ],
   "id": "64c6ea7554d341a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
