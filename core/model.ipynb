{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:17.222818Z",
     "start_time": "2024-11-29T16:31:17.220679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:20.425352Z",
     "start_time": "2024-11-29T16:31:19.147374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "id": "9b8a8833fcaa7e6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x737d5c256570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset.\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] \n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:22.425664Z",
     "start_time": "2024-11-29T16:31:22.423642Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_file = \"./../data/corpus.preprocessed.csv\"  # It's this",
   "id": "d70b9004cc53f3fc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! What size to search is a good question and should be studied (Which I may be doing later). \n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:25.505117Z",
     "start_time": "2024-11-29T16:31:25.502952Z"
    }
   },
   "cell_type": "code",
   "source": "aspect_size = 2 * 7",
   "id": "319cff948ad5e033",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Corpus Considerations\n",
    "Should move where dataset ipynb is but:"
   ],
   "id": "94b539dcf823d1c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Setup",
   "id": "52d8be30943c43e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:31.037442Z",
     "start_time": "2024-11-29T16:31:27.439212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import core.embeddings as embeddings\n",
    "import core.utils\n",
    "\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    core.utils.LoadCorpusUtility(), max_vocab_size=16000, embedding_size=128,\n",
    "    target_model_file=\"./../data/word-embeddings.model\", corpus_file=corpus_file\n",
    ")\n",
    "\n",
    "aspect_embeddings_model = embeddings.AspectEmbedding(\n",
    "    aspect_size=aspect_size, embedding_size=128, base_embeddings=embeddings_model,\n",
    "    target_model_file=\"./../data/aspects-embedding.model\"\n",
    ")"
   ],
   "id": "eb3470aaa495a5cd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:43.746120Z",
     "start_time": "2024-11-29T16:31:43.673472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_model.load_model()\n",
    "aspect_embeddings_model.load_model()"
   ],
   "id": "bfe27fa3b412d9ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ../data/word-embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': '../data/word-embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from ../data/word-embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': '../data/word-embeddings.model', 'datetime': '2024-11-29T17:31:43.744425', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65d6492c659ce55c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the data",
   "id": "89216349e5c6f335"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:49.753987Z",
     "start_time": "2024-11-29T16:31:44.860250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocabulary = embeddings_model.model.wv.key_to_index\n",
    "\n",
    "train = dataset.PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=vocabulary, csv_dataset_path=corpus_file, negative_size=15\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)"
   ],
   "id": "58801e13d855b211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ./../data/corpus.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50461 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5262ca6d1d7d4845b71c074d620ba8c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 136 points.This is 0.2695150710449654% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1235  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:51.698005Z",
     "start_time": "2024-11-29T16:31:51.693719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.model import ABAEGenerator\n",
    "\n",
    "generator = ABAEGenerator(256, train.negative_size, embeddings_model, aspect_embeddings_model)"
   ],
   "id": "ea223a0ab512537",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "3adc8a873389afd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:52.331111Z",
     "start_time": "2024-11-29T16:31:52.328886Z"
    }
   },
   "cell_type": "code",
   "source": "from core import utils",
   "id": "a68c3c29b7d96160",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Why SGD: You know why! todo: Link the papers",
   "id": "2afaad25bf7cdab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:31:53.663354Z",
     "start_time": "2024-11-29T16:31:53.658478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "cc29011d02db49fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have too much data for my little PC:\n",
    "\n",
    "> Sampling: Randomly select a subset of your data that represents the overall distribution of aspects. This will help maintain diversity while reducing the size.\n",
    "Filtering: Focus on the most informative or high-quality samples. For example, if certain reviews are very short, irrelevant, or don't have useful context for aspect extraction, remove them.\n",
    "Focus on Diversity: If you reduce the data, make sure the remaining dataset is still representative of the diversity of aspects you're trying to capture."
   ],
   "id": "54caba09394445b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:44:23.760213Z",
     "start_time": "2024-11-29T16:32:13.978337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_model = generator.make_training_model()\n",
    "training_model.compile(optimizer='SGD', loss=[utils.max_margin_loss], metrics={'max_margin': utils.max_margin_loss})\n",
    "history = training_model.fit(x=train_dataloader, batch_size=128, epochs=15)"
   ],
   "id": "e02073957d81e7d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 60ms/step - loss: 14.1123 - max_margin_loss: 14.1123\n",
      "Epoch 2/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 61ms/step - loss: 12.5177 - max_margin_loss: 12.5177\n",
      "Epoch 3/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 63ms/step - loss: 11.2422 - max_margin_loss: 11.2422\n",
      "Epoch 4/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 62ms/step - loss: 10.0412 - max_margin_loss: 10.0412\n",
      "Epoch 5/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 62ms/step - loss: 9.3263 - max_margin_loss: 9.3263\n",
      "Epoch 6/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 64ms/step - loss: 8.9555 - max_margin_loss: 8.9555\n",
      "Epoch 7/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 62ms/step - loss: 8.6766 - max_margin_loss: 8.6766\n",
      "Epoch 8/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 61ms/step - loss: 8.4919 - max_margin_loss: 8.4919\n",
      "Epoch 9/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 62ms/step - loss: 8.3503 - max_margin_loss: 8.3503\n",
      "Epoch 10/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 61ms/step - loss: 8.2060 - max_margin_loss: 8.2060\n",
      "Epoch 11/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 62ms/step - loss: 8.1186 - max_margin_loss: 8.1186\n",
      "Epoch 12/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 61ms/step - loss: 7.9971 - max_margin_loss: 7.9971\n",
      "Epoch 13/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 60ms/step - loss: 7.8695 - max_margin_loss: 7.8695\n",
      "Epoch 14/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 60ms/step - loss: 7.7242 - max_margin_loss: 7.7242\n",
      "Epoch 15/15\n",
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 63ms/step - loss: 7.5640 - max_margin_loss: 7.5640\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:44:55.546822Z",
     "start_time": "2024-11-29T16:44:55.506475Z"
    }
   },
   "cell_type": "code",
   "source": "training_model.save(\"./../data/abae.keras\")",
   "id": "8b3ab43c76041221",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "b000cd79defb39bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:45:01.689402Z",
     "start_time": "2024-11-29T16:45:01.643142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load evaluation model\n",
    "inference_model = generator.make_model(\"./../data/abae.keras\")"
   ],
   "id": "fe6595f99460d942",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:45:40.977222Z",
     "start_time": "2024-11-29T16:45:05.192556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = inference_model.predict(x=train_dataloader)\n",
    "np.argmax(out[2], axis=-1)  # The associated labels"
   ],
   "id": "48815717db4ed6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m789/789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find aspect most representative words",
   "id": "60641b3d29be817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:14:01.765039Z",
     "start_time": "2024-11-29T18:14:01.744503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_emb = inference_model.get_layer('word_embedding').get_weights()[0]\n",
    "word_emb = torch.from_numpy(word_emb)\n",
    "word_emb.shape"
   ],
   "id": "92373d07480df156",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12954, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:19:30.779574Z",
     "start_time": "2024-11-29T18:19:30.749969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aspect_embeddings = inference_model.get_layer('weighted_aspect_emb').W\n",
    "vocab_inv = embeddings_model.model.wv.index_to_key\n",
    "\n",
    "for aspect in aspect_embeddings:\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    aspect = aspect.cpu()\n",
    "    similarity = word_emb.matmul(aspect.T)\n",
    "\n",
    "    ordered_words = np.argsort(similarity.detach().numpy())[::-1]\n",
    "    desc_list = [vocab_inv[w] for w in ordered_words[:15]]\n",
    "    print(desc_list)"
   ],
   "id": "b7202702cf2f7f51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sleeve', 'placement', 'mm', '2021', '2023', 'min', 'promo', '2022', '2020', '2024', '2017', '2018', '2019', 'copy', 'kickstarter']\n",
      "['placement', 'sleeve', 'solo', 'edition', 'friend', 'art', 'mm', 'year', '2023', '2021', 'copy', '2022', 'co', 'group', 'people']\n",
      "['min', 'minute', 'teach', 'turn', 'round', 'solo', '60', 'learn', 'count', 'placement', '2', '1', '3', '45', '4']\n",
      "['action', 'opponent', 'turn', 'round', 'decision', 'resource', 'plan', 'phase', 'order', 'scoring', 'selection', 'place', 'tile', 'track', 'move']\n",
      "['filler', 'min', 'kid', 'teach', 'minute', 'weight', 'hour', 'gamer', 'family', 'euro', 'co', 'medium', 'highly', 'adult', 'year']\n",
      "['sleeve', '2021', 'promo', 'kickstarter', '2023', '2022', 'pledge', 'ks', 'mm', '2020', 'edition', '2019', '2018', '2024', 'metal']\n",
      "['area', 'action', 'mm', 'money', 'pay', 'selection', 'sleeve', 'order', 'promo', 'resource', 'space', 'gain', 'collect', '2021', 'ship']\n",
      "['group', 'co', 'people', 'gamer', 'teach', '4', 'learn', 'friend', 'solo', 'kid', 'party', 'placement', 'recommend', 'highly', 'luck']\n",
      "['worker', 'luck', 'dice', 'resource', 'action', 'interaction', 'decision', 'victory', 'deck', 'choice', 'engine', 'building', 'op', 'euro', 'roll']\n",
      "['order', 'decision', 'round', 'action', 'dice', 'die', 'area', 'selection', 'attack', 'pay', 'phase', 'opponent', 'gain', 'plan', 'end']\n",
      "['promo', 'solo', 'sleeve', 'kickstarter', 'building', 'expansion', '1', '2021', '2023', 'builder', '2022', 'co', 'ks', 'mm', 'forward']\n",
      "['hour', 'minute', 'min', 'play', '3', '4', '5', '6', '30', '2', 'group', 'year', 'rate', '10', 'rating']\n",
      "['round', 'turn', 'plan', 'draw', 'die', 'order', '1', 'rule', 'opponent', 'player', 'take', 'start', 'choose', 'number', 'card']\n",
      "['box', 'rule', 'read', 'away', 'review', 'rulebook', 'answer', 'opponent', 'trade', 'problem', 'book', 'will', 'check', 'kickstarter', 'room']\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pred = m.predict(x=train_dataloader)",
   "id": "38484e8406dfd633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pred",
   "id": "935562df8a5fd3b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(pred)"
   ],
   "id": "3d83fa39a092e09a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
