{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:04.745474Z",
     "start_time": "2024-12-04T16:29:04.743542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T21:28:52.475058Z",
     "start_time": "2024-12-03T21:28:51.208176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "id": "9b8a8833fcaa7e6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7a18fd227860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset.\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] \n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:51:05.688055Z",
     "start_time": "2024-12-03T15:51:05.686116Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_file = \"./../data/corpus.preprocessed.csv\"  # It's this",
   "id": "d70b9004cc53f3fc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! What size to search is a good question and should be studied (Which I may be doing later). \n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:58:09.654296Z",
     "start_time": "2024-12-03T16:58:09.652217Z"
    }
   },
   "cell_type": "code",
   "source": "aspect_size = 2 * 7",
   "id": "319cff948ad5e033",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Setup",
   "id": "52d8be30943c43e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:35.091825Z",
     "start_time": "2024-12-02T14:12:31.632436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import core.embeddings as embeddings\n",
    "import core.utils\n",
    "\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    core.utils.LoadCorpusUtility(), max_vocab_size=16000, embedding_size=128,\n",
    "    target_model_file=\"./../data/word-embeddings.model\", corpus_file=corpus_file\n",
    ")\n",
    "\n",
    "aspect_embeddings_model = embeddings.AspectEmbedding(\n",
    "    aspect_size=aspect_size, embedding_size=128, base_embeddings=embeddings_model,\n",
    "    target_model_file=\"./../data/aspects-embedding.model\"\n",
    ")"
   ],
   "id": "eb3470aaa495a5cd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:35.167229Z",
     "start_time": "2024-12-02T14:12:35.093012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_model.load_model()\n",
    "aspect_embeddings_model.load_model()"
   ],
   "id": "bfe27fa3b412d9ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ../data/word-embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': '../data/word-embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from ../data/word-embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': '../data/word-embeddings.model', 'datetime': '2024-12-02T15:12:35.165483', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65d6492c659ce55c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the data",
   "id": "89216349e5c6f335"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:39.696556Z",
     "start_time": "2024-12-02T14:12:35.167857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocabulary = embeddings_model.model.wv.key_to_index\n",
    "\n",
    "train = dataset.PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=vocabulary, csv_dataset_path=corpus_file, negative_size=15\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=32, shuffle=True)"
   ],
   "id": "58801e13d855b211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ./../data/corpus.preprocessed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50461 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "825fcfb4f59449a7ab07375879038e7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 136 points.This is 0.2695150710449654% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1235  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:39.700066Z",
     "start_time": "2024-12-02T14:12:39.697422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.model import ABAEGenerator\n",
    "\n",
    "generator = ABAEGenerator(256, train.negative_size, embeddings_model, aspect_embeddings_model)"
   ],
   "id": "ea223a0ab512537",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "3adc8a873389afd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:37:09.405521Z",
     "start_time": "2024-11-30T17:37:09.403875Z"
    }
   },
   "cell_type": "code",
   "source": "from core import utils",
   "id": "a68c3c29b7d96160",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Why SGD: You know why! todo: Link the papers",
   "id": "2afaad25bf7cdab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:37:09.409347Z",
     "start_time": "2024-11-30T17:37:09.406072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "cc29011d02db49fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have too much data for my little PC:\n",
    "\n",
    "> Sampling: Randomly select a subset of your data that represents the overall distribution of aspects. This will help maintain diversity while reducing the size.\n",
    "Filtering: Focus on the most informative or high-quality samples. For example, if certain reviews are very short, irrelevant, or don't have useful context for aspect extraction, remove them.\n",
    "Focus on Diversity: If you reduce the data, make sure the remaining dataset is still representative of the diversity of aspects you're trying to capture."
   ],
   "id": "54caba09394445b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:53:50.256911Z",
     "start_time": "2024-11-30T17:37:09.410551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_model = generator.make_training_model()\n",
    "training_model.compile(optimizer='SGD', loss=[utils.max_margin_loss], metrics={'max_margin': utils.max_margin_loss})\n",
    "history = training_model.fit(x=train_dataloader, batch_size=32, epochs=15)"
   ],
   "id": "e02073957d81e7d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m66s\u001B[0m 42ms/step - loss: 13.7152 - max_margin_loss: 13.7152\n",
      "Epoch 2/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m66s\u001B[0m 42ms/step - loss: 11.2962 - max_margin_loss: 11.2962\n",
      "Epoch 3/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 40ms/step - loss: 9.4733 - max_margin_loss: 9.4733\n",
      "Epoch 4/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 41ms/step - loss: 8.7165 - max_margin_loss: 8.7165\n",
      "Epoch 5/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 41ms/step - loss: 8.3181 - max_margin_loss: 8.3181\n",
      "Epoch 6/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 41ms/step - loss: 8.1459 - max_margin_loss: 8.1459\n",
      "Epoch 7/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 40ms/step - loss: 7.9774 - max_margin_loss: 7.9774\n",
      "Epoch 8/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 41ms/step - loss: 7.8275 - max_margin_loss: 7.8275\n",
      "Epoch 9/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m68s\u001B[0m 43ms/step - loss: 7.6478 - max_margin_loss: 7.6478\n",
      "Epoch 10/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m68s\u001B[0m 43ms/step - loss: 7.4167 - max_margin_loss: 7.4167\n",
      "Epoch 11/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m69s\u001B[0m 44ms/step - loss: 7.0284 - max_margin_loss: 7.0284\n",
      "Epoch 12/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m69s\u001B[0m 44ms/step - loss: 6.5674 - max_margin_loss: 6.5674\n",
      "Epoch 13/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m69s\u001B[0m 44ms/step - loss: 5.9915 - max_margin_loss: 5.9915\n",
      "Epoch 14/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 45ms/step - loss: 5.6416 - max_margin_loss: 5.6416\n",
      "Epoch 15/15\n",
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m69s\u001B[0m 44ms/step - loss: 5.4395 - max_margin_loss: 5.4395\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:53:50.259142Z",
     "start_time": "2024-11-30T17:53:50.257504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How to Address Issues (If Any):\n",
    "# Introduce Hard Negatives:\n",
    "# Instead of randomly selecting negative samples, use hard negatives—examples that are more challenging to distinguish from positive pairs. This keeps the max-margin loss informative and prevents the model from converging too quickly.\n",
    "\n",
    "# Regularization:\n",
    "# Apply regularization (e.g., L2 regularization) to prevent overfitting and ensure the model generalizes well.\n",
    "\n",
    "# Early Stopping:\n",
    "# If the loss plateaus and aspect quality is satisfactory, consider using early stopping to avoid unnecessary training."
   ],
   "id": "64c6ea7554d341a9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:53:50.290450Z",
     "start_time": "2024-11-30T17:53:50.259787Z"
    }
   },
   "cell_type": "code",
   "source": "training_model.save(\"./../data/abae.keras\")",
   "id": "8b3ab43c76041221",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "b000cd79defb39bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:46.437067Z",
     "start_time": "2024-12-02T14:12:46.318391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load evaluation model\n",
    "inference_model = generator.make_model(\"./../data/abae.keras\")"
   ],
   "id": "fe6595f99460d942",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:54:28.237930Z",
     "start_time": "2024-11-30T17:53:50.319985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = inference_model.predict(x=train_dataloader)\n",
    "np.argmax(out[2], axis=-1)  # The associated labels"
   ],
   "id": "48815717db4ed6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1577/1577\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 0, 7, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find aspect most representative words",
   "id": "60641b3d29be817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:12:49.517696Z",
     "start_time": "2024-12-02T14:12:49.508556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_emb = inference_model.get_layer('word_embedding').get_weights()[0]\n",
    "word_emb = torch.from_numpy(word_emb)\n",
    "word_emb.shape"
   ],
   "id": "92373d07480df156",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12954, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:13:13.662756Z",
     "start_time": "2024-12-02T14:13:13.660382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aspect_embeddings = inference_model.get_layer('weighted_aspect_emb').W\n",
    "vocab_inv = embeddings_model.model.wv.index_to_key"
   ],
   "id": "f4cfefd11b6ce84c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:18:02.211568Z",
     "start_time": "2024-12-02T14:18:02.160148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "aspect_words = []\n",
    "aspect_index = 0\n",
    "\n",
    "for aspect in aspect_embeddings:\n",
    "    aspect = aspect.cpu()\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    word_emb = word_emb / torch.linalg.norm(word_emb, dim=-1, keepdim=True)\n",
    "    aspect = aspect / torch.linalg.norm(aspect, dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = word_emb.matmul(aspect.T)\n",
    "\n",
    "    numpy_similarity = similarity.detach().numpy()\n",
    "\n",
    "    ordered_words = np.argsort(numpy_similarity)[::-1]\n",
    "    desc_list = [(vocab_inv[w], numpy_similarity[w]) for w in ordered_words[:15]]\n",
    "    aspect_words.append(desc_list)\n",
    "\n",
    "    print(\"Aspect \", aspect_index)\n",
    "    for i in desc_list:\n",
    "        # hr][/i is not a valid word. meh.\n",
    "        print(\"Word: \", i[0], f\"({i[1]})\")\n",
    "\n",
    "    aspect_index += 1"
   ],
   "id": "b7202702cf2f7f51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect  0\n",
      "Word:  release (0.5447438955307007)\n",
      "Word:  buy (0.5116141438484192)\n",
      "Word:  back (0.4773869514465332)\n",
      "Word:  unlock (0.477204293012619)\n",
      "Word:  magic (0.4655134677886963)\n",
      "Word:  ffg (0.46209990978240967)\n",
      "Word:  kickstarter (0.46151912212371826)\n",
      "Word:  marvel (0.459344744682312)\n",
      "Word:  retail (0.45813944935798645)\n",
      "Word:  glad (0.4541388750076294)\n",
      "Word:  pls (0.44647061824798584)\n",
      "Word:  cosmetic (0.437392920255661)\n",
      "Word:  copy (0.4348754286766052)\n",
      "Word:  legendary (0.43482765555381775)\n",
      "Word:  regret (0.4348164200782776)\n",
      "Aspect  1\n",
      "Word:  belive (0.7245154976844788)\n",
      "Word:  email (0.7236083745956421)\n",
      "Word:  familiarize (0.7202874422073364)\n",
      "Word:  incarnation (0.7198400497436523)\n",
      "Word:  clothe (0.719068169593811)\n",
      "Word:  tmb (0.7155524492263794)\n",
      "Word:  technically (0.7127645611763)\n",
      "Word:  preset (0.7095102071762085)\n",
      "Word:  kg (0.7089599370956421)\n",
      "Word:  rondell (0.7079117298126221)\n",
      "Word:  ppl (0.7070233821868896)\n",
      "Word:  dame (0.7059849500656128)\n",
      "Word:  erratas (0.7055681943893433)\n",
      "Word:  ot (0.7048789262771606)\n",
      "Word:  hte (0.7038723230361938)\n",
      "Aspect  2\n",
      "Word:  skilled (0.6982916593551636)\n",
      "Word:  rut (0.6949139833450317)\n",
      "Word:  combative (0.6894547939300537)\n",
      "Word:  fifth (0.688031017780304)\n",
      "Word:  suspicious (0.6866863965988159)\n",
      "Word:  summarise (0.6839203834533691)\n",
      "Word:  contention (0.6790550947189331)\n",
      "Word:  mathe (0.6789302825927734)\n",
      "Word:  contrary (0.6784725189208984)\n",
      "Word:  handicap (0.677964448928833)\n",
      "Word:  else (0.6771073341369629)\n",
      "Word:  thon (0.6763026118278503)\n",
      "Word:  disregard (0.6756355166435242)\n",
      "Word:  drown (0.6739515066146851)\n",
      "Word:  gene (0.6731894016265869)\n",
      "Aspect  3\n",
      "Word:  angst (0.4518653154373169)\n",
      "Word:  mathe (0.4456774592399597)\n",
      "Word:  lull (0.4449758529663086)\n",
      "Word:  attentive (0.44487059116363525)\n",
      "Word:  upkeep (0.44027280807495117)\n",
      "Word:  simultaneously (0.43783822655677795)\n",
      "Word:  skilled (0.4343178868293762)\n",
      "Word:  rut (0.4342578053474426)\n",
      "Word:  beginning (0.43380972743034363)\n",
      "Word:  summarise (0.4309144914150238)\n",
      "Word:  calculation (0.43076011538505554)\n",
      "Word:  decimate (0.4281756281852722)\n",
      "Word:  critical (0.4271489679813385)\n",
      "Word:  induce (0.4258180856704712)\n",
      "Word:  consideration (0.4254299998283386)\n",
      "Aspect  4\n",
      "Word:  mechanically (0.7395925521850586)\n",
      "Word:  thematic (0.7256996631622314)\n",
      "Word:  engaging (0.7094951868057251)\n",
      "Word:  elegant (0.6952482461929321)\n",
      "Word:  innovative (0.6849705576896667)\n",
      "Word:  abstract (0.6726769208908081)\n",
      "Word:  style (0.6667015552520752)\n",
      "Word:  theme (0.6659834384918213)\n",
      "Word:  nicely (0.6627437472343445)\n",
      "Word:  simplicity (0.6504080295562744)\n",
      "Word:  thinky (0.648577094078064)\n",
      "Word:  surprisingly (0.6426346302032471)\n",
      "Word:  cute (0.6402686834335327)\n",
      "Word:  cutesy (0.6397366523742676)\n",
      "Word:  blend (0.6395053267478943)\n",
      "Aspect  5\n",
      "Word:  pledge (0.5881250500679016)\n",
      "Word:  25th (0.576857328414917)\n",
      "Word:  ~ (0.5745965242385864)\n",
      "Word:  md2 (0.572848916053772)\n",
      "Word:  playmat (0.5622756481170654)\n",
      "Word:  44x63 (0.5615267753601074)\n",
      "Word:  black (0.5609537363052368)\n",
      "Word:  metal (0.5596048831939697)\n",
      "Word:  promo (0.5478048324584961)\n",
      "Word:  dark (0.5469409227371216)\n",
      "Word:  premium (0.5420743227005005)\n",
      "Word:  sleeve (0.5390489101409912)\n",
      "Word:  mayday (0.5351731181144714)\n",
      "Word:  north (0.5350816249847412)\n",
      "Word:  exp (0.5278605222702026)\n",
      "Aspect  6\n",
      "Word:  contract (0.7533861398696899)\n",
      "Word:  income (0.733350396156311)\n",
      "Word:  vp (0.7302042245864868)\n",
      "Word:  track (0.7245877981185913)\n",
      "Word:  gain (0.7081347703933716)\n",
      "Word:  bonus (0.7074660658836365)\n",
      "Word:  marker (0.7014265060424805)\n",
      "Word:  boat (0.6960002183914185)\n",
      "Word:  region (0.6936243772506714)\n",
      "Word:  food (0.6920195817947388)\n",
      "Word:  cube (0.6915662288665771)\n",
      "Word:  location (0.6874231696128845)\n",
      "Word:  money (0.6810256838798523)\n",
      "Word:  collect (0.6770788431167603)\n",
      "Word:  claim (0.6759439706802368)\n",
      "Aspect  7\n",
      "Word:  play (0.563241720199585)\n",
      "Word:  maxing (0.5241923928260803)\n",
      "Word:  edit (0.5024509429931641)\n",
      "Word:  rate (0.4789552390575409)\n",
      "Word:  11/17 (0.4773992896080017)\n",
      "Word:  session (0.4707276523113251)\n",
      "Word:  4p (0.44348227977752686)\n",
      "Word:  week (0.440207302570343)\n",
      "Word:  10 (0.438071608543396)\n",
      "Word:  update (0.4344898462295532)\n",
      "Word:  hour (0.4309106171131134)\n",
      "Word:  4/4 (0.42425668239593506)\n",
      "Word:  30 (0.41886770725250244)\n",
      "Word:  daughter (0.4180385172367096)\n",
      "Word:  3p (0.4162169396877289)\n",
      "Aspect  8\n",
      "Word:  conflict (0.4559025466442108)\n",
      "Word:  timing (0.438757061958313)\n",
      "Word:  direct (0.43573862314224243)\n",
      "Word:  economic (0.42962801456451416)\n",
      "Word:  satisfy (0.42925870418548584)\n",
      "Word:  tension (0.4263601303100586)\n",
      "Word:  generate (0.4241257905960083)\n",
      "Word:  bluff (0.41874784231185913)\n",
      "Word:  race (0.4169393479824066)\n",
      "Word:  military (0.4146558940410614)\n",
      "Word:  competition (0.41405415534973145)\n",
      "Word:  economy (0.41236376762390137)\n",
      "Word:  encourage (0.40813058614730835)\n",
      "Word:  malleable (0.40620434284210205)\n",
      "Word:  pivot (0.4061592221260071)\n",
      "Aspect  9\n",
      "Word:  keyple (0.6437544822692871)\n",
      "Word:  resolve (0.6330983638763428)\n",
      "Word:  perform (0.6057081818580627)\n",
      "Word:  income (0.6033197641372681)\n",
      "Word:  supply (0.5931459665298462)\n",
      "Word:  activation (0.5909640789031982)\n",
      "Word:  accumulate (0.5872229337692261)\n",
      "Word:  critical (0.5856596231460571)\n",
      "Word:  crucial (0.5854961276054382)\n",
      "Word:  enchanter (0.583258330821991)\n",
      "Word:  lock (0.582610011100769)\n",
      "Word:  determine (0.5801068544387817)\n",
      "Word:  consequence (0.5755595564842224)\n",
      "Word:  reward (0.5713491439819336)\n",
      "Word:  phase (0.5691577792167664)\n",
      "Aspect  10\n",
      "Word:  enought (0.4639054536819458)\n",
      "Word:  mechs (0.4528099298477173)\n",
      "Word:  cull (0.447909414768219)\n",
      "Word:  log (0.4444660544395447)\n",
      "Word:  currently (0.44291603565216064)\n",
      "Word:  chronology (0.4399208426475525)\n",
      "Word:  rumor (0.4393736720085144)\n",
      "Word:  previously (0.43861159682273865)\n",
      "Word:  wanna (0.4380771517753601)\n",
      "Word:  preset (0.43652787804603577)\n",
      "Word:  heather (0.43603402376174927)\n",
      "Word:  ham (0.43216949701309204)\n",
      "Word:  hoax (0.43075352907180786)\n",
      "Word:  finite (0.42915573716163635)\n",
      "Word:  automata (0.42695096135139465)\n",
      "Aspect  11\n",
      "Word:  3- (0.5087751150131226)\n",
      "Word:  2 (0.4878714978694916)\n",
      "Word:  disadvantage (0.4564206004142761)\n",
      "Word:  \\13\\ (0.4532138705253601)\n",
      "Word:  count (0.4319762885570526)\n",
      "Word:  variant (0.43002015352249146)\n",
      "Word:  suspect (0.42525720596313477)\n",
      "Word:  imagine (0.42429882287979126)\n",
      "Word:  minimum (0.42157381772994995)\n",
      "Word:  dueler (0.4204886555671692)\n",
      "Word:  3 (0.4171687960624695)\n",
      "Word:  alienate (0.41337770223617554)\n",
      "Word:  shaky (0.41277408599853516)\n",
      "Word:  party (0.40866589546203613)\n",
      "Word:  experienced (0.406451940536499)\n",
      "Aspect  12\n",
      "Word:  eliminate (0.6780023574829102)\n",
      "Word:  single (0.6657650470733643)\n",
      "Word:  beginning (0.6498178243637085)\n",
      "Word:  face (0.639141321182251)\n",
      "Word:  simultaneously (0.6355462074279785)\n",
      "Word:  randomly (0.6203116178512573)\n",
      "Word:  predict (0.6099243760108948)\n",
      "Word:  choose (0.6089282631874084)\n",
      "Word:  effectively (0.6066470146179199)\n",
      "Word:  happen (0.6042191982269287)\n",
      "Word:  attack (0.6015000343322754)\n",
      "Word:  cause (0.6007243990898132)\n",
      "Word:  sequence (0.5990183353424072)\n",
      "Word:  person (0.5982658863067627)\n",
      "Word:  determine (0.5977545976638794)\n",
      "Aspect  13\n",
      "Word:  pendragon (0.5009773969650269)\n",
      "Word:  check (0.4889921247959137)\n",
      "Word:  fail (0.4768734276294708)\n",
      "Word:  error (0.47563832998275757)\n",
      "Word:  tidbit (0.47560590505599976)\n",
      "Word:  nauseum (0.4742206931114197)\n",
      "Word:  dishearten (0.4710935950279236)\n",
      "Word:  fix (0.4705195128917694)\n",
      "Word:  miss (0.4659067392349243)\n",
      "Word:  impatiently (0.46543434262275696)\n",
      "Word:  live (0.46320411562919617)\n",
      "Word:  survive (0.4621725082397461)\n",
      "Word:  premature (0.461147665977478)\n",
      "Word:  stride (0.46014708280563354)\n",
      "Word:  eventually (0.4600198566913605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50715/4171825579.py:10: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3697.)\n",
      "  similarity = word_emb.matmul(aspect.T)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluate coherence\n",
    "Topical coherence measures the semantic consistency of terms grouped under a topic or aspect. It checks whether the terms frequently co-occur in similar contexts within your dataset, reflecting a meaningful grouping. For each topic (aspect), calculate pairwise co-occurrence of terms across the dataset. Terms that co-occur frequently in the same context are considered more coherent\n",
    "\n"
   ],
   "id": "f58d27ab60b7ea76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:18:04.077446Z",
     "start_time": "2024-12-02T14:18:04.056233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each word of aspect for the aspect we calculate the coherence by AVG distance between top words\n",
    "for aspect_most_representative_words in aspect_words:\n",
    "    coherence = []\n",
    "    for word in aspect_most_representative_words:\n",
    "        w, score = word\n",
    "        for word2 in aspect_most_representative_words:\n",
    "            w2, score = word2\n",
    "            if w != w2:\n",
    "                coherence.append(embeddings_model.model.wv.similarity(w, w2))\n",
    "    # todo fai avgf cosi natualmente sbagliato  \n",
    "    print(\"Aspect i has total coherence of\", np.mean(coherence, axis=0))  # AVG"
   ],
   "id": "4beec4cd26c90a88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect i has total coherence of 0.6271665\n",
      "Aspect i has total coherence of 0.8668326\n",
      "Aspect i has total coherence of 0.84780085\n",
      "Aspect i has total coherence of 0.74030197\n",
      "Aspect i has total coherence of 0.7200178\n",
      "Aspect i has total coherence of 0.8413263\n",
      "Aspect i has total coherence of 0.8098694\n",
      "Aspect i has total coherence of 0.54224765\n",
      "Aspect i has total coherence of 0.69913083\n",
      "Aspect i has total coherence of 0.7870225\n",
      "Aspect i has total coherence of 0.7779807\n",
      "Aspect i has total coherence of 0.5372712\n",
      "Aspect i has total coherence of 0.73999083\n",
      "Aspect i has total coherence of 0.65727794\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:54:28.303845Z",
     "start_time": "2024-11-30T17:54:28.302529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Words to remove:\n",
    "\"\"\"\n",
    "\\13\\\n",
    "~\n",
    "\"\"\""
   ],
   "id": "c0c830cfabd1d94c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:17:16.372146Z",
     "start_time": "2024-12-02T14:17:16.325812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gold_standard_topics = ['luck', 'alea', 'bookkeeping', 'downtime', 'strategy', 'interaction', 'complicated', 'complex']\n",
    "counter = 0\n",
    "for aspect in aspect_embeddings:\n",
    "    aspect = aspect.cpu()\n",
    "\n",
    "    aspect = aspect / torch.linalg.norm(aspect, dim=-1, keepdim=True)\n",
    "    word_emb = word_emb / torch.linalg.norm(word_emb, dim=-1, keepdim=True)\n",
    "\n",
    "    print(\"Aspect \", counter)\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    for topic in gold_standard_topics:\n",
    "        index = embeddings_model.model.wv.get_index(topic)\n",
    "        print(f\"'{topic}' similarity: \", word_emb[index].dot(aspect))\n",
    "    embeddings_model.model.wv.get_vector('luck')\n",
    "    counter += 1"
   ],
   "id": "118e43d292e819ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect  0\n",
      "'luck' similarity:  tensor(-0.0714, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.1196, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.1017, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.4651, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.1643, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.3416, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.0198, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.1619, grad_fn=<DotBackward0>)\n",
      "Aspect  1\n",
      "'luck' similarity:  tensor(0.0218, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.5491, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.5055, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.1841, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.1607, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.0652, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.2053, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.1512, grad_fn=<DotBackward0>)\n",
      "Aspect  2\n",
      "'luck' similarity:  tensor(0.0953, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.4546, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.5756, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.3840, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.2584, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.2992, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.2878, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.2767, grad_fn=<DotBackward0>)\n",
      "Aspect  3\n",
      "'luck' similarity:  tensor(0.0672, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.1654, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.3531, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.3199, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.1165, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.2556, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.1704, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.1537, grad_fn=<DotBackward0>)\n",
      "Aspect  4\n",
      "'luck' similarity:  tensor(0.1379, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0677, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.0329, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.0293, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.1768, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.2823, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.4433, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.4908, grad_fn=<DotBackward0>)\n",
      "Aspect  5\n",
      "'luck' similarity:  tensor(-0.2931, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.0535, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.2738, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.4685, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.5090, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.3262, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.4697, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.3480, grad_fn=<DotBackward0>)\n",
      "Aspect  6\n",
      "'luck' similarity:  tensor(-0.0309, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.3653, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0231, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.0435, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.0875, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.0982, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.2290, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.3042, grad_fn=<DotBackward0>)\n",
      "Aspect  7\n",
      "'luck' similarity:  tensor(-0.1787, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.1750, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.4648, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.1128, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.1292, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.2083, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.1606, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.0735, grad_fn=<DotBackward0>)\n",
      "Aspect  8\n",
      "'luck' similarity:  tensor(0.2866, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0923, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.2281, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.1683, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.2726, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.3854, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.0184, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0563, grad_fn=<DotBackward0>)\n",
      "Aspect  9\n",
      "'luck' similarity:  tensor(0.0954, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.0329, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.3919, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.1449, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.1318, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.1479, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.1166, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0522, grad_fn=<DotBackward0>)\n",
      "Aspect  10\n",
      "'luck' similarity:  tensor(0.0092, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.3075, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.1744, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.0246, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.0400, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.0278, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.0554, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.0205, grad_fn=<DotBackward0>)\n",
      "Aspect  11\n",
      "'luck' similarity:  tensor(0.1812, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.2220, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.1926, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.3600, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.1958, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.2258, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.0562, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0134, grad_fn=<DotBackward0>)\n",
      "Aspect  12\n",
      "'luck' similarity:  tensor(0.2453, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.0020, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.4146, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.4842, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.2621, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.2027, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.1515, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0392, grad_fn=<DotBackward0>)\n",
      "Aspect  13\n",
      "'luck' similarity:  tensor(-0.0335, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.3230, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.2554, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.0523, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.0237, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.2008, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.1778, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0130, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:32:43.486584Z",
     "start_time": "2024-12-02T14:32:43.482916Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings_model.model.wv.get_vector('4p')",
   "id": "a8cf7b9e4c19e10b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06464096,  0.09093551,  0.0028071 ,  0.29852405,  0.602484  ,\n",
       "       -0.7610173 , -0.3652719 ,  0.27201098,  0.15539584, -0.23070729,\n",
       "        0.85078335, -0.14241563, -0.3791329 , -0.62965673, -0.28053308,\n",
       "       -0.21491268, -0.12933089,  0.6480796 ,  0.4191215 , -0.7608188 ,\n",
       "       -0.23891445,  0.14498414,  0.8824801 ,  0.1812332 ,  0.0582756 ,\n",
       "        0.10913613, -0.5965477 , -0.06786239, -0.42944986,  0.11144818,\n",
       "        0.01937047, -0.48778266, -0.03961428,  1.133933  , -0.3447342 ,\n",
       "        0.5208978 ,  0.42408213,  0.21653399,  0.8246546 ,  0.21316518,\n",
       "       -0.12819354,  0.23317572, -0.04644729,  0.22567058, -0.32728085,\n",
       "        0.2677897 , -0.2627684 ,  0.24090816, -0.16617578,  0.21517266,\n",
       "        0.16364598, -0.2701411 ,  0.46338764,  0.6053269 , -0.06960589,\n",
       "        0.33252728,  0.2450767 , -0.2168208 , -0.9041195 ,  0.1471062 ,\n",
       "        0.16257447, -0.51723665,  0.20917365, -0.44860741,  0.6085149 ,\n",
       "       -0.26305255,  0.6510224 ,  0.26636028,  0.12356767, -0.17459211,\n",
       "       -0.26713172, -0.5284473 , -0.33066425, -0.93129474, -0.44488627,\n",
       "       -0.7629487 ,  0.19416238,  0.06983414, -0.17088789,  0.31673402,\n",
       "       -0.626181  ,  0.39849433,  0.2800551 ,  0.60768205, -0.03161115,\n",
       "        0.05377203,  0.18453859, -0.6222929 , -0.26965782,  0.42047068,\n",
       "       -0.0987955 , -0.8037428 , -0.01431629,  0.06813628,  0.9829925 ,\n",
       "       -0.50837094, -0.04545911, -0.3662371 ,  0.11702596, -0.4084151 ,\n",
       "       -0.53439623, -0.8728792 ,  0.04507098,  0.33611456, -0.27512234,\n",
       "       -0.5816877 , -0.06660621,  0.25882974,  0.42010912, -0.5605831 ,\n",
       "       -0.01886374, -0.24070564,  0.3171749 , -0.05047924, -0.1779365 ,\n",
       "       -0.06032563,  0.14916798,  0.3916119 ,  0.35040343, -0.10882244,\n",
       "       -0.45591035, -0.21956599,  0.07475622,  0.15602767, -0.16127367,\n",
       "       -0.25403368, -0.47367436,  0.54004735], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9217fbe281e2fb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Kickstarter Less Dataset (64K)",
   "id": "da621a3b2ab32e64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "File paths (Embeddings and corpus)",
   "id": "d126e5c53cdd574a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:28.556765Z",
     "start_time": "2024-12-03T16:57:28.554910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_file = \"./../data/corpus.preprocessed.kickstarter_removed.csv\"\n",
    "embeddings_file = \"./../data/word-embeddings.kickstarter_removed.model\"\n",
    "aspects_file = \"./../data/aspects-embedding.kickstarter_removed.model\""
   ],
   "id": "fa5e196ad6800a01",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters\n",
    "These should have been discussed earlier. <br>\n",
    "We could do hyperparmeter optimization, but how do we 'validate' our model? <br>"
   ],
   "id": "12cc28937090ae66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:28.924909Z",
     "start_time": "2024-12-03T16:57:28.922654Z"
    }
   },
   "cell_type": "code",
   "source": "max_vocab_size = 16000  # Maximum amount of different words in vocabulary",
   "id": "ca55efd5d1b571f5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:57:29.139388Z",
     "start_time": "2024-12-03T16:57:29.136575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_embedding_size = 128  # Size of the word embeddings\n",
    "aspect_embedding_size = 128  # Size of the aspect embeddings"
   ],
   "id": "1ccf623a42eebb53",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading",
   "id": "f495be92a0b71f91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:58:01.948893Z",
     "start_time": "2024-12-03T16:58:01.857492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import core.utils\n",
    "\n",
    "corpus_load_utility = core.utils.LoadCorpusUtility()"
   ],
   "id": "263215177decadaf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:58:14.887220Z",
     "start_time": "2024-12-03T16:58:14.885038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import core.embeddings as embeddings\n",
    "\n",
    "emb_model = embeddings.WordEmbedding(\n",
    "    corpus_loader_utility=corpus_load_utility, max_vocab_size=max_vocab_size,\n",
    "    embedding_size=word_embedding_size, target_model_file=embeddings_file, corpus_file=corpus_file\n",
    ")\n",
    "\n",
    "aspect_emb_model = embeddings.AspectEmbedding(\n",
    "    aspect_size=aspect_size, embedding_size=aspect_embedding_size,\n",
    "    base_embeddings=emb_model, target_model_file=aspects_file\n",
    ")"
   ],
   "id": "8bee656742d03d72",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load models",
   "id": "9e6d144c380681b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:58:43.914495Z",
     "start_time": "2024-12-03T16:58:32.942168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emb_model.load_model()\n",
    "aspect_emb_model.load_model()"
   ],
   "id": "510150b83f3f9ee6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "407e8a75d30840038d81a495f7cdfd31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40e704add689484d800a139000e10d3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 196913 words, keeping 8892 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 427194 words, keeping 11177 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 648339 words, keeping 12220 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 884135 words, keeping 12706 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1127436 words, keeping 12920 word types\n",
      "INFO:gensim.models.word2vec:collected 12922 word types from a corpus of 1130593 raw words and 50115 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 12922 unique words (100.00% of original 12922, drops 0)', 'datetime': '2024-12-03T17:58:42.379939', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1130593 word corpus (100.00% of original 1130593, drops 0)', 'datetime': '2024-12-03T17:58:42.380876', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 12922 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 47 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 957121.1097200726 word corpus (84.7%% of prior 1130593)', 'datetime': '2024-12-03T17:58:42.420071', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 12922 words and 128 dimensions: 19693128 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-03T17:58:42.480625', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training core with 8 workers on 12922 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-03T17:58:42.481036', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 1130593 raw words (957049 effective words) took 0.3s, 3485358 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 1130593 raw words (956901 effective words) took 0.3s, 3602517 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 13 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 1130593 raw words (957106 effective words) took 0.2s, 3931222 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 1130593 raw words (956836 effective words) took 0.2s, 4264747 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 114 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 15 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 14 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 1130593 raw words (957124 effective words) took 0.2s, 4146536 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 5652965 raw words (4785016 effective words) took 1.3s, 3783645 effective words/s', 'datetime': '2024-12-03T17:58:43.745938', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12922, vector_size=128, alpha=0.025>', 'datetime': '2024-12-03T17:58:43.746335', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './../data/word-embeddings.kickstarter_removed.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-03T17:58:43.763895', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './../data/word-embeddings.kickstarter_removed.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./../data/word-embeddings.kickstarter_removed.model\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:58:50.853317Z",
     "start_time": "2024-12-03T16:58:45.869780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocabulary = emb_model.model.wv.key_to_index\n",
    "\n",
    "train = dataset.PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=vocabulary, csv_dataset_path=corpus_file, negative_size=15\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)"
   ],
   "id": "6531a99fd98731b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ./../data/corpus.preprocessed.kickstarter_removed.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d1aa1b009574748a7cf6cb6b49b85e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 136 points.This is 0.2713758355781702% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1235  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Setup",
   "id": "5a629c5fdfce7295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:59:00.103236Z",
     "start_time": "2024-12-03T16:59:00.100124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.model import ABAEGenerator\n",
    "\n",
    "generator = ABAEGenerator(train.max_seq_length, train.negative_size, emb_model, aspect_emb_model)"
   ],
   "id": "9c1f013539bb6810",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:59:00.869051Z",
     "start_time": "2024-12-03T16:59:00.865275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Asses that we have GPU working\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "299fd7f33c416aa2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Phase",
   "id": "734f86198c2d9ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:59:03.141949Z",
     "start_time": "2024-12-03T16:59:03.139211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = 'adam'  # We try Adam as it converges faster\n",
    "batch_size = 64\n",
    "epochs = 20"
   ],
   "id": "fb18cc90450c13ec",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:09:16.263545Z",
     "start_time": "2024-12-03T16:59:04.232698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core import utils\n",
    "\n",
    "training_model = generator.make_training_model()\n",
    "training_model.compile(optimizer=optimizer, loss=[utils.max_margin_loss], metrics={'max_margin': utils.max_margin_loss})\n",
    "history = training_model.fit(x=train_dataloader, batch_size=64, epochs=15)"
   ],
   "id": "b2037a83a19c40c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - loss: 9.5550 - max_margin_loss: 9.5550\n",
      "Epoch 2/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 48ms/step - loss: 4.8051 - max_margin_loss: 4.8051\n",
      "Epoch 3/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 47ms/step - loss: 4.2061 - max_margin_loss: 4.2061\n",
      "Epoch 4/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 48ms/step - loss: 4.0448 - max_margin_loss: 4.0448\n",
      "Epoch 5/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 53ms/step - loss: 3.9617 - max_margin_loss: 3.9617\n",
      "Epoch 6/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.9426 - max_margin_loss: 3.9426\n",
      "Epoch 7/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 53ms/step - loss: 3.8991 - max_margin_loss: 3.8991\n",
      "Epoch 8/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.8871 - max_margin_loss: 3.8871\n",
      "Epoch 9/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.8583 - max_margin_loss: 3.8583\n",
      "Epoch 10/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 53ms/step - loss: 3.8567 - max_margin_loss: 3.8567\n",
      "Epoch 11/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.8637 - max_margin_loss: 3.8637\n",
      "Epoch 12/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 53ms/step - loss: 3.8649 - max_margin_loss: 3.8649\n",
      "Epoch 13/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 53ms/step - loss: 3.8312 - max_margin_loss: 3.8312\n",
      "Epoch 14/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 54ms/step - loss: 3.8144 - max_margin_loss: 3.8144\n",
      "Epoch 15/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.8309 - max_margin_loss: 3.8309\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Persist model",
   "id": "72e9fa24f8cf8218"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:04.752017Z",
     "start_time": "2024-12-03T17:48:04.715757Z"
    }
   },
   "cell_type": "code",
   "source": "training_model.save(\"./../data/abae.kickstarter_removed_64K.keras\")",
   "id": "97b7e4288c9cfc18",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "ebff482ae6fb225e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:05.608611Z",
     "start_time": "2024-12-03T17:48:05.562086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load evaluation model\n",
    "inference_model = generator.make_model(\"./../data/abae.kickstarter_removed_64K.keras\")"
   ],
   "id": "924eaacb4373045b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:06.916303Z",
     "start_time": "2024-12-03T17:48:06.910436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_emb = inference_model.get_layer('word_embedding').get_weights()[0]\n",
    "word_emb = torch.from_numpy(word_emb)\n",
    "\n",
    "word_emb.shape"
   ],
   "id": "7ef33a0ebda5b205",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12922, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:07.399001Z",
     "start_time": "2024-12-03T17:48:07.396327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aspect_embeddings = inference_model.get_layer('weighted_aspect_emb').W\n",
    "vocab_inv = emb_model.model.wv.index_to_key"
   ],
   "id": "b2acc1200eefafc",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:07.668190Z",
     "start_time": "2024-12-03T17:48:07.636049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "#todo method\n",
    "aspect_words = []\n",
    "aspect_index = 0\n",
    "\n",
    "for aspect in aspect_embeddings:\n",
    "    aspect = aspect.cpu()\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    word_emb = word_emb / torch.linalg.norm(word_emb, dim=-1, keepdim=True)\n",
    "    aspect = aspect / torch.linalg.norm(aspect, dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = word_emb.matmul(aspect.T)\n",
    "\n",
    "    numpy_similarity = similarity.detach().numpy()\n",
    "\n",
    "    ordered_words = np.argsort(numpy_similarity)[::-1]\n",
    "    desc_list = [(vocab_inv[w], numpy_similarity[w]) for w in ordered_words[:15]]\n",
    "    aspect_words.append(desc_list)\n",
    "\n",
    "    print(\"Aspect \", aspect_index)\n",
    "    for i in desc_list:\n",
    "        # hr][/i is not a valid word. meh.\n",
    "        print(\"Word: \", i[0], f\"({i[1]})\")\n",
    "\n",
    "    aspect_index += 1"
   ],
   "id": "716414d14e7b10a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect  0\n",
      "Word:  not (0.719325065612793)\n",
      "Word:  will (0.681557297706604)\n",
      "Word:  play (0.6385544538497925)\n",
      "Word:  soon (0.6287999749183655)\n",
      "Word:  say (0.6282198429107666)\n",
      "Word:  excited (0.6232393980026245)\n",
      "Word:  having (0.6223326325416565)\n",
      "Word:  know (0.6069502830505371)\n",
      "Word:  hear (0.598689079284668)\n",
      "Word:  anymore (0.5897776484489441)\n",
      "Word:  try (0.5886120796203613)\n",
      "Word:  finally (0.5843713283538818)\n",
      "Word:  exactly (0.5788878202438354)\n",
      "Word:  sure (0.5717058181762695)\n",
      "Word:  continue (0.5636522769927979)\n",
      "Aspect  1\n",
      "Word:  carte (0.4393817186355591)\n",
      "Word:  paddle (0.31424713134765625)\n",
      "Word:  11/17 (0.26404044032096863)\n",
      "Word:  maxing (0.22924785315990448)\n",
      "Word:  players:2 (0.18000350892543793)\n",
      "Word:  steph (0.176033154129982)\n",
      "Word:  dice (0.17405401170253754)\n",
      "Word:  rule (0.15369495749473572)\n",
      "Word:  65x100 (0.1498761773109436)\n",
      "Word:  upd (0.1420460045337677)\n",
      "Word:  boni (0.13681727647781372)\n",
      "Word:  card (0.13198794424533844)\n",
      "Word:  worker (0.1316823959350586)\n",
      "Word:  lv (0.13022227585315704)\n",
      "Word:  bedford (0.1233481764793396)\n",
      "Aspect  2\n",
      "Word:  gain (0.8589072227478027)\n",
      "Word:  contract (0.8477118015289307)\n",
      "Word:  track (0.8379690647125244)\n",
      "Word:  income (0.8307192921638489)\n",
      "Word:  bonus (0.8294844627380371)\n",
      "Word:  vp (0.8165055513381958)\n",
      "Word:  block (0.815031886100769)\n",
      "Word:  region (0.8142853379249573)\n",
      "Word:  position (0.8139848709106445)\n",
      "Word:  location (0.8098782300949097)\n",
      "Word:  claim (0.8065125942230225)\n",
      "Word:  vps (0.7804019451141357)\n",
      "Word:  marker (0.7762234210968018)\n",
      "Word:  boat (0.775740385055542)\n",
      "Word:  advance (0.7716835737228394)\n",
      "Aspect  3\n",
      "Word:  mix (0.6559852361679077)\n",
      "Word:  economic (0.6466602087020874)\n",
      "Word:  tableau (0.6426592469215393)\n",
      "Word:  combine (0.6330661177635193)\n",
      "Word:  unique (0.6230589747428894)\n",
      "Word:  trading (0.6137843728065491)\n",
      "Word:  deckbuilding (0.6109145283699036)\n",
      "Word:  strong (0.600659966468811)\n",
      "Word:  civ (0.5989500284194946)\n",
      "Word:  tech (0.5975072979927063)\n",
      "Word:  management (0.5909059643745422)\n",
      "Word:  building (0.5908499956130981)\n",
      "Word:  aspect (0.5906785726547241)\n",
      "Word:  combination (0.5845129489898682)\n",
      "Word:  exploration (0.5809946060180664)\n",
      "Aspect  4\n",
      "Word:  <UNK> (0.5990791916847229)\n",
      "Word:  designer (0.40778523683547974)\n",
      "Word:  publisher (0.37603330612182617)\n",
      "Word:  12/20 (0.3737955689430237)\n",
      "Word:  piff (0.3627280294895172)\n",
      "Word:  sophie (0.3559439778327942)\n",
      "Word:  isaac (0.34978726506233215)\n",
      "Word:  j. (0.3471839129924774)\n",
      "Word:  album (0.34280747175216675)\n",
      "Word:  darkgreen][/color (0.3418430685997009)\n",
      "Word:  119 (0.33482712507247925)\n",
      "Word:  sydney (0.32864466309547424)\n",
      "Word:  gadget (0.3276911675930023)\n",
      "Word:  econ (0.32388925552368164)\n",
      "Word:  kmc (0.3215320408344269)\n",
      "Aspect  5\n",
      "Word:  intuitive (0.7642618417739868)\n",
      "Word:  complicated (0.761641263961792)\n",
      "Word:  complex (0.7431743144989014)\n",
      "Word:  fairly (0.7415742874145508)\n",
      "Word:  learning (0.737786591053009)\n",
      "Word:  relatively (0.7317211627960205)\n",
      "Word:  explain (0.7285305261611938)\n",
      "Word:  rule (0.7095352411270142)\n",
      "Word:  iconography (0.7029197216033936)\n",
      "Word:  curve (0.6872715950012207)\n",
      "Word:  confusing (0.6788454055786133)\n",
      "Word:  ruleset (0.6586854457855225)\n",
      "Word:  smooth (0.6577022075653076)\n",
      "Word:  rulebook (0.6565356254577637)\n",
      "Word:  grasp (0.6540411710739136)\n",
      "Aspect  6\n",
      "Word:  family (0.7456180453300476)\n",
      "Word:  gamer (0.7324247360229492)\n",
      "Word:  casual (0.7282995581626892)\n",
      "Word:  party (0.7018249034881592)\n",
      "Word:  young (0.6986703872680664)\n",
      "Word:  adult (0.6838368773460388)\n",
      "Word:  kid (0.6826094388961792)\n",
      "Word:  night (0.6798765659332275)\n",
      "Word:  gateway (0.6766979694366455)\n",
      "Word:  child (0.6750786304473877)\n",
      "Word:  gaming (0.6603400111198425)\n",
      "Word:  social (0.6479837894439697)\n",
      "Word:  boardgame (0.6330210566520691)\n",
      "Word:  friend (0.6302310824394226)\n",
      "Word:  deduction (0.6092300415039062)\n",
      "Aspect  7\n",
      "Word:  outcome (0.7382886409759521)\n",
      "Word:  lucky (0.7359994649887085)\n",
      "Word:  randomness (0.7175490856170654)\n",
      "Word:  mitigate (0.7101521492004395)\n",
      "Word:  success (0.7045121192932129)\n",
      "Word:  die (0.7022254467010498)\n",
      "Word:  result (0.6984173059463501)\n",
      "Word:  probabilistic (0.6882749795913696)\n",
      "Word:  luck (0.6763785481452942)\n",
      "Word:  swing (0.6492516994476318)\n",
      "Word:  unlucky (0.6482275128364563)\n",
      "Word:  random (0.6426502466201782)\n",
      "Word:  happen (0.6406224966049194)\n",
      "Word:  roll (0.6365934610366821)\n",
      "Word:  screw (0.6353144645690918)\n",
      "Aspect  8\n",
      "Word:  5 (0.8060051202774048)\n",
      "Word:  4 (0.7810972929000854)\n",
      "Word:  6 (0.7795281410217285)\n",
      "Word:  2 (0.7643346786499023)\n",
      "Word:  3 (0.7307590246200562)\n",
      "Word:  4p (0.729219913482666)\n",
      "Word:  ease'play (0.7214466333389282)\n",
      "Word:  1 (0.7127108573913574)\n",
      "Word:  min (0.6980432868003845)\n",
      "Word:  4/4 (0.6865743398666382)\n",
      "Word:  max (0.6855091452598572)\n",
      "Word:  7.3 (0.6574913859367371)\n",
      "Word:  60 (0.6573320627212524)\n",
      "Word:  12 (0.6470527648925781)\n",
      "Word:  60–120 (0.6439566612243652)\n",
      "Aspect  9\n",
      "Word:  war (0.721706211566925)\n",
      "Word:  lord (0.6312472820281982)\n",
      "Word:  imperium (0.6078771352767944)\n",
      "Word:  imperial (0.596443772315979)\n",
      "Word:  memoir (0.5949827432632446)\n",
      "Word:  ring (0.5923588275909424)\n",
      "Word:  star (0.5892086029052734)\n",
      "Word:  vi (0.5854843258857727)\n",
      "Word:  trek (0.5812350511550903)\n",
      "Word:  sea (0.5799443125724792)\n",
      "Word:  realm (0.5786351561546326)\n",
      "Word:  civil (0.5753166675567627)\n",
      "Word:  ancient (0.5742967128753662)\n",
      "Word:  dorado (0.5739076137542725)\n",
      "Word:  wing (0.5651799440383911)\n",
      "Aspect  10\n",
      "Word:  insert (0.8290242552757263)\n",
      "Word:  plastic (0.8042731881141663)\n",
      "Word:  print (0.7663625478744507)\n",
      "Word:  storage (0.749025821685791)\n",
      "Word:  cardboard (0.7387248277664185)\n",
      "Word:  mini (0.7273460626602173)\n",
      "Word:  3d (0.7115734219551086)\n",
      "Word:  tray (0.696864128112793)\n",
      "Word:  paint (0.6964814066886902)\n",
      "Word:  miniature (0.6961793303489685)\n",
      "Word:  custom (0.6782448291778564)\n",
      "Word:  sleeve (0.6748064160346985)\n",
      "Word:  mat (0.6656802296638489)\n",
      "Word:  box (0.6601845026016235)\n",
      "Word:  thick (0.6438164710998535)\n",
      "Aspect  11\n",
      "Word:  2017 (0.8244156241416931)\n",
      "Word:  2023 (0.8177643418312073)\n",
      "Word:  2021 (0.8138757348060608)\n",
      "Word:  2020 (0.8129057884216309)\n",
      "Word:  2019 (0.8107484579086304)\n",
      "Word:  2014 (0.8043575286865234)\n",
      "Word:  june (0.8033514022827148)\n",
      "Word:  2022 (0.796685516834259)\n",
      "Word:  2016 (0.7953429222106934)\n",
      "Word:  essen (0.7933415174484253)\n",
      "Word:  2018 (0.7882845401763916)\n",
      "Word:  2015 (0.7880985736846924)\n",
      "Word:  dec (0.786654531955719)\n",
      "Word:  2024 (0.7832876443862915)\n",
      "Word:  october (0.7789247035980225)\n",
      "Aspect  12\n",
      "Word:  design (0.6125873923301697)\n",
      "Word:  graphic (0.5972588062286377)\n",
      "Word:  theme (0.5922380089759827)\n",
      "Word:  presentation (0.5829888582229614)\n",
      "Word:  feld (0.5828423500061035)\n",
      "Word:  artwork (0.5792340040206909)\n",
      "Word:  art (0.5751392841339111)\n",
      "Word:  implementation (0.5552822351455688)\n",
      "Word:  modern (0.5549209117889404)\n",
      "Word:  knizia (0.5543962717056274)\n",
      "Word:  wallace (0.5456991791725159)\n",
      "Word:  euro (0.5376086235046387)\n",
      "Word:  job (0.5318530797958374)\n",
      "Word:  aesthetic (0.5221399068832397)\n",
      "Word:  stylize (0.5202203989028931)\n",
      "Aspect  13\n",
      "Word:  tower (0.5534568428993225)\n",
      "Word:  hadrians (0.524125337600708)\n",
      "Word:  dice (0.4886391758918762)\n",
      "Word:  castle (0.4854046702384949)\n",
      "Word:  replace (0.4819451570510864)\n",
      "Word:  cube (0.4634876847267151)\n",
      "Word:  ship (0.4631989598274231)\n",
      "Word:  meeple (0.45936650037765503)\n",
      "Word:  wall (0.4125358462333679)\n",
      "Word:  bonanza (0.41208040714263916)\n",
      "Word:  place (0.4099637269973755)\n",
      "Word:  proximity (0.404344379901886)\n",
      "Word:  block (0.39971923828125)\n",
      "Word:  galaxy (0.3931801915168762)\n",
      "Word:  land (0.39205896854400635)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:48:36.437212Z",
     "start_time": "2024-12-03T17:48:36.403969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each word of aspect for the aspect we calculate the coherence by AVG distance between top words\n",
    "counter = 0\n",
    "for aspect_most_representative_words in aspect_words:\n",
    "    coherence = []\n",
    "    for word in aspect_most_representative_words:\n",
    "        w, score = word\n",
    "        for word2 in aspect_most_representative_words:\n",
    "            w2, score = word2\n",
    "            if w != w2:\n",
    "                coherence.append(emb_model.model.wv.similarity(w, w2))\n",
    "    # todo fai avgf cosi natualmente sbagliato  \n",
    "    print(f\"Aspect {counter}  has total coherence of\", np.mean(coherence, axis=0))  # AVG\n",
    "    counter += 1"
   ],
   "id": "e6a1250015382aff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0  has total coherence of 0.6825146\n",
      "Aspect 1  has total coherence of 0.22206149\n",
      "Aspect 2  has total coherence of 0.8343714\n",
      "Aspect 3  has total coherence of 0.615195\n",
      "Aspect 4  has total coherence of 0.58994347\n",
      "Aspect 5  has total coherence of 0.66871715\n",
      "Aspect 6  has total coherence of 0.69296455\n",
      "Aspect 7  has total coherence of 0.6968302\n",
      "Aspect 8  has total coherence of 0.73674154\n",
      "Aspect 9  has total coherence of 0.7429665\n",
      "Aspect 10  has total coherence of 0.7725564\n",
      "Aspect 11  has total coherence of 0.9686056\n",
      "Aspect 12  has total coherence of 0.654336\n",
      "Aspect 13  has total coherence of 0.5454725\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:52:06.043017Z",
     "start_time": "2024-12-03T17:52:06.006429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gold_standard_topics = ['luck', 'alea', 'bookkeeping', 'downtime', 'strategy', 'interaction', 'complicated', 'complex',\n",
    "                        '<UNK>']\n",
    "counter = 0\n",
    "for aspect in aspect_embeddings:\n",
    "    aspect = aspect.cpu()\n",
    "\n",
    "    aspect = aspect / torch.linalg.norm(aspect, dim=-1, keepdim=True)\n",
    "    word_emb = word_emb / torch.linalg.norm(word_emb, dim=-1, keepdim=True)\n",
    "\n",
    "    print(\"Aspect \", counter)\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    for topic in gold_standard_topics:\n",
    "        index = emb_model.model.wv.get_index(topic)\n",
    "        print(f\"'{topic}' similarity: \", word_emb[index].dot(aspect))\n",
    "    emb_model.model.wv.get_vector('luck')\n",
    "    counter += 1"
   ],
   "id": "2c3aa83109bb0929",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect  0\n",
      "'luck' similarity:  tensor(-0.2443, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0075, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0400, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.0263, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.0364, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.2552, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.1602, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.0380, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.1207, grad_fn=<DotBackward0>)\n",
      "Aspect  1\n",
      "'luck' similarity:  tensor(-0.0703, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.3540, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.4647, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.2009, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.1533, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.2276, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.3251, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.2667, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.3712, grad_fn=<DotBackward0>)\n",
      "Aspect  2\n",
      "'luck' similarity:  tensor(0.0218, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.1812, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.0339, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.1438, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.2323, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.3088, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.1035, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.2121, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.1188, grad_fn=<DotBackward0>)\n",
      "Aspect  3\n",
      "'luck' similarity:  tensor(0.2476, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.2624, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0246, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.1241, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.3030, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.3119, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.0312, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.1055, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.0392, grad_fn=<DotBackward0>)\n",
      "Aspect  4\n",
      "'luck' similarity:  tensor(-0.1783, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0901, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.1487, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.2147, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.2220, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.0363, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.2273, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.1008, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.5991, grad_fn=<DotBackward0>)\n",
      "Aspect  5\n",
      "'luck' similarity:  tensor(-0.0488, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0338, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.3308, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.2426, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.2933, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.1525, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.7616, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.7432, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.1170, grad_fn=<DotBackward0>)\n",
      "Aspect  6\n",
      "'luck' similarity:  tensor(0.0322, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.1985, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0834, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.0592, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.0186, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.0492, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.0856, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.1138, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.2473, grad_fn=<DotBackward0>)\n",
      "Aspect  7\n",
      "'luck' similarity:  tensor(0.6764, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.2348, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.2188, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.3120, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(0.3192, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.1384, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.0082, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.1199, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.1015, grad_fn=<DotBackward0>)\n",
      "Aspect  8\n",
      "'luck' similarity:  tensor(-0.0478, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(-0.0226, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.1940, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(0.3978, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.0909, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.1309, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.2707, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.1397, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.0965, grad_fn=<DotBackward0>)\n",
      "Aspect  9\n",
      "'luck' similarity:  tensor(-0.2497, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.2614, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(0.2328, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.2721, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.1180, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.1929, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.1076, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0124, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.3900, grad_fn=<DotBackward0>)\n",
      "Aspect  10\n",
      "'luck' similarity:  tensor(-0.1054, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.2336, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0828, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.3013, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.3125, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.2806, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.1819, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.2608, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.0932, grad_fn=<DotBackward0>)\n",
      "Aspect  11\n",
      "'luck' similarity:  tensor(-0.1575, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.3572, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.1130, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.2817, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.3261, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.3969, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.3171, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.2488, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.2788, grad_fn=<DotBackward0>)\n",
      "Aspect  12\n",
      "'luck' similarity:  tensor(-0.0440, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.0062, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0514, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.1604, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.2094, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(0.0815, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(0.0098, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(0.0769, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(-0.2322, grad_fn=<DotBackward0>)\n",
      "Aspect  13\n",
      "'luck' similarity:  tensor(-0.0676, grad_fn=<DotBackward0>)\n",
      "'alea' similarity:  tensor(0.0506, grad_fn=<DotBackward0>)\n",
      "'bookkeeping' similarity:  tensor(-0.0156, grad_fn=<DotBackward0>)\n",
      "'downtime' similarity:  tensor(-0.1270, grad_fn=<DotBackward0>)\n",
      "'strategy' similarity:  tensor(-0.2515, grad_fn=<DotBackward0>)\n",
      "'interaction' similarity:  tensor(-0.1271, grad_fn=<DotBackward0>)\n",
      "'complicated' similarity:  tensor(-0.0525, grad_fn=<DotBackward0>)\n",
      "'complex' similarity:  tensor(-0.0361, grad_fn=<DotBackward0>)\n",
      "'<UNK>' similarity:  tensor(0.1607, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:47:18.930310Z",
     "start_time": "2024-12-03T17:37:08.165753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train more and see\n",
    "history = training_model.fit(x=train_dataloader, batch_size=64, epochs=15)"
   ],
   "id": "47b705594e76876a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 51ms/step - loss: 3.8152 - max_margin_loss: 3.8152\n",
      "Epoch 2/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 50ms/step - loss: 3.8249 - max_margin_loss: 3.8249\n",
      "Epoch 3/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 50ms/step - loss: 3.8182 - max_margin_loss: 3.8182\n",
      "Epoch 4/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - loss: 3.7828 - max_margin_loss: 3.7828\n",
      "Epoch 5/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 52ms/step - loss: 3.8130 - max_margin_loss: 3.8130\n",
      "Epoch 6/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 52ms/step - loss: 3.7875 - max_margin_loss: 3.7875\n",
      "Epoch 7/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 50ms/step - loss: 3.7854 - max_margin_loss: 3.7854\n",
      "Epoch 8/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 50ms/step - loss: 3.7958 - max_margin_loss: 3.7958\n",
      "Epoch 9/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 55ms/step - loss: 3.7932 - max_margin_loss: 3.7932\n",
      "Epoch 10/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 51ms/step - loss: 3.7869 - max_margin_loss: 3.7869\n",
      "Epoch 11/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 54ms/step - loss: 3.7789 - max_margin_loss: 3.7789\n",
      "Epoch 12/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 54ms/step - loss: 3.7820 - max_margin_loss: 3.7820\n",
      "Epoch 13/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 54ms/step - loss: 3.7735 - max_margin_loss: 3.7735\n",
      "Epoch 14/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 54ms/step - loss: 3.7848 - max_margin_loss: 3.7848\n",
      "Epoch 15/15\n",
      "\u001B[1m784/784\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 52ms/step - loss: 3.7727 - max_margin_loss: 3.7727\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Kickstarter More Dataset (256K)",
   "id": "6f50cd04eb84377e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:22.243744Z",
     "start_time": "2024-12-04T16:29:18.188128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train import AbaeModelConfiguration\n",
    "\n",
    "config = AbaeModelConfiguration(\n",
    "    corpus_file=\"../data/corpus.preprocessed.kickstarter_removed.256k.csv\",\n",
    "    model_name=\"abae.kickstarter_removed.256k\",\n",
    "    aspect_size=16,\n",
    "    max_vocab_size=40000,\n",
    ")\n"
   ],
   "id": "ad5dc255d55251d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:22.948785Z",
     "start_time": "2024-12-04T16:29:22.247569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.train import AbaeModelManager\n",
    "\n",
    "manager = AbaeModelManager(config)"
   ],
   "id": "12a72e1da2cf0b14",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from output/abae.kickstarter_removed.256k.embeddings.model\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'output/abae.kickstarter_removed.256k.embeddings.model', 'mode': 'rb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:loading wv recursively from output/abae.kickstarter_removed.256k.embeddings.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'output/abae.kickstarter_removed.256k.embeddings.model', 'datetime': '2024-12-04T17:29:22.947195', 'gensim': '4.3.3', 'python': '3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]', 'platform': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.39', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:35.047803Z",
     "start_time": "2024-12-04T16:29:22.949400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocabulary = manager.embedding_model.model.wv.key_to_index\n",
    "train = dataset.PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=vocabulary, csv_dataset_path=\"../data/corpus.preprocessed.kickstarter_removed.256k.csv\", negative_size=15\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)"
   ],
   "id": "796f5358daedef4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model.\n",
      "Loading dataset from file: ../data/corpus.preprocessed.kickstarter_removed.256k.csv\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/200529 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9572b253f8b542afa7f166f3a98ad26a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "We loose information on 538 points.This is 0.2682903719661495% of the dataset.\n",
      "Padding sequences to max length (256).\n",
      "Max sequence length is:  1999  but we will limit sequences to 256 tokens.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:48:58.636622Z",
     "start_time": "2024-12-04T15:48:58.506856Z"
    }
   },
   "cell_type": "code",
   "source": "train_model = manager.prepare_training_model(optimizer='adam')",
   "id": "76cdfba9f80eec91",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:25:50.237763Z",
     "start_time": "2024-12-04T15:50:11.314393Z"
    }
   },
   "cell_type": "code",
   "source": "train_model.fit(train_dataloader, epochs=5, batch_size=64)",
   "id": "1c58704f9ec61287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m3134/3134\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m423s\u001B[0m 135ms/step - loss: 6.5713 - max_margin_loss: 6.5713\n",
      "Epoch 2/5\n",
      "\u001B[1m3134/3134\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m429s\u001B[0m 137ms/step - loss: 3.6638 - max_margin_loss: 3.6638\n",
      "Epoch 3/5\n",
      "\u001B[1m3134/3134\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m433s\u001B[0m 138ms/step - loss: 3.5371 - max_margin_loss: 3.5371\n",
      "Epoch 4/5\n",
      "\u001B[1m3134/3134\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m425s\u001B[0m 136ms/step - loss: 3.4970 - max_margin_loss: 3.4970\n",
      "Epoch 5/5\n",
      "\u001B[1m3134/3134\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m428s\u001B[0m 137ms/step - loss: 3.4750 - max_margin_loss: 3.4750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x702e3cf0c950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:28:13.954419Z",
     "start_time": "2024-12-04T16:28:13.951710Z"
    }
   },
   "cell_type": "code",
   "source": "manager.persist_model()",
   "id": "b678d72f6174a456",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/abae.kickstarter_removed.256k.keras'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:38.283623Z",
     "start_time": "2024-12-04T16:29:38.154658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#manager.persist_model()\n",
    "inference_model = manager.prepare_evaluation_model()"
   ],
   "id": "5d77ea233ddc2522",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacopo/PycharmProjects/nlp-course-project/core/layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the core instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:40.685963Z",
     "start_time": "2024-12-04T16:29:40.683928Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "d3f813073cb1bfb9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:40.926113Z",
     "start_time": "2024-12-04T16:29:40.906034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_emb = inference_model.get_layer('word_embedding').get_weights()[0]\n",
    "word_emb = torch.from_numpy(word_emb)\n",
    "\n",
    "word_emb.shape"
   ],
   "id": "f8400d90c97b1763",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25055, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:35:06.390015Z",
     "start_time": "2024-12-04T16:35:06.387528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aspect_embeddings = inference_model.get_layer('weighted_aspect_emb').W\n",
    "vocab_inv = manager.embedding_model.model.wv.index_to_key"
   ],
   "id": "b5708556da62ff70",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:35:16.527122Z",
     "start_time": "2024-12-04T16:35:16.524050Z"
    }
   },
   "cell_type": "code",
   "source": "type(aspect_embeddings[0])",
   "id": "f800d62d999e28fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:38:05.729258Z",
     "start_time": "2024-12-04T16:38:05.659790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "#todo method\n",
    "aspect_words = []\n",
    "aspect_index = 0\n",
    "aspect_embeddings = aspect_embeddings / torch.linalg.norm(aspect_embeddings, dim=-1, keepdim=True)\n",
    "for aspect in aspect_embeddings:\n",
    "    aspect = aspect.cpu()\n",
    "    # Calculate the cosine similarity of each word with the aspect\n",
    "    word_emb = word_emb / torch.linalg.norm(word_emb, dim=-1, keepdim=True)\n",
    "    #aspect = aspect / torch.linalg.norm(aspect, dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = word_emb.matmul(aspect.T)\n",
    "\n",
    "    numpy_similarity = similarity.detach().numpy()\n",
    "\n",
    "    ordered_words = np.argsort(numpy_similarity)[::-1]\n",
    "    desc_list = [(vocab_inv[w], numpy_similarity[w]) for w in ordered_words[:15]]\n",
    "    aspect_words.append(desc_list)\n",
    "\n",
    "    print(\"Aspect \", aspect_index)\n",
    "    for i in desc_list:\n",
    "        # hr][/i is not a valid word. meh.\n",
    "        print(\"Word: \", i[0], f\"({i[1]})\")\n",
    "\n",
    "    aspect_index += 1"
   ],
   "id": "d5744e2eb3c56a00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect  0\n",
      "Word:  successor (0.7392485737800598)\n",
      "Word:  caylus (0.7310449481010437)\n",
      "Word:  tigris (0.7292113900184631)\n",
      "Word:  vein (0.7269777655601501)\n",
      "Word:  concordia (0.7265373468399048)\n",
      "Word:  nova (0.7137109637260437)\n",
      "Word:  brass (0.7124441862106323)\n",
      "Word:  patchwork (0.7122060656547546)\n",
      "Word:  goa (0.7093117237091064)\n",
      "Word:  lorenzo (0.7025164365768433)\n",
      "Word:  predecessor (0.6919888257980347)\n",
      "Word:  scythe (0.6902366876602173)\n",
      "Word:  carcassone (0.6799776554107666)\n",
      "Word:  splendor (0.6772586107254028)\n",
      "Word:  istanbul (0.6771898865699768)\n",
      "Aspect  1\n",
      "Word:  illustration (0.8313093185424805)\n",
      "Word:  colorful (0.8279768228530884)\n",
      "Word:  sturdy (0.8033429384231567)\n",
      "Word:  functional (0.7723512649536133)\n",
      "Word:  thick (0.766491711139679)\n",
      "Word:  presentation (0.7586277723312378)\n",
      "Word:  vibrant (0.7327176332473755)\n",
      "Word:  bright (0.732177734375)\n",
      "Word:  artwork (0.7297290563583374)\n",
      "Word:  chunky (0.727603554725647)\n",
      "Word:  adorable (0.7207970023155212)\n",
      "Word:  colourful (0.7113686203956604)\n",
      "Word:  visually (0.7086130976676941)\n",
      "Word:  aesthetically (0.699967622756958)\n",
      "Word:  gorgeous (0.6986837387084961)\n",
      "Aspect  2\n",
      "Word:  government (0.6537379026412964)\n",
      "Word:  pound (0.6390856504440308)\n",
      "Word:  slave (0.6383841037750244)\n",
      "Word:  takeover (0.6376345157623291)\n",
      "Word:  indirectly (0.637200653553009)\n",
      "Word:  rice (0.6238287687301636)\n",
      "Word:  russia (0.6230993270874023)\n",
      "Word:  revolt (0.6208958029747009)\n",
      "Word:  carriage (0.6183582544326782)\n",
      "Word:  airborne (0.6181812882423401)\n",
      "Word:  patriot (0.6179949641227722)\n",
      "Word:  peasant (0.6166446805000305)\n",
      "Word:  chief (0.61564040184021)\n",
      "Word:  ore (0.6151481866836548)\n",
      "Word:  missile (0.6138739585876465)\n",
      "Aspect  3\n",
      "Word:  playmat (0.773808479309082)\n",
      "Word:  bundle (0.7345288395881653)\n",
      "Word:  exclusive (0.7040220499038696)\n",
      "Word:  backer (0.7020175457000732)\n",
      "Word:  promos (0.6994059085845947)\n",
      "Word:  ks (0.6845317482948303)\n",
      "Word:  addon (0.6824579834938049)\n",
      "Word:  preordere (0.6769403219223022)\n",
      "Word:  kit (0.6731395721435547)\n",
      "Word:  incl (0.6719321012496948)\n",
      "Word:  ebay (0.671379804611206)\n",
      "Word:  retail (0.6695592403411865)\n",
      "Word:  separately (0.6644646525382996)\n",
      "Word:  -plus (0.6613636612892151)\n",
      "Word:  kickstarte (0.6577873826026917)\n",
      "Aspect  4\n",
      "Word:  theming (0.6056111454963684)\n",
      "Word:  artwork- (0.5323972105979919)\n",
      "Word:  mechanisms- (0.5203343629837036)\n",
      "Word:  cyberpunk (0.5156962275505066)\n",
      "Word:  components- (0.49776193499565125)\n",
      "Word:  ip (0.4850093722343445)\n",
      "Word:  theme (0.47910934686660767)\n",
      "Word:  sibbling (0.4679129123687744)\n",
      "Word:  storytelling (0.4626336693763733)\n",
      "Word:  microbadge=34746 (0.4447489380836487)\n",
      "Word:  sol (0.4338463544845581)\n",
      "Word:  eurogame (0.4333736002445221)\n",
      "Word:  presentation (0.4269278645515442)\n",
      "Word:  pfister (0.42535629868507385)\n",
      "Word:  humour (0.4231661558151245)\n",
      "Aspect  5\n",
      "Word:  tedious (0.7040528655052185)\n",
      "Word:  uninteresting (0.6661335229873657)\n",
      "Word:  pointless (0.6614547967910767)\n",
      "Word:  ultimately (0.6479504108428955)\n",
      "Word:  slog (0.645614743232727)\n",
      "Word:  procedural (0.6411250829696655)\n",
      "Word:  unsatisfying (0.6237403154373169)\n",
      "Word:  unforgiving (0.6145417094230652)\n",
      "Word:  dull (0.6091749668121338)\n",
      "Word:  arbitrary (0.6034442186355591)\n",
      "Word:  harsh (0.5990813374519348)\n",
      "Word:  liking (0.5989380478858948)\n",
      "Word:  painful (0.5880246758460999)\n",
      "Word:  disappointing (0.5827527642250061)\n",
      "Word:  shallow (0.582634687423706)\n",
      "Aspect  6\n",
      "Word:  engaging (0.7185422778129578)\n",
      "Word:  puzzly (0.7067983150482178)\n",
      "Word:  thoughtful (0.6699002981185913)\n",
      "Word:  intriguing (0.6347543001174927)\n",
      "Word:  satisfy (0.6342341899871826)\n",
      "Word:  fascinating (0.6336718201637268)\n",
      "Word:  delightful (0.6239219307899475)\n",
      "Word:  immersive (0.6169160604476929)\n",
      "Word:  intense (0.6093376278877258)\n",
      "Word:  intricate (0.6018016934394836)\n",
      "Word:  blend (0.5988197326660156)\n",
      "Word:  surprising (0.5976182222366333)\n",
      "Word:  emergent (0.5904868245124817)\n",
      "Word:  innovative (0.5901941061019897)\n",
      "Word:  wonderfully (0.5896987915039062)\n",
      "Aspect  7\n",
      "Word:  2012 (0.8957140445709229)\n",
      "Word:  2011 (0.8950109481811523)\n",
      "Word:  2009 (0.8904664516448975)\n",
      "Word:  2007 (0.8823026418685913)\n",
      "Word:  2008 (0.8765960931777954)\n",
      "Word:  january (0.8718916177749634)\n",
      "Word:  2010 (0.8690791726112366)\n",
      "Word:  2016 (0.8681637048721313)\n",
      "Word:  2024 (0.8656597137451172)\n",
      "Word:  jan (0.864478349685669)\n",
      "Word:  aug (0.8642047643661499)\n",
      "Word:  sept (0.8639708757400513)\n",
      "Word:  august (0.8623991012573242)\n",
      "Word:  2014 (0.8601714372634888)\n",
      "Word:  feb (0.8594971895217896)\n",
      "Aspect  8\n",
      "Word:  5p (0.7386441826820374)\n",
      "Word:  6p (0.731103777885437)\n",
      "Word:  2.5 (0.7261812686920166)\n",
      "Word:  2,3 (0.7075560688972473)\n",
      "Word:  1–4 (0.6965786218643188)\n",
      "Word:  120 (0.6900813579559326)\n",
      "Word:  minimum (0.6892633438110352)\n",
      "Word:  4 (0.6830364465713501)\n",
      "Word:  3.5 (0.6827676296234131)\n",
      "Word:  5 (0.6705047488212585)\n",
      "Word:  hrs (0.6703525185585022)\n",
      "Word:  p (0.6677823066711426)\n",
      "Word:  2 (0.663541316986084)\n",
      "Word:  3–5 (0.6587302684783936)\n",
      "Word:  | (0.6584082841873169)\n",
      "Aspect  9\n",
      "Word:  haba (0.6249263882637024)\n",
      "Word:  de (0.6214011907577515)\n",
      "Word:  g (0.6091287732124329)\n",
      "Word:  tabula (0.6086819171905518)\n",
      "Word:  r (0.5986111164093018)\n",
      "Word:  duchess (0.5939171314239502)\n",
      "Word:  desde (0.5936492085456848)\n",
      "Word:  46 (0.5928093194961548)\n",
      "Word:  kitty (0.5900702476501465)\n",
      "Word:  100pk (0.5896608829498291)\n",
      "Word:  der (0.5892238616943359)\n",
      "Word:  halloween (0.5882336497306824)\n",
      "Word:  mdg-7104 (0.586501955986023)\n",
      "Word:  harley (0.5863574743270874)\n",
      "Word:  beer (0.5862794518470764)\n",
      "Aspect  10\n",
      "Word:  learn (0.8976333141326904)\n",
      "Word:  teach (0.8746163845062256)\n",
      "Word:  grasp (0.8320752382278442)\n",
      "Word:  explain (0.8127039670944214)\n",
      "Word:  teaching (0.7579811811447144)\n",
      "Word:  understand (0.7480486035346985)\n",
      "Word:  grok (0.7227581143379211)\n",
      "Word:  digest (0.704822301864624)\n",
      "Word:  internalize (0.6550202965736389)\n",
      "Word:  learning (0.6286235451698303)\n",
      "Word:  hang (0.6033832430839539)\n",
      "Word:  newbie (0.601612389087677)\n",
      "Word:  intimidate (0.593424916267395)\n",
      "Word:  comprehend (0.5811084508895874)\n",
      "Word:  internalise (0.5792596340179443)\n",
      "Aspect  11\n",
      "Word:  hut (0.7790427207946777)\n",
      "Word:  crop (0.7767229080200195)\n",
      "Word:  district (0.7736755609512329)\n",
      "Word:  contract (0.7651358246803284)\n",
      "Word:  currency (0.7630131244659424)\n",
      "Word:  palace (0.7591584920883179)\n",
      "Word:  plant (0.755264163017273)\n",
      "Word:  harvest (0.7329472899436951)\n",
      "Word:  disc (0.7323476672172546)\n",
      "Word:  tent (0.7311030030250549)\n",
      "Word:  valuable (0.7309062480926514)\n",
      "Word:  income (0.7275106310844421)\n",
      "Word:  mining (0.7260744571685791)\n",
      "Word:  caravan (0.7228035926818848)\n",
      "Word:  boat (0.7216872572898865)\n",
      "Aspect  12\n",
      "Word:  manual (0.8068217039108276)\n",
      "Word:  instruction (0.794753909111023)\n",
      "Word:  faq (0.7892135381698608)\n",
      "Word:  booklet (0.7840954661369324)\n",
      "Word:  index (0.7768781781196594)\n",
      "Word:  rulebook (0.7751757502555847)\n",
      "Word:  reference (0.7750813364982605)\n",
      "Word:  typo (0.7634032964706421)\n",
      "Word:  clarification (0.7510778903961182)\n",
      "Word:  paragraph (0.7408996820449829)\n",
      "Word:  wording (0.7395058870315552)\n",
      "Word:  forum (0.7378412485122681)\n",
      "Word:  glossary (0.7334437966346741)\n",
      "Word:  consult (0.7199386954307556)\n",
      "Word:  unclear (0.7195554971694946)\n",
      "Aspect  13\n",
      "Word:  eliminate (0.7285048961639404)\n",
      "Word:  initiative (0.6980313062667847)\n",
      "Word:  randomly (0.6952903866767883)\n",
      "Word:  outcome (0.6767880916595459)\n",
      "Word:  effectively (0.6764274835586548)\n",
      "Word:  discard (0.6756821870803833)\n",
      "Word:  determine (0.6727367639541626)\n",
      "Word:  effect (0.672685980796814)\n",
      "Word:  prevent (0.6725395917892456)\n",
      "Word:  potentially (0.6654207706451416)\n",
      "Word:  beneficial (0.6639979481697083)\n",
      "Word:  simultaneously (0.6603032350540161)\n",
      "Word:  directly (0.6594131588935852)\n",
      "Word:  occur (0.655897855758667)\n",
      "Word:  ensure (0.6555535793304443)\n",
      "Aspect  14\n",
      "Word:  shadow (0.791347861289978)\n",
      "Word:  legend (0.7901989817619324)\n",
      "Word:  warhammer (0.7698391675949097)\n",
      "Word:  invasion (0.7605602741241455)\n",
      "Word:  mansion (0.7588604092597961)\n",
      "Word:  vampire (0.752719521522522)\n",
      "Word:  wrath (0.748620331287384)\n",
      "Word:  brimstone (0.741341233253479)\n",
      "Word:  demon (0.7389448881149292)\n",
      "Word:  mythic (0.7384161353111267)\n",
      "Word:  firefly (0.7376453876495361)\n",
      "Word:  heroic (0.7284348011016846)\n",
      "Word:  runebound (0.7236553430557251)\n",
      "Word:  greek (0.7224931120872498)\n",
      "Word:  rogue (0.719290018081665)\n",
      "Aspect  15\n",
      "Word:  request (0.6947983503341675)\n",
      "Word:  wife (0.685591459274292)\n",
      "Word:  occasion (0.6790377497673035)\n",
      "Word:  anymore (0.6783737540245056)\n",
      "Word:  friend (0.6666617393493652)\n",
      "Word:  regularly (0.6568692922592163)\n",
      "Word:  bored (0.6476596593856812)\n",
      "Word:  happily (0.6414668560028076)\n",
      "Word:  folk (0.6311484575271606)\n",
      "Word:  convince (0.6278557777404785)\n",
      "Word:  husband (0.6261847019195557)\n",
      "Word:  partner (0.6122449636459351)\n",
      "Word:  anytime (0.6119512915611267)\n",
      "Word:  ask (0.6043254137039185)\n",
      "Word:  everybody (0.6041883230209351)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:42.504519Z",
     "start_time": "2024-12-04T16:29:42.477914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each word of aspect for the aspect we calculate the coherence by AVG distance between top words\n",
    "counter = 0\n",
    "for aspect_most_representative_words in aspect_words:\n",
    "    coherence = []\n",
    "    for word in aspect_most_representative_words:\n",
    "        w, score = word\n",
    "        for word2 in aspect_most_representative_words:\n",
    "            w2, score = word2\n",
    "            if w != w2:\n",
    "                coherence.append(manager.embedding_model.model.wv.similarity(w, w2))\n",
    "    # todo fai avgf cosi natualmente sbagliato  \n",
    "    print(f\"Aspect {counter}  has total coherence of\", np.mean(coherence, axis=0))  # AVG\n",
    "    counter += 1"
   ],
   "id": "b87ccaeccafba997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0  has total coherence of 0.6461885\n",
      "Aspect 1  has total coherence of 0.6505935\n",
      "Aspect 2  has total coherence of 0.67488617\n",
      "Aspect 3  has total coherence of 0.6656921\n",
      "Aspect 4  has total coherence of 0.4421558\n",
      "Aspect 5  has total coherence of 0.5678725\n",
      "Aspect 6  has total coherence of 0.54107755\n",
      "Aspect 7  has total coherence of 0.9236712\n",
      "Aspect 8  has total coherence of 0.58108777\n",
      "Aspect 9  has total coherence of 0.6290098\n",
      "Aspect 10  has total coherence of 0.59425944\n",
      "Aspect 11  has total coherence of 0.6616599\n",
      "Aspect 12  has total coherence of 0.7247656\n",
      "Aspect 13  has total coherence of 0.5515223\n",
      "Aspect 14  has total coherence of 0.6631714\n",
      "Aspect 15  has total coherence of 0.52314514\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:42.757708Z",
     "start_time": "2024-12-04T16:29:42.755931Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a33f2f2d3fdd8675",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:42.922060Z",
     "start_time": "2024-12-04T16:29:42.920496Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9f9185923303f67c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "289b3875385912d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
