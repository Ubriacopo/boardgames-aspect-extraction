{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#https://github.com/alexeyev/abae-pytorch \n",
    "\n",
    "# TODO pulisci questo file\n",
    "# todo  prova con is_alpha in preprocessing e buttiamo via il resto (con sequenze piu lunghe)!\n",
    "#  --> leggi top 1000 di restaruant e vedi split con '' quanto lunghe in media\n",
    "# todo fix regularization sia su matrix che generation di aspects / emb\n",
    "\n",
    "# https://arxiv.org/pdf/1803.09820\n",
    "# https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/"
   ],
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Una considerazione: Potrei sfruttare gli aspetti inferiti da quello che ho per filtrare via dal testo documenti che parlano di \"Piattaforme\", \"Kickstarter\", \"Materiali\" visto che queste le becca con facilita.\n",
    "# Il mio dataset é probabilmente troppo vario al momento. Un filtraggio di questo tipo faciliterebbe l'apprendimento.\n",
    "\n",
    "# Filtro a mano i commenti scaricati per togliere quelli poco significativi? Riduco rumore!"
   ],
   "id": "49f165d09275bde5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset. <br>\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] <br>\n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors <br>\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Choice of max_seq size is relative to the ds. 95th percentile (at max 5% loss of information)",
   "id": "377fe340c9c009f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T23:26:24.413127Z",
     "start_time": "2025-02-04T23:26:10.021172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PandasPositiveNegativeNumericTextDataset\n",
    "from core.train import ABAEModelConfiguration, ABAEModelManager\n",
    "import pandas as pd\n",
    "\n",
    "corpus = \"../output/dataset/default-pre-processed/80k.preprocessed.csv\"\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"ff\")\n",
    "config.epochs = 1  # Just one epoch\n",
    "config.max_sequence_length = 40\n",
    "config.aspect_size = 16\n",
    "config.negative_sample_size = 20\n",
    "print(f\"Running on default config:\\n {config}\")\n",
    "\n",
    "# Without any hp tuning we just try and see how it goes.\n",
    "manager = ABAEModelManager(config)\n",
    "train_dataset = PandasPositiveNegativeNumericTextDataset(\n",
    "    dataframe=pd.read_csv(corpus),\n",
    "    vocabulary=manager.embedding_model.vocabulary(),\n",
    "    negative_size=config.negative_sample_size,\n",
    "    max_seq_length=config.max_sequence_length\n",
    ")"
   ],
   "id": "830abaf1d37c6bb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on default config:\n",
      " ABAEModelConfiguration(corpus_file='../output/dataset/default-pre-processed/80k.preprocessed.csv', model_name='ff', aspect_size=16, max_vocab_size=None, embedding_size=200, learning_rate=0.01, decay_rate=0.95, momentum=0.9, max_sequence_length=40, negative_sample_size=20, batch_size=64, epochs=1, output_path='./output')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/94248 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "298a7e022dc1495291d4214cef56c5b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/94248 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "216f33de15b64104bc9649927d5aa661"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 82472 words, keeping 4112 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 168974 words, keeping 4464 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 252543 words, keeping 4521 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 335541 words, keeping 4536 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 419176 words, keeping 4538 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 504237 words, keeping 4538 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 587063 words, keeping 4543 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 672240 words, keeping 4543 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 754565 words, keeping 4544 word types\n",
      "INFO:gensim.models.word2vec:collected 4544 word types from a corpus of 790458 raw words and 94248 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4544 unique words (100.00% of original 4544, drops 0)', 'datetime': '2025-02-05T00:26:18.741946', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 790458 word corpus (100.00% of original 790458, drops 0)', 'datetime': '2025-02-05T00:26:18.743449', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 4544 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 630285.2688280742 word corpus (79.7%% of prior 790458)', 'datetime': '2025-02-05T00:26:18.766964', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:estimated required memory for 4544 words and 200 dimensions: 9542400 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-02-05T00:26:18.806964', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4544 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-02-05T00:26:18.807464', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 80 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 25 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 28 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 790458 raw words (630147 effective words) took 0.7s, 853551 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 80 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 25 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 29 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 790458 raw words (630151 effective words) took 0.7s, 844996 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 80 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 790458 raw words (630503 effective words) took 0.8s, 812892 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 80 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 790458 raw words (629947 effective words) took 0.8s, 809059 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 80 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 26 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 27 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 790458 raw words (630164 effective words) took 0.8s, 821740 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 3952290 raw words (3150912 effective words) took 3.8s, 820392 effective words/s', 'datetime': '2025-02-05T00:26:22.648846', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4544, vector_size=200, alpha=0.025>', 'datetime': '2025-02-05T00:26:22.649848', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './output/ff/embeddings.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-02-05T00:26:22.652845', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './output/ff/embeddings.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./output/ff/embeddings.model\n",
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/94248 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4765e9131f2405bbc404b3d612ecd2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  416 . The limit is set to 40 tokens.\n",
      "We loose information on 166 points.This is 0.17613105848399965% of the dataset.\n",
      "Padding sequences to length (40).\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T23:36:37.468700Z",
     "start_time": "2025-02-04T23:32:54.488216Z"
    }
   },
   "cell_type": "code",
   "source": "manager.run_train_process(train_dataset, optimizer='adam')",
   "id": "d3278a09ace24e58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1473/1473\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 151ms/step - loss: 12.9852 - max_margin_loss: 12.9535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1473/1473\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m223s\u001B[0m 151ms/step - loss: 12.9837 - max_margin_loss: 12.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.src.callbacks.history.History at 0x1ffb5708f50>,\n",
       " <Functional name=functional_2, built=True>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:48:57.254300Z",
     "start_time": "2025-02-03T16:48:57.203458Z"
    }
   },
   "cell_type": "code",
   "source": "iteration_model = manager.get_training_model()",
   "id": "c4e25c02eab60eff",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:49:18.995237Z",
     "start_time": "2025-02-03T16:49:01.224869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "from keras import ops as K\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embeddings').weights[0].value.data\n",
    "aspect_embeddings = iteration_model.get_layer('aspect_embeddings').w\n",
    "\n",
    "word_emb = torch.nn.functional.normalize(word_emb, p=2, dim=-1)\n",
    "aspect_embeddings = torch.nn.functional.normalize(aspect_embeddings, p=2, dim=-1)\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=50) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(corpus, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "db228e03f66dec96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  chapter (0.4432419240474701)\n",
      "Word:  mistake (0.4312449097633362)\n",
      "Word:  error (0.3956793248653412)\n",
      "Word:  target (0.3923909664154053)\n",
      "Word:  forum (0.38552427291870117)\n",
      "Word:  playthrough (0.38476938009262085)\n",
      "Word:  spoiler (0.378058522939682)\n",
      "Word:  halfway (0.37712612748146057)\n",
      "Word:  repair (0.3751535415649414)\n",
      "Word:  reach (0.3593621850013733)\n",
      "Word:  answer (0.3591642677783966)\n",
      "Word:  spawn (0.355338454246521)\n",
      "Word:  stop (0.34936630725860596)\n",
      "Word:  passage (0.3478010296821594)\n",
      "Word:  tank (0.3472113609313965)\n",
      "Word:  punish (0.33871927857398987)\n",
      "Word:  lidless (0.3372008800506592)\n",
      "Word:  finish (0.3365933895111084)\n",
      "Word:  raid (0.33627763390541077)\n",
      "Word:  correct (0.3348654508590698)\n",
      "Word:  impossible (0.33347582817077637)\n",
      "Word:  correctly (0.33326420187950134)\n",
      "Word:  faq (0.3319393992424011)\n",
      "Word:  manual (0.32983875274658203)\n",
      "Word:  sight (0.3298065662384033)\n",
      "Word:  failure (0.32836371660232544)\n",
      "Word:  runaway (0.32643431425094604)\n",
      "Word:  ambiguity (0.3258361220359802)\n",
      "Word:  giver (0.3253058195114136)\n",
      "Word:  defeat (0.3214990198612213)\n",
      "Word:  sneak (0.3203740417957306)\n",
      "Word:  marine (0.31957191228866577)\n",
      "Word:  hint (0.31882941722869873)\n",
      "Word:  hunter (0.31749752163887024)\n",
      "Word:  gap (0.3170555830001831)\n",
      "Word:  guard (0.31312111020088196)\n",
      "Word:  cause (0.31239619851112366)\n",
      "Word:  errata (0.3123917877674103)\n",
      "Word:  wrong (0.311234712600708)\n",
      "Word:  cheat (0.3101232945919037)\n",
      "Word:  rule (0.30871862173080444)\n",
      "Word:  opaque (0.3069268465042114)\n",
      "Word:  fail (0.3064453601837158)\n",
      "Word:  visit (0.30572840571403503)\n",
      "Word:  walk (0.3053928017616272)\n",
      "Word:  refer (0.30483362078666687)\n",
      "Word:  abruptly (0.3045433461666107)\n",
      "Word:  somebody (0.30307674407958984)\n",
      "Word:  paragraph (0.30091550946235657)\n",
      "Word:  achieve (0.3000985383987427)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  ruleset (0.5455198287963867)\n",
      "Word:  nuance (0.5116507411003113)\n",
      "Word:  rule (0.4860306978225708)\n",
      "Word:  gameplay (0.46318626403808594)\n",
      "Word:  incredibly (0.4596996605396271)\n",
      "Word:  emergent (0.45180898904800415)\n",
      "Word:  rewarding (0.439881831407547)\n",
      "Word:  overly (0.43513983488082886)\n",
      "Word:  intuitive (0.4334307312965393)\n",
      "Word:  intriguing (0.4277477264404297)\n",
      "Word:  albeit (0.42602112889289856)\n",
      "Word:  straightforward (0.4195247292518616)\n",
      "Word:  engaging (0.41842132806777954)\n",
      "Word:  smooth (0.41340941190719604)\n",
      "Word:  reading (0.4081358015537262)\n",
      "Word:  replayable (0.407798707485199)\n",
      "Word:  fairly (0.4034750461578369)\n",
      "Word:  deceptively (0.3979814946651459)\n",
      "Word:  grasp (0.3928791880607605)\n",
      "Word:  surprisingly (0.39099037647247314)\n",
      "Word:  overhead (0.39001762866973877)\n",
      "Word:  subtlety (0.3863677978515625)\n",
      "Word:  brilliantly (0.385282427072525)\n",
      "Word:  depth (0.3847281336784363)\n",
      "Word:  procedural (0.38371771574020386)\n",
      "Word:  mesh (0.3790186643600464)\n",
      "Word:  complicate (0.3789301812648773)\n",
      "Word:  paced (0.37704968452453613)\n",
      "Word:  challenging (0.37504416704177856)\n",
      "Word:  arc (0.37408679723739624)\n",
      "Word:  mechanical (0.37112975120544434)\n",
      "Word:  familiar (0.3708350658416748)\n",
      "Word:  smoothly (0.36924633383750916)\n",
      "Word:  flow (0.3687208890914917)\n",
      "Word:  reasonably (0.36848753690719604)\n",
      "Word:  shallow (0.3653368353843689)\n",
      "Word:  minimal (0.36517956852912903)\n",
      "Word:  surprising (0.3651394546031952)\n",
      "Word:  deterministic (0.3646908104419708)\n",
      "Word:  clean (0.3644271492958069)\n",
      "Word:  entertaining (0.36383265256881714)\n",
      "Word:  subtle (0.3609985113143921)\n",
      "Word:  narrative (0.3607192039489746)\n",
      "Word:  scope (0.3600810766220093)\n",
      "Word:  glance (0.35956764221191406)\n",
      "Word:  snappy (0.3585965037345886)\n",
      "Word:  breeze (0.3547596335411072)\n",
      "Word:  unpredictable (0.3540481925010681)\n",
      "Word:  tactic (0.3540225923061371)\n",
      "Word:  simple (0.35317522287368774)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  <game_name> (0.6937202215194702)\n",
      "Word:  promos (0.46496763825416565)\n",
      "Word:  flames (0.4475317597389221)\n",
      "Word:  de (0.42717623710632324)\n",
      "Word:  promo (0.42607003450393677)\n",
      "Word:  url (0.4244347810745239)\n",
      "Word:  gearloc (0.41592341661453247)\n",
      "Word:  exp (0.40725165605545044)\n",
      "Word:  expansions (0.4030020833015442)\n",
      "Word:  hill (0.40004613995552063)\n",
      "Word:  ii (0.39118272066116333)\n",
      "Word:  wing (0.3836468458175659)\n",
      "Word:  includes (0.3835884630680084)\n",
      "Word:  usa (0.37958335876464844)\n",
      "Word:  v (0.3789827525615692)\n",
      "Word:  north (0.37642019987106323)\n",
      "Word:  sleeves (0.37396371364593506)\n",
      "Word:  flight (0.36350902915000916)\n",
      "Word:  playmat (0.3593284487724304)\n",
      "Word:  premium (0.35373878479003906)\n",
      "Word:  elves (0.3530803322792053)\n",
      "Word:  cards (0.3527919352054596)\n",
      "Word:  pacific (0.34649449586868286)\n",
      "Word:  traded (0.3426750898361206)\n",
      "Word:  aces (0.3417538106441498)\n",
      "Word:  elf (0.34034329652786255)\n",
      "Word:  queen (0.3338134288787842)\n",
      "Word:  pledge (0.33304262161254883)\n",
      "Word:  crossover (0.331498920917511)\n",
      "Word:  status (0.33051443099975586)\n",
      "Word:  printed (0.32994598150253296)\n",
      "Word:  phoenix (0.3290007710456848)\n",
      "Word:  summoner (0.32870012521743774)\n",
      "Word:  mexico (0.32864564657211304)\n",
      "Word:  collector (0.32793793082237244)\n",
      "Word:  imperium (0.3275521993637085)\n",
      "Word:  addon (0.32737332582473755)\n",
      "Word:  j (0.3265290856361389)\n",
      "Word:  edition (0.3261864483356476)\n",
      "Word:  compatible (0.3246051073074341)\n",
      "Word:  retail (0.3243259787559509)\n",
      "Word:  sleeved (0.32420921325683594)\n",
      "Word:  anniversary (0.3237142264842987)\n",
      "Word:  ks (0.32107865810394287)\n",
      "Word:  games (0.3208327889442444)\n",
      "Word:  metal (0.3206044137477875)\n",
      "Word:  south (0.3200807273387909)\n",
      "Word:  following (0.3191990256309509)\n",
      "Word:  dec (0.31705254316329956)\n",
      "Word:  accessory (0.3164843022823334)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  activate (0.5255782604217529)\n",
      "Word:  column (0.506187915802002)\n",
      "Word:  adjacent (0.48445558547973633)\n",
      "Word:  colored (0.4834195375442505)\n",
      "Word:  distribute (0.4819747507572174)\n",
      "Word:  recruit (0.47825562953948975)\n",
      "Word:  valuable (0.4706496298313141)\n",
      "Word:  correspond (0.4675433337688446)\n",
      "Word:  palace (0.46617424488067627)\n",
      "Word:  fulfill (0.46592724323272705)\n",
      "Word:  access (0.4570824205875397)\n",
      "Word:  contract (0.4546370506286621)\n",
      "Word:  district (0.450908899307251)\n",
      "Word:  settlement (0.4494132995605469)\n",
      "Word:  obtain (0.4489075839519501)\n",
      "Word:  pepper (0.4488717317581177)\n",
      "Word:  brick (0.4471730589866638)\n",
      "Word:  trigger (0.4453243315219879)\n",
      "Word:  artifact (0.4451131224632263)\n",
      "Word:  randomly (0.4437299966812134)\n",
      "Word:  deploy (0.44366034865379333)\n",
      "Word:  boost (0.4424551725387573)\n",
      "Word:  temple (0.4407757818698883)\n",
      "Word:  currency (0.4391680955886841)\n",
      "Word:  harvest (0.43479636311531067)\n",
      "Word:  specialist (0.4337351322174072)\n",
      "Word:  matching (0.4336177706718445)\n",
      "Word:  income (0.4320767819881439)\n",
      "Word:  citizen (0.43129926919937134)\n",
      "Word:  manipulate (0.43062469363212585)\n",
      "Word:  farm (0.42970937490463257)\n",
      "Word:  bonus (0.42960819602012634)\n",
      "Word:  slot (0.4291881024837494)\n",
      "Word:  fulfil (0.42609071731567383)\n",
      "Word:  relic (0.42371273040771484)\n",
      "Word:  population (0.42229220271110535)\n",
      "Word:  hut (0.4178611636161804)\n",
      "Word:  display (0.41775739192962646)\n",
      "Word:  chit (0.4173637628555298)\n",
      "Word:  village (0.41458648443222046)\n",
      "Word:  technology (0.41240787506103516)\n",
      "Word:  initiative (0.41122686862945557)\n",
      "Word:  prestige (0.41041696071624756)\n",
      "Word:  creature (0.4088901877403259)\n",
      "Word:  assistant (0.4066513180732727)\n",
      "Word:  activation (0.4058872163295746)\n",
      "Word:  purple (0.40425723791122437)\n",
      "Word:  sector (0.40385323762893677)\n",
      "Word:  multiply (0.4034004509449005)\n",
      "Word:  select (0.4021145701408386)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  horror (0.5460355877876282)\n",
      "Word:  zombie (0.5164757966995239)\n",
      "Word:  fantasy (0.5063478350639343)\n",
      "Word:  wars (0.505608081817627)\n",
      "Word:  movie (0.49143660068511963)\n",
      "Word:  cooperative (0.4791039228439331)\n",
      "Word:  rpg (0.47437942028045654)\n",
      "Word:  crawler (0.47176894545555115)\n",
      "Word:  marvel (0.46927323937416077)\n",
      "Word:  cthulhu (0.4624696373939514)\n",
      "Word:  crawl (0.4397061765193939)\n",
      "Word:  dungeon (0.4364301562309265)\n",
      "Word:  legendary (0.4290693998336792)\n",
      "Word:  lovecraft (0.4265046715736389)\n",
      "Word:  lcg (0.42109501361846924)\n",
      "Word:  atmosphere (0.42090892791748047)\n",
      "Word:  sci (0.4196625053882599)\n",
      "Word:  live (0.41661423444747925)\n",
      "Word:  setting (0.41197603940963745)\n",
      "Word:  evoke (0.41171932220458984)\n",
      "Word:  medieval (0.41127097606658936)\n",
      "Word:  holmes (0.41080889105796814)\n",
      "Word:  trek (0.41073450446128845)\n",
      "Word:  league (0.4085557460784912)\n",
      "Word:  film (0.4080934524536133)\n",
      "Word:  storytelling (0.4072713851928711)\n",
      "Word:  immersive (0.4050651788711548)\n",
      "Word:  theme (0.4021371006965637)\n",
      "Word:  fi (0.4017249345779419)\n",
      "Word:  universe (0.40019774436950684)\n",
      "Word:  ip (0.39874908328056335)\n",
      "Word:  superhero (0.3937731683254242)\n",
      "Word:  consulting (0.3935677707195282)\n",
      "Word:  realm (0.39296501874923706)\n",
      "Word:  warcraft (0.39224401116371155)\n",
      "Word:  lotr (0.39013606309890747)\n",
      "Word:  detective (0.38636696338653564)\n",
      "Word:  world (0.38218849897384644)\n",
      "Word:  crawling (0.3772236108779907)\n",
      "Word:  thrones (0.37710028886795044)\n",
      "Word:  tv (0.37669944763183594)\n",
      "Word:  tabletop (0.37310338020324707)\n",
      "Word:  adventurer (0.3694581985473633)\n",
      "Word:  roleplay (0.369092732667923)\n",
      "Word:  mouse (0.3669469952583313)\n",
      "Word:  unmatched (0.3657344877719879)\n",
      "Word:  survival (0.3619231581687927)\n",
      "Word:  firefly (0.3590993285179138)\n",
      "Word:  lover (0.3567380905151367)\n",
      "Word:  greek (0.3550490140914917)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  <UNK> (0.7598536610603333)\n",
      "Word:  unlimited (0.5667366981506348)\n",
      "Word:  chest (0.5290654301643372)\n",
      "Word:  resin (0.5253718495368958)\n",
      "Word:  lord (0.5182327032089233)\n",
      "Word:  bomber (0.5154748558998108)\n",
      "Word:  cursed (0.5010157823562622)\n",
      "Word:  aces (0.5003286600112915)\n",
      "Word:  orc (0.5001363158226013)\n",
      "Word:  swarm (0.4988817274570465)\n",
      "Word:  armor (0.4976440966129303)\n",
      "Word:  jurassic (0.4963451027870178)\n",
      "Word:  elemental (0.4949571490287781)\n",
      "Word:  rebel (0.4924240708351135)\n",
      "Word:  cobble (0.49211302399635315)\n",
      "Word:  millenium (0.4901168644428253)\n",
      "Word:  infantry (0.48841094970703125)\n",
      "Word:  rebellion (0.48837825655937195)\n",
      "Word:  phoenix (0.48774898052215576)\n",
      "Word:  shadow (0.4873015880584717)\n",
      "Word:  france (0.48713579773902893)\n",
      "Word:  exp (0.4860648512840271)\n",
      "Word:  etsy (0.486061155796051)\n",
      "Word:  horde (0.4841881990432739)\n",
      "Word:  defender (0.48343944549560547)\n",
      "Word:  raven (0.4820516109466553)\n",
      "Word:  demon (0.4806986451148987)\n",
      "Word:  assault (0.4803898334503174)\n",
      "Word:  troll (0.4803503155708313)\n",
      "Word:  european (0.47767841815948486)\n",
      "Word:  warrior (0.47731533646583557)\n",
      "Word:  silver (0.4766836166381836)\n",
      "Word:  soviet (0.476471871137619)\n",
      "Word:  robinson (0.475991815328598)\n",
      "Word:  crusoe (0.4756932258605957)\n",
      "Word:  raptor (0.47468119859695435)\n",
      "Word:  swamp (0.47401830554008484)\n",
      "Word:  survivor (0.4724369943141937)\n",
      "Word:  cave (0.4722628891468048)\n",
      "Word:  service (0.4720580577850342)\n",
      "Word:  accessory (0.4712730646133423)\n",
      "Word:  fighter (0.4711073637008667)\n",
      "Word:  allies (0.47095412015914917)\n",
      "Word:  navy (0.46959665417671204)\n",
      "Word:  sword (0.4689585566520691)\n",
      "Word:  iron (0.4686341881752014)\n",
      "Word:  invader (0.4682757556438446)\n",
      "Word:  raider (0.4677439332008362)\n",
      "Word:  clash (0.4675232768058777)\n",
      "Word:  orcs (0.46687209606170654)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  economic (0.6252667903900146)\n",
      "Word:  eurogame (0.5270206332206726)\n",
      "Word:  optimization (0.5064324736595154)\n",
      "Word:  salad (0.49397343397140503)\n",
      "Word:  efficiency (0.4785580337047577)\n",
      "Word:  mash (0.4731144607067108)\n",
      "Word:  blocking (0.4727262258529663)\n",
      "Word:  placement (0.46632516384124756)\n",
      "Word:  trading (0.4499293565750122)\n",
      "Word:  rondel (0.44982004165649414)\n",
      "Word:  throat (0.4469524025917053)\n",
      "Word:  crunchy (0.43497002124786377)\n",
      "Word:  competition (0.432964026927948)\n",
      "Word:  tight (0.4281434416770935)\n",
      "Word:  interactive (0.4237213730812073)\n",
      "Word:  spacial (0.41768190264701843)\n",
      "Word:  thinky (0.4116722345352173)\n",
      "Word:  confrontational (0.40677666664123535)\n",
      "Word:  rail (0.4063912630081177)\n",
      "Word:  lightweight (0.4062781035900116)\n",
      "Word:  puzzly (0.4051809012889862)\n",
      "Word:  polyomino (0.4045725464820862)\n",
      "Word:  laying (0.4044824242591858)\n",
      "Word:  euro (0.40010344982147217)\n",
      "Word:  economy (0.39950597286224365)\n",
      "Word:  manipulation (0.39841386675834656)\n",
      "Word:  indirect (0.3969022035598755)\n",
      "Word:  feld (0.3938561677932739)\n",
      "Word:  racing (0.3933746814727783)\n",
      "Word:  euros (0.3930845558643341)\n",
      "Word:  bidding (0.3925500512123108)\n",
      "Word:  programming (0.38926613330841064)\n",
      "Word:  kanban (0.3877640962600708)\n",
      "Word:  mancala (0.38703209161758423)\n",
      "Word:  essence (0.3851582705974579)\n",
      "Word:  spatial (0.38420185446739197)\n",
      "Word:  connection (0.3814801573753357)\n",
      "Word:  medium (0.38113659620285034)\n",
      "Word:  unusual (0.3803664445877075)\n",
      "Word:  auction (0.37616193294525146)\n",
      "Word:  negotiation (0.3743916153907776)\n",
      "Word:  intricate (0.37372520565986633)\n",
      "Word:  blend (0.37090975046157837)\n",
      "Word:  interlock (0.3693241477012634)\n",
      "Word:  worker (0.3668115735054016)\n",
      "Word:  pure (0.36529073119163513)\n",
      "Word:  mish (0.3646309971809387)\n",
      "Word:  simultaneous (0.36373430490493774)\n",
      "Word:  allocation (0.35954129695892334)\n",
      "Word:  majority (0.3575800061225891)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  prone (0.4957444965839386)\n",
      "Word:  left (0.4840671122074127)\n",
      "Word:  screw (0.477093368768692)\n",
      "Word:  unlucky (0.4744111895561218)\n",
      "Word:  eliminate (0.4643353521823883)\n",
      "Word:  accidentally (0.4604427218437195)\n",
      "Word:  somebody (0.45425039529800415)\n",
      "Word:  aware (0.4498399496078491)\n",
      "Word:  lucky (0.44937509298324585)\n",
      "Word:  sit (0.4492788314819336)\n",
      "Word:  mercy (0.4484604001045227)\n",
      "Word:  jack (0.446178138256073)\n",
      "Word:  notice (0.44413670897483826)\n",
      "Word:  correctly (0.44194433093070984)\n",
      "Word:  stuck (0.43752413988113403)\n",
      "Word:  card (0.43655604124069214)\n",
      "Word:  calculate (0.435774564743042)\n",
      "Word:  disadvantage (0.4336271286010742)\n",
      "Word:  everybody (0.4310179352760315)\n",
      "Word:  prevent (0.4307671785354614)\n",
      "Word:  actively (0.42936888337135315)\n",
      "Word:  hose (0.42806053161621094)\n",
      "Word:  person (0.42641210556030273)\n",
      "Word:  paralysis (0.4225422143936157)\n",
      "Word:  abruptly (0.4210556745529175)\n",
      "Word:  occasion (0.4194032549858093)\n",
      "Word:  seriously (0.4191429913043976)\n",
      "Word:  pass (0.4157472848892212)\n",
      "Word:  analysis (0.41484272480010986)\n",
      "Word:  favour (0.4144821763038635)\n",
      "Word:  ask (0.41366565227508545)\n",
      "Word:  guess (0.41092175245285034)\n",
      "Word:  shot (0.41049984097480774)\n",
      "Word:  critical (0.4098847210407257)\n",
      "Word:  upset (0.40882307291030884)\n",
      "Word:  discard (0.4087582528591156)\n",
      "Word:  thwart (0.40811532735824585)\n",
      "Word:  blame (0.4075557589530945)\n",
      "Word:  question (0.40731239318847656)\n",
      "Word:  blind (0.4072890281677246)\n",
      "Word:  reveal (0.4044002890586853)\n",
      "Word:  law (0.4029502868652344)\n",
      "Word:  recover (0.40285688638687134)\n",
      "Word:  shut (0.4022529125213623)\n",
      "Word:  join (0.40206247568130493)\n",
      "Word:  dependent (0.40154898166656494)\n",
      "Word:  bet (0.4011498689651489)\n",
      "Word:  bust (0.4005817174911499)\n",
      "Word:  occasionally (0.4003717303276062)\n",
      "Word:  bind (0.39747369289398193)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  thick (0.534967303276062)\n",
      "Word:  sturdy (0.5176661610603333)\n",
      "Word:  insert (0.5022398233413696)\n",
      "Word:  gorgeous (0.4974846839904785)\n",
      "Word:  notch (0.49725040793418884)\n",
      "Word:  wooden (0.4952154755592346)\n",
      "Word:  presentation (0.4896519184112549)\n",
      "Word:  colorful (0.4895148277282715)\n",
      "Word:  stunning (0.48463916778564453)\n",
      "Word:  cardboard (0.4841320514678955)\n",
      "Word:  graphic (0.47933685779571533)\n",
      "Word:  ugly (0.47472816705703735)\n",
      "Word:  flimsy (0.4735037088394165)\n",
      "Word:  functional (0.4671400785446167)\n",
      "Word:  component (0.4602733254432678)\n",
      "Word:  vibrant (0.46022236347198486)\n",
      "Word:  material (0.45975208282470703)\n",
      "Word:  plastic (0.45518553256988525)\n",
      "Word:  lovely (0.45148277282714844)\n",
      "Word:  artwork (0.44998764991760254)\n",
      "Word:  quality (0.4478461742401123)\n",
      "Word:  detailed (0.4467705488204956)\n",
      "Word:  chunky (0.4461424946784973)\n",
      "Word:  beautiful (0.4397006928920746)\n",
      "Word:  components (0.4358348250389099)\n",
      "Word:  packaging (0.4323672950267792)\n",
      "Word:  art (0.43223175406455994)\n",
      "Word:  outstanding (0.42786887288093567)\n",
      "Word:  miniature (0.4253377914428711)\n",
      "Word:  bright (0.4221676290035248)\n",
      "Word:  illustration (0.4184767007827759)\n",
      "Word:  iconography (0.415225088596344)\n",
      "Word:  sized (0.4144355058670044)\n",
      "Word:  garish (0.4101479649543762)\n",
      "Word:  stellar (0.4098479151725769)\n",
      "Word:  storage (0.4070497155189514)\n",
      "Word:  production (0.40077725052833557)\n",
      "Word:  minis (0.4007149338722229)\n",
      "Word:  deluxe (0.4005807042121887)\n",
      "Word:  illustrate (0.3973994255065918)\n",
      "Word:  cmon (0.39591163396835327)\n",
      "Word:  visually (0.389812171459198)\n",
      "Word:  font (0.38970309495925903)\n",
      "Word:  thin (0.3895830512046814)\n",
      "Word:  paint (0.3894294500350952)\n",
      "Word:  instruction (0.38827210664749146)\n",
      "Word:  detail (0.3841572105884552)\n",
      "Word:  looking (0.3832041025161743)\n",
      "Word:  ffg (0.37998372316360474)\n",
      "Word:  colourful (0.37991440296173096)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  play (0.6474059820175171)\n",
      "Word:  copy (0.3953804075717926)\n",
      "Word:  week (0.3951311409473419)\n",
      "Word:  today (0.38870254158973694)\n",
      "Word:  ago (0.3676486909389496)\n",
      "Word:  month (0.34701022505760193)\n",
      "Word:  request (0.3364871144294739)\n",
      "Word:  buddy (0.3346799612045288)\n",
      "Word:  july (0.32977747917175293)\n",
      "Word:  hr (0.3265051245689392)\n",
      "Word:  revisit (0.32474225759506226)\n",
      "Word:  happy (0.3244546949863434)\n",
      "Word:  wife (0.32394689321517944)\n",
      "Word:  holiday (0.32139939069747925)\n",
      "Word:  anytime (0.3187476098537445)\n",
      "Word:  night (0.3186284601688385)\n",
      "Word:  glad (0.3159107267856598)\n",
      "Word:  bga (0.3135765790939331)\n",
      "Word:  husband (0.3127211332321167)\n",
      "Word:  essen (0.3126731514930725)\n",
      "Word:  daughter (0.31266260147094727)\n",
      "Word:  own (0.31254157423973083)\n",
      "Word:  gift (0.31189650297164917)\n",
      "Word:  weekend (0.31165435910224915)\n",
      "Word:  update (0.31105658411979675)\n",
      "Word:  gladly (0.30952852964401245)\n",
      "Word:  edit (0.30952125787734985)\n",
      "Word:  twice (0.3085136413574219)\n",
      "Word:  april (0.3079841434955597)\n",
      "Word:  year (0.30659183859825134)\n",
      "Word:  girlfriend (0.3064269423484802)\n",
      "Word:  recently (0.30545860528945923)\n",
      "Word:  date (0.3034836947917938)\n",
      "Word:  demo (0.29967600107192993)\n",
      "Word:  june (0.2989828288555145)\n",
      "Word:  sister (0.2971806526184082)\n",
      "Word:  boy (0.296501100063324)\n",
      "Word:  list (0.29599103331565857)\n",
      "Word:  son (0.29519766569137573)\n",
      "Word:  tonight (0.29498210549354553)\n",
      "Word:  club (0.2922767102718353)\n",
      "Word:  currently (0.2911076843738556)\n",
      "Word:  hype (0.28952354192733765)\n",
      "Word:  release (0.2876493036746979)\n",
      "Word:  hobby (0.28473538160324097)\n",
      "Word:  folk (0.2828008234500885)\n",
      "Word:  happily (0.28093886375427246)\n",
      "Word:  ranking (0.28068116307258606)\n",
      "Word:  evening (0.2803072929382324)\n",
      "Word:  caveat (0.2796746790409088)\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/94248 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e45473bdfc6439fa82419edc7797ad7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  416 . The limit is set to 80 tokens.\n",
      "We loose information on 21 points.This is 0.022281639928698752% of the dataset.\n",
      "Padding sequences to length (80).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary<7882 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary<11292 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary<13709 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary<15621 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary<17363 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary<18957 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary<20461 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary<21806 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary<23061 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<23504 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 94248 documents (total 790458 corpus positions)\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Dictionary\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<23504 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 94248 documents (total 790458 corpus positions)\", 'datetime': '2025-02-03T17:49:03.443446', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator<processes=15, batch_size=64> to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:15 batches submitted to accumulate stats from 960 documents (-1026 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:24 batches submitted to accumulate stats from 1536 documents (-1484 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:26 batches submitted to accumulate stats from 1664 documents (-1476 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:33 batches submitted to accumulate stats from 2112 documents (-1814 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:47 batches submitted to accumulate stats from 3008 documents (-2865 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:57 batches submitted to accumulate stats from 3648 documents (-3346 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:84 batches submitted to accumulate stats from 5376 documents (-4888 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:85 batches submitted to accumulate stats from 5440 documents (-4884 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:89 batches submitted to accumulate stats from 5696 documents (-5030 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:98 batches submitted to accumulate stats from 6272 documents (-5546 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:99 batches submitted to accumulate stats from 6336 documents (-5475 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:103 batches submitted to accumulate stats from 6592 documents (-5566 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:107 batches submitted to accumulate stats from 6848 documents (-5722 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:108 batches submitted to accumulate stats from 6912 documents (-5596 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:109 batches submitted to accumulate stats from 6976 documents (-5593 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:112 batches submitted to accumulate stats from 7168 documents (-5755 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:124 batches submitted to accumulate stats from 7936 documents (-6417 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:133 batches submitted to accumulate stats from 8512 documents (-6801 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:139 batches submitted to accumulate stats from 8896 documents (-6947 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:146 batches submitted to accumulate stats from 9344 documents (-7196 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:148 batches submitted to accumulate stats from 9472 documents (-7222 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:152 batches submitted to accumulate stats from 9728 documents (-7332 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:154 batches submitted to accumulate stats from 9856 documents (-7369 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:158 batches submitted to accumulate stats from 10112 documents (-7557 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:167 batches submitted to accumulate stats from 10688 documents (-7920 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:168 batches submitted to accumulate stats from 10752 documents (-7898 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:171 batches submitted to accumulate stats from 10944 documents (-8032 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:174 batches submitted to accumulate stats from 11136 documents (-8038 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:175 batches submitted to accumulate stats from 11200 documents (-8005 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:177 batches submitted to accumulate stats from 11328 documents (-8112 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:178 batches submitted to accumulate stats from 11392 documents (-8082 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:184 batches submitted to accumulate stats from 11776 documents (-8244 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:186 batches submitted to accumulate stats from 11904 documents (-8269 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:188 batches submitted to accumulate stats from 12032 documents (-8271 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:195 batches submitted to accumulate stats from 12480 documents (-8703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:197 batches submitted to accumulate stats from 12608 documents (-8585 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:200 batches submitted to accumulate stats from 12800 documents (-8654 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:205 batches submitted to accumulate stats from 13120 documents (-8830 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:206 batches submitted to accumulate stats from 13184 documents (-8828 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:210 batches submitted to accumulate stats from 13440 documents (-8936 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:214 batches submitted to accumulate stats from 13696 documents (-9062 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:218 batches submitted to accumulate stats from 13952 documents (-9070 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:225 batches submitted to accumulate stats from 14400 documents (-9398 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:230 batches submitted to accumulate stats from 14720 documents (-9540 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:231 batches submitted to accumulate stats from 14784 documents (-9522 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:232 batches submitted to accumulate stats from 14848 documents (-9501 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:251 batches submitted to accumulate stats from 16064 documents (-10462 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:252 batches submitted to accumulate stats from 16128 documents (-10433 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:255 batches submitted to accumulate stats from 16320 documents (-10453 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:257 batches submitted to accumulate stats from 16448 documents (-10524 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:258 batches submitted to accumulate stats from 16512 documents (-10433 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:261 batches submitted to accumulate stats from 16704 documents (-10520 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:266 batches submitted to accumulate stats from 17024 documents (-10728 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:267 batches submitted to accumulate stats from 17088 documents (-10708 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:270 batches submitted to accumulate stats from 17280 documents (-10746 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:273 batches submitted to accumulate stats from 17472 documents (-10826 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:276 batches submitted to accumulate stats from 17664 documents (-10856 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:278 batches submitted to accumulate stats from 17792 documents (-10789 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:279 batches submitted to accumulate stats from 17856 documents (-10788 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:288 batches submitted to accumulate stats from 18432 documents (-11029 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:289 batches submitted to accumulate stats from 18496 documents (-10996 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:296 batches submitted to accumulate stats from 18944 documents (-11229 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:297 batches submitted to accumulate stats from 19008 documents (-11217 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:299 batches submitted to accumulate stats from 19136 documents (-11211 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:302 batches submitted to accumulate stats from 19328 documents (-11240 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:303 batches submitted to accumulate stats from 19392 documents (-11164 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:304 batches submitted to accumulate stats from 19456 documents (-11114 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:305 batches submitted to accumulate stats from 19520 documents (-11101 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:306 batches submitted to accumulate stats from 19584 documents (-10959 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:309 batches submitted to accumulate stats from 19776 documents (-11004 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:311 batches submitted to accumulate stats from 19904 documents (-11025 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:312 batches submitted to accumulate stats from 19968 documents (-10979 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:316 batches submitted to accumulate stats from 20224 documents (-11063 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:325 batches submitted to accumulate stats from 20800 documents (-11518 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:334 batches submitted to accumulate stats from 21376 documents (-11917 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:337 batches submitted to accumulate stats from 21568 documents (-11959 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:354 batches submitted to accumulate stats from 22656 documents (-12714 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:360 batches submitted to accumulate stats from 23040 documents (-12923 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:361 batches submitted to accumulate stats from 23104 documents (-12906 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:362 batches submitted to accumulate stats from 23168 documents (-12818 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:366 batches submitted to accumulate stats from 23424 documents (-12889 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:372 batches submitted to accumulate stats from 23808 documents (-13185 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:375 batches submitted to accumulate stats from 24000 documents (-13345 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:385 batches submitted to accumulate stats from 24640 documents (-13790 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:390 batches submitted to accumulate stats from 24960 documents (-13975 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:398 batches submitted to accumulate stats from 25472 documents (-14574 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:402 batches submitted to accumulate stats from 25728 documents (-14659 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:413 batches submitted to accumulate stats from 26432 documents (-15247 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:428 batches submitted to accumulate stats from 27392 documents (-16168 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:435 batches submitted to accumulate stats from 27840 documents (-16461 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:436 batches submitted to accumulate stats from 27904 documents (-16434 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:437 batches submitted to accumulate stats from 27968 documents (-16407 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:438 batches submitted to accumulate stats from 28032 documents (-16373 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:440 batches submitted to accumulate stats from 28160 documents (-16413 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:441 batches submitted to accumulate stats from 28224 documents (-16391 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:457 batches submitted to accumulate stats from 29248 documents (-17272 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:458 batches submitted to accumulate stats from 29312 documents (-17217 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:459 batches submitted to accumulate stats from 29376 documents (-17196 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:462 batches submitted to accumulate stats from 29568 documents (-17201 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:464 batches submitted to accumulate stats from 29696 documents (-17309 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:476 batches submitted to accumulate stats from 30464 documents (-17957 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:483 batches submitted to accumulate stats from 30912 documents (-18414 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:492 batches submitted to accumulate stats from 31488 documents (-18955 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:496 batches submitted to accumulate stats from 31744 documents (-19069 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:504 batches submitted to accumulate stats from 32256 documents (-19492 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:505 batches submitted to accumulate stats from 32320 documents (-19440 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:519 batches submitted to accumulate stats from 33216 documents (-20190 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:526 batches submitted to accumulate stats from 33664 documents (-20406 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:532 batches submitted to accumulate stats from 34048 documents (-20490 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:537 batches submitted to accumulate stats from 34368 documents (-20691 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:539 batches submitted to accumulate stats from 34496 documents (-20689 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:548 batches submitted to accumulate stats from 35072 documents (-21242 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:554 batches submitted to accumulate stats from 35456 documents (-21517 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:578 batches submitted to accumulate stats from 36992 documents (-22659 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:582 batches submitted to accumulate stats from 37248 documents (-22731 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:583 batches submitted to accumulate stats from 37312 documents (-22550 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:585 batches submitted to accumulate stats from 37440 documents (-22608 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:586 batches submitted to accumulate stats from 37504 documents (-22607 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:594 batches submitted to accumulate stats from 38016 documents (-23115 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:595 batches submitted to accumulate stats from 38080 documents (-23095 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:596 batches submitted to accumulate stats from 38144 documents (-23066 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:598 batches submitted to accumulate stats from 38272 documents (-22930 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:602 batches submitted to accumulate stats from 38528 documents (-23111 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:604 batches submitted to accumulate stats from 38656 documents (-23170 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:622 batches submitted to accumulate stats from 39808 documents (-24276 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:627 batches submitted to accumulate stats from 40128 documents (-24457 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:632 batches submitted to accumulate stats from 40448 documents (-24577 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:637 batches submitted to accumulate stats from 40768 documents (-24754 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:638 batches submitted to accumulate stats from 40832 documents (-24703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:640 batches submitted to accumulate stats from 40960 documents (-24703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:642 batches submitted to accumulate stats from 41088 documents (-24758 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:648 batches submitted to accumulate stats from 41472 documents (-25142 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:657 batches submitted to accumulate stats from 42048 documents (-25673 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:659 batches submitted to accumulate stats from 42176 documents (-25746 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:661 batches submitted to accumulate stats from 42304 documents (-25772 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:662 batches submitted to accumulate stats from 42368 documents (-25668 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:667 batches submitted to accumulate stats from 42688 documents (-25749 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:672 batches submitted to accumulate stats from 43008 documents (-25925 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:673 batches submitted to accumulate stats from 43072 documents (-25897 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:675 batches submitted to accumulate stats from 43200 documents (-25926 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:691 batches submitted to accumulate stats from 44224 documents (-27058 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:692 batches submitted to accumulate stats from 44288 documents (-26963 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:695 batches submitted to accumulate stats from 44480 documents (-27142 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:696 batches submitted to accumulate stats from 44544 documents (-27063 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:698 batches submitted to accumulate stats from 44672 documents (-27115 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:702 batches submitted to accumulate stats from 44928 documents (-27374 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:704 batches submitted to accumulate stats from 45056 documents (-27326 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:710 batches submitted to accumulate stats from 45440 documents (-27619 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:712 batches submitted to accumulate stats from 45568 documents (-27688 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:724 batches submitted to accumulate stats from 46336 documents (-28461 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:729 batches submitted to accumulate stats from 46656 documents (-28685 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:730 batches submitted to accumulate stats from 46720 documents (-28630 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:731 batches submitted to accumulate stats from 46784 documents (-28591 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:732 batches submitted to accumulate stats from 46848 documents (-28478 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:743 batches submitted to accumulate stats from 47552 documents (-29131 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:744 batches submitted to accumulate stats from 47616 documents (-29109 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:745 batches submitted to accumulate stats from 47680 documents (-29094 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:748 batches submitted to accumulate stats from 47872 documents (-29183 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:749 batches submitted to accumulate stats from 47936 documents (-29121 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:768 batches submitted to accumulate stats from 49152 documents (-30537 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:770 batches submitted to accumulate stats from 49280 documents (-30560 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:778 batches submitted to accumulate stats from 49792 documents (-31007 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:779 batches submitted to accumulate stats from 49856 documents (-30674 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:781 batches submitted to accumulate stats from 49984 documents (-30779 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:785 batches submitted to accumulate stats from 50240 documents (-31052 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:788 batches submitted to accumulate stats from 50432 documents (-31176 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:789 batches submitted to accumulate stats from 50496 documents (-31057 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:798 batches submitted to accumulate stats from 51072 documents (-31277 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:799 batches submitted to accumulate stats from 51136 documents (-31250 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:805 batches submitted to accumulate stats from 51520 documents (-31507 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:816 batches submitted to accumulate stats from 52224 documents (-31894 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:820 batches submitted to accumulate stats from 52480 documents (-32082 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:836 batches submitted to accumulate stats from 53504 documents (-32912 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:842 batches submitted to accumulate stats from 53888 documents (-33231 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:853 batches submitted to accumulate stats from 54592 documents (-33866 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:856 batches submitted to accumulate stats from 54784 documents (-33747 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:857 batches submitted to accumulate stats from 54848 documents (-33745 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:858 batches submitted to accumulate stats from 54912 documents (-33680 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:859 batches submitted to accumulate stats from 54976 documents (-33636 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:875 batches submitted to accumulate stats from 56000 documents (-34610 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:877 batches submitted to accumulate stats from 56128 documents (-34567 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:880 batches submitted to accumulate stats from 56320 documents (-34445 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:882 batches submitted to accumulate stats from 56448 documents (-34307 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:883 batches submitted to accumulate stats from 56512 documents (-34286 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:884 batches submitted to accumulate stats from 56576 documents (-34266 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:885 batches submitted to accumulate stats from 56640 documents (-34184 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:886 batches submitted to accumulate stats from 56704 documents (-34143 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:888 batches submitted to accumulate stats from 56832 documents (-34206 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:890 batches submitted to accumulate stats from 56960 documents (-34207 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:892 batches submitted to accumulate stats from 57088 documents (-34232 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:896 batches submitted to accumulate stats from 57344 documents (-34249 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:897 batches submitted to accumulate stats from 57408 documents (-34176 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:904 batches submitted to accumulate stats from 57856 documents (-34533 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:909 batches submitted to accumulate stats from 58176 documents (-34715 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:916 batches submitted to accumulate stats from 58624 documents (-35068 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:922 batches submitted to accumulate stats from 59008 documents (-35284 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:931 batches submitted to accumulate stats from 59584 documents (-35667 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:933 batches submitted to accumulate stats from 59712 documents (-35597 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:936 batches submitted to accumulate stats from 59904 documents (-35716 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:952 batches submitted to accumulate stats from 60928 documents (-36271 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:955 batches submitted to accumulate stats from 61120 documents (-36367 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:960 batches submitted to accumulate stats from 61440 documents (-36494 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:962 batches submitted to accumulate stats from 61568 documents (-36524 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:968 batches submitted to accumulate stats from 61952 documents (-36767 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:975 batches submitted to accumulate stats from 62400 documents (-37138 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:981 batches submitted to accumulate stats from 62784 documents (-37354 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:993 batches submitted to accumulate stats from 63552 documents (-38153 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:994 batches submitted to accumulate stats from 63616 documents (-38146 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1003 batches submitted to accumulate stats from 64192 documents (-38450 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1019 batches submitted to accumulate stats from 65216 documents (-39530 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1020 batches submitted to accumulate stats from 65280 documents (-39516 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1021 batches submitted to accumulate stats from 65344 documents (-39489 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1023 batches submitted to accumulate stats from 65472 documents (-39304 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1030 batches submitted to accumulate stats from 65920 documents (-39696 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1031 batches submitted to accumulate stats from 65984 documents (-39655 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1032 batches submitted to accumulate stats from 66048 documents (-39640 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1035 batches submitted to accumulate stats from 66240 documents (-39719 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1040 batches submitted to accumulate stats from 66560 documents (-39984 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1048 batches submitted to accumulate stats from 67072 documents (-40559 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1051 batches submitted to accumulate stats from 67264 documents (-40581 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1052 batches submitted to accumulate stats from 67328 documents (-40545 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1064 batches submitted to accumulate stats from 68096 documents (-41423 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1072 batches submitted to accumulate stats from 68608 documents (-42032 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1080 batches submitted to accumulate stats from 69120 documents (-42451 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1084 batches submitted to accumulate stats from 69376 documents (-42506 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1087 batches submitted to accumulate stats from 69568 documents (-42562 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1094 batches submitted to accumulate stats from 70016 documents (-42806 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1095 batches submitted to accumulate stats from 70080 documents (-42782 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1103 batches submitted to accumulate stats from 70592 documents (-43065 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1106 batches submitted to accumulate stats from 70784 documents (-43110 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1108 batches submitted to accumulate stats from 70912 documents (-43120 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1110 batches submitted to accumulate stats from 71040 documents (-43182 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1114 batches submitted to accumulate stats from 71296 documents (-43455 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1117 batches submitted to accumulate stats from 71488 documents (-43597 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1135 batches submitted to accumulate stats from 72640 documents (-44794 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1136 batches submitted to accumulate stats from 72704 documents (-44790 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1142 batches submitted to accumulate stats from 73088 documents (-45135 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1152 batches submitted to accumulate stats from 73728 documents (-45741 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1166 batches submitted to accumulate stats from 74624 documents (-46295 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1167 batches submitted to accumulate stats from 74688 documents (-46200 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1168 batches submitted to accumulate stats from 74752 documents (-46118 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1169 batches submitted to accumulate stats from 74816 documents (-46067 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1171 batches submitted to accumulate stats from 74944 documents (-46158 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1172 batches submitted to accumulate stats from 75008 documents (-46018 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1181 batches submitted to accumulate stats from 75584 documents (-46323 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1182 batches submitted to accumulate stats from 75648 documents (-46186 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1183 batches submitted to accumulate stats from 75712 documents (-46126 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1184 batches submitted to accumulate stats from 75776 documents (-46073 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1187 batches submitted to accumulate stats from 75968 documents (-46041 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1188 batches submitted to accumulate stats from 76032 documents (-46002 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1189 batches submitted to accumulate stats from 76096 documents (-45972 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1190 batches submitted to accumulate stats from 76160 documents (-45961 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1195 batches submitted to accumulate stats from 76480 documents (-46201 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1196 batches submitted to accumulate stats from 76544 documents (-46161 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1197 batches submitted to accumulate stats from 76608 documents (-46156 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1210 batches submitted to accumulate stats from 77440 documents (-46932 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1211 batches submitted to accumulate stats from 77504 documents (-46913 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1214 batches submitted to accumulate stats from 77696 documents (-47056 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1215 batches submitted to accumulate stats from 77760 documents (-47011 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1218 batches submitted to accumulate stats from 77952 documents (-47144 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1221 batches submitted to accumulate stats from 78144 documents (-47265 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1225 batches submitted to accumulate stats from 78400 documents (-47361 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1230 batches submitted to accumulate stats from 78720 documents (-47408 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1239 batches submitted to accumulate stats from 79296 documents (-47980 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1242 batches submitted to accumulate stats from 79488 documents (-48035 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1243 batches submitted to accumulate stats from 79552 documents (-48018 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1246 batches submitted to accumulate stats from 79744 documents (-48104 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1247 batches submitted to accumulate stats from 79808 documents (-48012 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1248 batches submitted to accumulate stats from 79872 documents (-47714 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1250 batches submitted to accumulate stats from 80000 documents (-47760 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1257 batches submitted to accumulate stats from 80448 documents (-48188 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1259 batches submitted to accumulate stats from 80576 documents (-48278 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1268 batches submitted to accumulate stats from 81152 documents (-48758 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1272 batches submitted to accumulate stats from 81408 documents (-48967 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1286 batches submitted to accumulate stats from 82304 documents (-49457 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1293 batches submitted to accumulate stats from 82752 documents (-49769 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1301 batches submitted to accumulate stats from 83264 documents (-50211 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1316 batches submitted to accumulate stats from 84224 documents (-51035 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1318 batches submitted to accumulate stats from 84352 documents (-51125 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1320 batches submitted to accumulate stats from 84480 documents (-51149 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1321 batches submitted to accumulate stats from 84544 documents (-51045 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1322 batches submitted to accumulate stats from 84608 documents (-51014 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1333 batches submitted to accumulate stats from 85312 documents (-51721 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1335 batches submitted to accumulate stats from 85440 documents (-51821 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1337 batches submitted to accumulate stats from 85568 documents (-51887 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1340 batches submitted to accumulate stats from 85760 documents (-52053 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1346 batches submitted to accumulate stats from 86144 documents (-52387 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1372 batches submitted to accumulate stats from 87808 documents (-54052 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1377 batches submitted to accumulate stats from 88128 documents (-54210 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1386 batches submitted to accumulate stats from 88704 documents (-54609 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1397 batches submitted to accumulate stats from 89408 documents (-55073 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1399 batches submitted to accumulate stats from 89536 documents (-55043 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1412 batches submitted to accumulate stats from 90368 documents (-55686 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1419 batches submitted to accumulate stats from 90816 documents (-55995 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1423 batches submitted to accumulate stats from 91072 documents (-56103 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1429 batches submitted to accumulate stats from 91456 documents (-56375 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1430 batches submitted to accumulate stats from 91520 documents (-56374 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1440 batches submitted to accumulate stats from 92160 documents (-56954 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1449 batches submitted to accumulate stats from 92736 documents (-57493 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1450 batches submitted to accumulate stats from 92800 documents (-57423 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1451 batches submitted to accumulate stats from 92864 documents (-57307 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1453 batches submitted to accumulate stats from 92992 documents (-57254 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1454 batches submitted to accumulate stats from 93056 documents (-57208 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1461 batches submitted to accumulate stats from 93504 documents (-57486 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1462 batches submitted to accumulate stats from 93568 documents (-57468 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1464 batches submitted to accumulate stats from 93696 documents (-57482 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1465 batches submitted to accumulate stats from 93760 documents (-57429 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1466 batches submitted to accumulate stats from 93824 documents (-57330 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:15 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 196893 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.09092904916702085"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:49:30.772494Z",
     "start_time": "2025-02-03T16:49:30.763924Z"
    }
   },
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "65e0b816a06ee9b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.26420369098348445,\n",
       " -0.14356760386375775,\n",
       " -0.03789819035103982,\n",
       " -0.2244762502671176,\n",
       " 0.27837966425198574,\n",
       " -0.19413690041888643,\n",
       " -0.2332492330429037,\n",
       " -0.29038464030316125,\n",
       " 0.13041520886622168,\n",
       " 0.06983114444193494]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:49:32.195833Z",
     "start_time": "2025-02-03T16:49:30.892418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.evaluation import coherence_per_aspect\n",
    "\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]\n",
    "c, m = coherence_per_aspect(aspect_words, ds.text_ds, 3)"
   ],
   "id": "a1b8c8af12ad8460",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 94000 documents\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:49:59.940487Z",
     "start_time": "2025-02-03T16:49:59.935623Z"
    }
   },
   "cell_type": "code",
   "source": "c",
   "id": "8ead46ac716e85b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-15.415429966180449,\n",
       " -8.324924786749554,\n",
       " -11.348559564460805,\n",
       " -9.810242581716443,\n",
       " -3.3119131673751063,\n",
       " -14.003766614120599,\n",
       " -15.635291474652163,\n",
       " -15.271609512678603,\n",
       " -3.005173628424085,\n",
       " -5.745478321201425]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:50:02.076056Z",
     "start_time": "2025-02-03T16:50:02.071495Z"
    }
   },
   "cell_type": "code",
   "source": "m.get_coherence()",
   "id": "569dd10217a1e3a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.187238961755924"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"hands_on\")\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=3) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "13e9619f5dead821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! <br>\n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The main goal of ABAE is to extract interpretable and meaningful aspects, which makes coherence the more aligned metric.<br> Reconstruction error might help guide training but doesn’t guarantee that the extracted aspects are semantically useful.",
   "id": "cd4dfe8c62445d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper, HyperparameterTuningManager\n",
    "\n",
    "configurations = 4  # We try 15 different configurations\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "print(f\"Starting procedure. We try a total of {configurations}\")\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "hp_tuning_manager = HyperparameterTuningManager(hp_wrapper, corpus_file, \"./output\")"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hp_tuning_manager(different_configurations=configurations, repeat=3) # todo tune not on metric but on loss. We want the best reconstruciton model.\n",
    "# Bottom line: Prioritize classification performance, but monitor coherence if it affects usability.\n",
    "# The output of this cell is very long therefore I deleted it"
   ],
   "id": "8b31050bf118ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    columns=[\n",
    "        'name', 'mean_loss', 'mean_coherence', 'aspect_size', 'embedding_size', 'epochs', 'batch_size', 'learning_rate',\n",
    "        'decay_rate', 'momentum', 'negative_sample_size'\n",
    "    ]\n",
    ")\n",
    "\n",
    "i = 0\n",
    "# Inspect results we stored for process. todo: Quando finisci salvalo come file da \"tenere\" in repo cosi che chi controlla lo ha gia.\n",
    "for element in Path(\"./output\").iterdir():\n",
    "\n",
    "    # Search only for the tuning process runs.\n",
    "    if element.is_dir() and element.name.startswith(\"tuning\"):\n",
    "        run_result = json.load(open(str(element) + \"/run_results.json\"))\n",
    "        # We want the best configurations.\n",
    "\n",
    "        obj = {\n",
    "            'name': element.name,\n",
    "            'mean_loss': np.mean(run_result[\"evaluation_loss\"]),\n",
    "            'mean_coherence': np.mean(run_result[\"coherence\"])\n",
    "        }\n",
    "        temp = pd.DataFrame(run_result['params'] | obj, index=[i])\n",
    "        dataframe = pd.concat([temp, dataframe])\n",
    "        i += 1"
   ],
   "id": "6f11e2391039f8fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds.text_ds.apply(lambda x: x.split(' '))",
   "id": "83f0afe3f02329d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, normalize, get_aspect_top_k_words\n",
    "import json\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "model_name = 'tuning_7128da5c-04c0-4e91-abf2-462f9646b48d'\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "parameters = json.load(open(f\"./output/{model_name}/run_results.json\"))['params']\n",
    "config = ABAEModelConfiguration(corpus_file=c_file, model_name=model_name, **parameters)\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "\n",
    "# word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "# aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=20) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=10)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "804ef5608c713f42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manager.embedding_model.model.wv",
   "id": "6f3bd8c300970a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(ds)",
   "id": "49c5e904ed44fc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:51:36.371499Z",
     "start_time": "2025-02-03T16:51:36.358525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from core.dataset import SimpleWord2VecEmbeddingsDataset\n",
    "\n",
    "iteration_model = manager.get_inference_model()\n",
    "custom_review = [\n",
    "    'game hard fun', # 0\n",
    "    'brother forever decide',\n",
    "    'zero luck all skill',\n",
    "    'lot downtime', # 3\n",
    "    'i have banana',\n",
    "    'plastic inserts',\n",
    "    'complex game rules' # 6\n",
    "]\n",
    "# todo rebuild a prediciton model senza negative predict. Devo ristrutturare tutto1\n",
    "ds = SimpleWord2VecEmbeddingsDataset(custom_review, manager.embedding_model.model)\n",
    "\n",
    "res = iteration_model.predict(DataLoader(ds, batch_size=32))"
   ],
   "id": "2a96d8734cbb8224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:51:45.934471Z",
     "start_time": "2025-02-03T16:51:45.929614Z"
    }
   },
   "cell_type": "code",
   "source": "res[0]",
   "id": "4d41a4a20d0ec5fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.6263971e-01, 1.6070150e-01, 6.7665911e-01],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 3.7546426e-01, 2.5973019e-01, 3.6480579e-01],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        9.6259519e-02, 9.6259445e-02, 7.1114939e-01, 9.6332021e-02],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.2061855e-01, 8.7938166e-01],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 4.6828935e-01, 6.3421622e-02, 4.6828935e-01],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 4.9606889e-01, 5.0393128e-01],\n",
       "       [1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 1.0000000e-07, 1.0000000e-07, 1.0000000e-07,\n",
       "        1.0000000e-07, 2.1877122e-01, 9.3999110e-02, 6.8722999e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T16:51:37.379540Z",
     "start_time": "2025-02-03T16:51:37.375303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Il modello fa schifo non serve a niente.\n",
    "res[1]  # Riddle correctly yields a classification to the aspect for representative."
   ],
   "id": "70e3c77deb50ee48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0817567 , 0.1313807 , 0.05315639, 0.08426162, 0.12962303,\n",
       "        0.08049816, 0.11351769, 0.10415612, 0.10272776, 0.11892185],\n",
       "       [0.10805491, 0.07200906, 0.09453166, 0.10224397, 0.09893427,\n",
       "        0.07725929, 0.08951658, 0.13686667, 0.08059887, 0.13998476],\n",
       "       [0.08384306, 0.09782003, 0.10227967, 0.09017856, 0.08234558,\n",
       "        0.1649447 , 0.09660699, 0.11256695, 0.08123559, 0.08817887],\n",
       "       [0.10481264, 0.15214735, 0.0577409 , 0.08501475, 0.05284812,\n",
       "        0.07585361, 0.12177281, 0.13394837, 0.10639574, 0.10946572],\n",
       "       [0.07773   , 0.08794104, 0.12174451, 0.08094119, 0.07236784,\n",
       "        0.1945742 , 0.09200998, 0.10570648, 0.07923861, 0.08774615],\n",
       "       [0.08017664, 0.06221417, 0.12141559, 0.10387982, 0.0707244 ,\n",
       "        0.1540822 , 0.08008087, 0.09180547, 0.1505977 , 0.08502316],\n",
       "       [0.11342146, 0.12564196, 0.09231173, 0.07948986, 0.08358911,\n",
       "        0.09293932, 0.08486223, 0.08784374, 0.13912956, 0.10077106]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "64b81ef255245128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "len(ds.dataset.to_list())\n",
    "list(filter(lambda x: x.any() is None or len(x) < 1, ds.dataset.to_list()))"
   ],
   "id": "b3d2de1dcde3bd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Focus on learning rate",
   "id": "cf07a77d637e025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We fix other params and now focus entirely on lr.\n",
    "# We have already a \"promising\" range defined.\n",
    "# We look in that space so we redefine lr on ABAERandomHyperparametersSelectionWrapper"
   ],
   "id": "c6f003172b40c1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See if the Hp tuning really improved upon our results:\n",
    "We used SGD anda learned its parameters under the assumption that we would do better. <br>\n",
    "Let's see if it really is the case, or we just wasted time.\n",
    "\n",
    "For comparison we use Adam that has the advantage of being robust enough without parameter scouting."
   ],
   "id": "9b8089270d722ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#todo",
   "id": "84bcef4b87f44595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test accuracy on small test sample we filled out\n",
    "\n",
    "### Test set definition"
   ],
   "id": "fcf1124990b1034f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "# We take around 1k records that were not seen yet from the model and label them by hand.\n",
    "dataset = pd.read_csv(\"../data/corpus.csv\")\n",
    "# Handle game names. Or not? I don't need the full pipeline I guess. todo\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()\n",
    "\n",
    "pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")\n",
    "\n",
    "# Extract 1k from dataset that are not in 200k\n",
    "train_ds = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "\n",
    "# Take top 2k. (We will select some good ones and reduce the number to 1k)\n",
    "test_set = dataset[~dataset[\"comments\"].isin(train_ds[\"comments\"])]"
   ],
   "id": "ae3809bf35d257e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to use labels:",
   "id": "f792cdc1d37508dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = {\n",
    "    '0': \"Luck/Alea\",\n",
    "    '1': 'Bookkeeping',\n",
    "    '2': 'Downtime',\n",
    "    '3': 'Interaction',\n",
    "    '4': 'Bash',\n",
    "    '5': 'Complicated/Complex',  # I could watch weight to see if there is a ratio relation.\n",
    "    '6': 'Misc'\n",
    "}"
   ],
   "id": "b87ccaeccafba997",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
