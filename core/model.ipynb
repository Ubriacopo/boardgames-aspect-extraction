{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:29.978561Z",
     "start_time": "2025-02-12T18:50:29.973277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#https://github.com/alexeyev/abae-pytorch \n",
    "\n",
    "# TODO pulisci questo file\n",
    "# todo  prova con is_alpha in preprocessing e buttiamo via il resto (con sequenze piu lunghe)!\n",
    "#  --> leggi top 1000 di restaruant e vedi split con '' quanto lunghe in media\n",
    "# todo fix regularization sia su matrix che generation di aspects / emb\n",
    "\n",
    "# https://arxiv.org/pdf/1803.09820\n",
    "# https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/"
   ],
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:29.983897Z",
     "start_time": "2025-02-12T18:50:29.978561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Una considerazione: Potrei sfruttare gli aspetti inferiti da quello che ho per filtrare via dal testo documenti che parlano di \"Piattaforme\", \"Kickstarter\", \"Materiali\" visto che queste le becca con facilita.\n",
    "# Il mio dataset é probabilmente troppo vario al momento. Un filtraggio di questo tipo faciliterebbe l'apprendimento.\n",
    "\n",
    "# Filtro a mano i commenti scaricati per togliere quelli poco significativi? Riduco rumore!"
   ],
   "id": "49f165d09275bde5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset. <br>\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] <br>\n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors <br>\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:30.161592Z",
     "start_time": "2025-02-12T18:50:30.159110Z"
    }
   },
   "cell_type": "code",
   "source": "# Choice of max_seq size is relative to the ds. 95th percentile (at max 5% loss of information)",
   "id": "377fe340c9c009f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.008273Z",
     "start_time": "2025-02-12T18:50:30.167052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PandasPositiveNegativeNumericTextDataset\n",
    "from core.train import ABAEModelConfiguration, ABAEModelManager\n",
    "import pandas as pd\n",
    "\n",
    "corpus = \"../output/dataset/default-pre-processed/80k.preprocessed.csv\"\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"ff\")\n",
    "config.epochs = 7  # Just one epoch\n",
    "config.max_sequence_length = 40\n",
    "config.aspect_size = 16\n",
    "config.negative_sample_size = 20\n",
    "print(f\"Running on default config:\\n {config}\")\n",
    "\n",
    "# Without any hp tuning we just try and see how it goes.\n",
    "manager = ABAEModelManager(config)\n",
    "train_dataset = PandasPositiveNegativeNumericTextDataset(\n",
    "    dataframe=pd.read_csv(corpus),\n",
    "    vocabulary=manager.embedding_model.vocabulary(),\n",
    "    negative_size=config.negative_sample_size,\n",
    "    max_seq_length=config.max_sequence_length\n",
    ")"
   ],
   "id": "830abaf1d37c6bb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on default config:\n",
      " ABAEModelConfiguration(corpus_file='../output/dataset/default-pre-processed/80k.preprocessed.csv', model_name='ff', aspect_size=16, max_vocab_size=None, embedding_size=200, learning_rate=0.01, decay_rate=0.95, momentum=0.9, max_sequence_length=40, negative_sample_size=20, batch_size=64, epochs=7, output_path='./output')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning on default config:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Without any hp tuning we just try and see how it goes.\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m manager \u001B[38;5;241m=\u001B[39m \u001B[43mABAEModelManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m PandasPositiveNegativeNumericTextDataset(\n\u001B[0;32m     16\u001B[0m     dataframe\u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mread_csv(corpus),\n\u001B[0;32m     17\u001B[0m     vocabulary\u001B[38;5;241m=\u001B[39mmanager\u001B[38;5;241m.\u001B[39membedding_model\u001B[38;5;241m.\u001B[39mvocabulary(),\n\u001B[0;32m     18\u001B[0m     negative_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnegative_sample_size,\n\u001B[0;32m     19\u001B[0m     max_seq_length\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_sequence_length\n\u001B[0;32m     20\u001B[0m )\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\train.py:69\u001B[0m, in \u001B[0;36mABAEModelManager.__init__\u001B[1;34m(self, config, override_existing)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_t_model: keras\u001B[38;5;241m.\u001B[39mModel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ev_model: keras\u001B[38;5;241m.\u001B[39mModel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverride_existing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverride_existing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\train.py:82\u001B[0m, in \u001B[0;36mABAEModelManager.load_embeddings\u001B[1;34m(self, corpus_file, persist, override_existing)\u001B[0m\n\u001B[0;32m     79\u001B[0m keep \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m override_existing\n\u001B[0;32m     81\u001B[0m load_utility \u001B[38;5;241m=\u001B[39m LoadCorpusUtility(column_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomments\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 82\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[43mload_utility\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_model \u001B[38;5;241m=\u001B[39m WordEmbedding(\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39membedding_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_path, max_vocab_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_vocab_size\n\u001B[0;32m     86\u001B[0m )\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_model\u001B[38;5;241m.\u001B[39mgenerate(corpus\u001B[38;5;241m=\u001B[39mcorpus, sg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, persist\u001B[38;5;241m=\u001B[39mpersist, load_existing\u001B[38;5;241m=\u001B[39mkeep)\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\core\\utils.py:52\u001B[0m, in \u001B[0;36mLoadCorpusUtility.load_data\u001B[1;34m(self, data_file_path)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_file_path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m---> 52\u001B[0m     corpus \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_file_path\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumn_name]\n\u001B[0;32m     54\u001B[0m     word_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[0;32m     55\u001B[0m     lines \u001B[38;5;241m=\u001B[39m corpus\u001B[38;5;241m.\u001B[39mswifter\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_tokenization(x, word_count))\u001B[38;5;241m.\u001B[39mdropna()\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mD:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2053\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m<frozen codecs>:331\u001B[0m, in \u001B[0;36mgetstate\u001B[1;34m(self)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manager.run_train_process(train_dataset, optimizer='adam')",
   "id": "d3278a09ace24e58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1473/1473\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m228s\u001B[0m 154ms/step - loss: 8.6859 - max_margin_loss: 8.6790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.src.callbacks.history.History at 0x29e671cf350>,\n",
       " <Functional name=functional_1, built=True>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.008273400Z",
     "start_time": "2025-02-08T14:46:37.245800Z"
    }
   },
   "cell_type": "code",
   "source": "iteration_model = manager.get_training_model()",
   "id": "c4e25c02eab60eff",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.008273400Z",
     "start_time": "2025-02-08T14:46:37.871797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "from keras import ops as K\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embeddings').weights[0].value.data\n",
    "aspect_embeddings = iteration_model.get_layer('aspect_embeddings').w\n",
    "\n",
    "word_emb = torch.nn.functional.normalize(word_emb, p=2, dim=-1)\n",
    "aspect_embeddings = torch.nn.functional.normalize(aspect_embeddings, p=2, dim=-1)\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=50) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(corpus, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "db228e03f66dec96",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\evaluation.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  similarity = word_embeddings @ aspect.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  rule (0.6814370155334473)\n",
      "Word:  rulebook (0.6234116554260254)\n",
      "Word:  manual (0.6020396947860718)\n",
      "Word:  reference (0.5943478345870972)\n",
      "Word:  instruction (0.5853652358055115)\n",
      "Word:  faq (0.5512462854385376)\n",
      "Word:  forum (0.5474805235862732)\n",
      "Word:  page (0.5156159996986389)\n",
      "Word:  guide (0.5124691128730774)\n",
      "Word:  clarification (0.5111934542655945)\n",
      "Word:  paragraph (0.48346424102783203)\n",
      "Word:  answer (0.4652719795703888)\n",
      "Word:  booklet (0.4625762701034546)\n",
      "Word:  ambiguous (0.4545668065547943)\n",
      "Word:  errata (0.45331618189811707)\n",
      "Word:  aid (0.44917163252830505)\n",
      "Word:  file (0.4448372721672058)\n",
      "Word:  poorly (0.443092405796051)\n",
      "Word:  translation (0.4428563714027405)\n",
      "Word:  refer (0.44281649589538574)\n",
      "Word:  download (0.4397127330303192)\n",
      "Word:  helpful (0.43926429748535156)\n",
      "Word:  sheet (0.4332764744758606)\n",
      "Word:  book (0.4292224049568176)\n",
      "Word:  iconography (0.42272233963012695)\n",
      "Word:  confusing (0.4203360974788666)\n",
      "Word:  tutorial (0.416186660528183)\n",
      "Word:  clearly (0.4048735499382019)\n",
      "Word:  code (0.4021674394607544)\n",
      "Word:  language (0.3994157612323761)\n",
      "Word:  unclear (0.3969077467918396)\n",
      "Word:  consult (0.3958933353424072)\n",
      "Word:  detail (0.3949538469314575)\n",
      "Word:  text (0.39102715253829956)\n",
      "Word:  solution (0.3876625597476959)\n",
      "Word:  font (0.3828662633895874)\n",
      "Word:  ambiguity (0.3782411217689514)\n",
      "Word:  read (0.37363195419311523)\n",
      "Word:  error (0.3706497848033905)\n",
      "Word:  interpret (0.3682147264480591)\n",
      "Word:  address (0.3677998185157776)\n",
      "Word:  section (0.3671278953552246)\n",
      "Word:  keyword (0.3635561764240265)\n",
      "Word:  vague (0.36294025182724)\n",
      "Word:  explain (0.35902708768844604)\n",
      "Word:  sk (0.3539882302284241)\n",
      "Word:  reading (0.3516422510147095)\n",
      "Word:  rules (0.34920650720596313)\n",
      "Word:  clarify (0.3480646312236786)\n",
      "Word:  mistake (0.34176838397979736)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  <game_name> (0.7266423106193542)\n",
      "Word:  gearloc (0.374199777841568)\n",
      "Word:  promo (0.3710631728172302)\n",
      "Word:  de (0.3541155457496643)\n",
      "Word:  la (0.34606799483299255)\n",
      "Word:  promos (0.328132688999176)\n",
      "Word:  elves (0.31727445125579834)\n",
      "Word:  flames (0.30839452147483826)\n",
      "Word:  exp (0.30526208877563477)\n",
      "Word:  west (0.30038389563560486)\n",
      "Word:  hill (0.29837697744369507)\n",
      "Word:  summoner (0.29509806632995605)\n",
      "Word:  burgundy (0.2889437675476074)\n",
      "Word:  ii (0.28723323345184326)\n",
      "Word:  wing (0.28487393260002136)\n",
      "Word:  american (0.28441929817199707)\n",
      "Word:  v (0.2841275930404663)\n",
      "Word:  url (0.28017473220825195)\n",
      "Word:  south (0.2793765962123871)\n",
      "Word:  usa (0.2788081765174866)\n",
      "Word:  north (0.2743127942085266)\n",
      "Word:  elf (0.27401483058929443)\n",
      "Word:  orcs (0.27071893215179443)\n",
      "Word:  shadow (0.2705414295196533)\n",
      "Word:  dwarf (0.2680237889289856)\n",
      "Word:  sleeves (0.2668243944644928)\n",
      "Word:  cave (0.264818012714386)\n",
      "Word:  tundra (0.26472800970077515)\n",
      "Word:  pacific (0.2638534605503082)\n",
      "Word:  wars (0.2602577209472656)\n",
      "Word:  phoenix (0.2592226266860962)\n",
      "Word:  kingdom (0.25854212045669556)\n",
      "Word:  resistance (0.25796520709991455)\n",
      "Word:  crossover (0.25598275661468506)\n",
      "Word:  w (0.2537292540073395)\n",
      "Word:  mexico (0.25135692954063416)\n",
      "Word:  expansions (0.2508222758769989)\n",
      "Word:  aces (0.24922329187393188)\n",
      "Word:  t (0.24854204058647156)\n",
      "Word:  traded (0.2479804903268814)\n",
      "Word:  premium (0.24567407369613647)\n",
      "Word:  flight (0.2455458790063858)\n",
      "Word:  original (0.24527063965797424)\n",
      "Word:  miniatures (0.24388203024864197)\n",
      "Word:  century (0.2405053675174713)\n",
      "Word:  cards (0.23808595538139343)\n",
      "Word:  cc (0.23691926896572113)\n",
      "Word:  season (0.23684704303741455)\n",
      "Word:  imperium (0.23382040858268738)\n",
      "Word:  crusoe (0.23346872627735138)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  competition (0.6048587560653687)\n",
      "Word:  economic (0.5846028327941895)\n",
      "Word:  blocking (0.554926872253418)\n",
      "Word:  political (0.5500199794769287)\n",
      "Word:  interactive (0.5292741060256958)\n",
      "Word:  conflict (0.5183952450752258)\n",
      "Word:  warfare (0.5132970809936523)\n",
      "Word:  spatial (0.5039563179016113)\n",
      "Word:  asymmetry (0.4962805509567261)\n",
      "Word:  emergent (0.49487289786338806)\n",
      "Word:  throat (0.49227163195610046)\n",
      "Word:  spacial (0.49202677607536316)\n",
      "Word:  indirect (0.4868091940879822)\n",
      "Word:  economy (0.4850062131881714)\n",
      "Word:  direct (0.48463308811187744)\n",
      "Word:  tug (0.48454827070236206)\n",
      "Word:  maintain (0.48289474844932556)\n",
      "Word:  optimization (0.48184531927108765)\n",
      "Word:  typical (0.4787161350250244)\n",
      "Word:  cutthroat (0.4757804870605469)\n",
      "Word:  cooperation (0.47272029519081116)\n",
      "Word:  incremental (0.46490478515625)\n",
      "Word:  connection (0.46432268619537354)\n",
      "Word:  salad (0.4638156592845917)\n",
      "Word:  efficiency (0.4607735574245453)\n",
      "Word:  diplomacy (0.4583851397037506)\n",
      "Word:  incentive (0.4566672146320343)\n",
      "Word:  fascinating (0.45601361989974976)\n",
      "Word:  emphasize (0.45523178577423096)\n",
      "Word:  alliance (0.4541073441505432)\n",
      "Word:  interactivity (0.4532441794872284)\n",
      "Word:  asymmetric (0.45240649580955505)\n",
      "Word:  eurogame (0.4473522901535034)\n",
      "Word:  linear (0.44700971245765686)\n",
      "Word:  mild (0.44615352153778076)\n",
      "Word:  scope (0.4458790123462677)\n",
      "Word:  intricate (0.4436107873916626)\n",
      "Word:  puzzly (0.44022029638290405)\n",
      "Word:  strategic (0.4307512044906616)\n",
      "Word:  majority (0.4306298494338989)\n",
      "Word:  negotiation (0.42918452620506287)\n",
      "Word:  sandbox (0.4290362298488617)\n",
      "Word:  tightly (0.424247145652771)\n",
      "Word:  network (0.4224514961242676)\n",
      "Word:  exploit (0.42228537797927856)\n",
      "Word:  tight (0.4187591075897217)\n",
      "Word:  confrontational (0.41391533613204956)\n",
      "Word:  closed (0.4138275384902954)\n",
      "Word:  tactical (0.4135853052139282)\n",
      "Word:  shared (0.4130555987358093)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  walk (0.506497323513031)\n",
      "Word:  forget (0.4993678331375122)\n",
      "Word:  sit (0.49500608444213867)\n",
      "Word:  stare (0.47569066286087036)\n",
      "Word:  talk (0.47532957792282104)\n",
      "Word:  clue (0.4717198610305786)\n",
      "Word:  click (0.47161805629730225)\n",
      "Word:  accomplish (0.4592614769935608)\n",
      "Word:  fly (0.458548367023468)\n",
      "Word:  leap (0.45605194568634033)\n",
      "Word:  frequently (0.4538668394088745)\n",
      "Word:  investigation (0.4522032141685486)\n",
      "Word:  progress (0.4503189027309418)\n",
      "Word:  head (0.450104683637619)\n",
      "Word:  wrap (0.44949963688850403)\n",
      "Word:  giver (0.4400557279586792)\n",
      "Word:  unable (0.43994227051734924)\n",
      "Word:  guesser (0.4393678605556488)\n",
      "Word:  halfway (0.43853941559791565)\n",
      "Word:  seat (0.43810445070266724)\n",
      "Word:  forth (0.4357655644416809)\n",
      "Word:  crack (0.434944212436676)\n",
      "Word:  till (0.43451839685440063)\n",
      "Word:  watch (0.4344373643398285)\n",
      "Word:  unfold (0.4337714910507202)\n",
      "Word:  wire (0.4313613474369049)\n",
      "Word:  grab (0.42919230461120605)\n",
      "Word:  interact (0.4284844398498535)\n",
      "Word:  lie (0.4284825921058655)\n",
      "Word:  alive (0.42733678221702576)\n",
      "Word:  waste (0.4268839359283447)\n",
      "Word:  barely (0.42509424686431885)\n",
      "Word:  shoot (0.4242357015609741)\n",
      "Word:  awhile (0.4234946668148041)\n",
      "Word:  ahead (0.42319780588150024)\n",
      "Word:  continually (0.42304864525794983)\n",
      "Word:  silence (0.4223560392856598)\n",
      "Word:  oh (0.4221148192882538)\n",
      "Word:  office (0.41600507497787476)\n",
      "Word:  gate (0.41383346915245056)\n",
      "Word:  deduce (0.41319161653518677)\n",
      "Word:  repeatedly (0.4127882122993469)\n",
      "Word:  stuck (0.41215625405311584)\n",
      "Word:  rarely (0.4119812548160553)\n",
      "Word:  desperately (0.41197535395622253)\n",
      "Word:  mystery (0.4119352698326111)\n",
      "Word:  communicate (0.40942585468292236)\n",
      "Word:  opposite (0.40906810760498047)\n",
      "Word:  bored (0.40856385231018066)\n",
      "Word:  mess (0.40476980805397034)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  develop (0.4759867489337921)\n",
      "Word:  slowly (0.418673574924469)\n",
      "Word:  infrastructure (0.41449159383773804)\n",
      "Word:  civilization (0.4128535985946655)\n",
      "Word:  obtain (0.4123602509498596)\n",
      "Word:  card (0.4104873239994049)\n",
      "Word:  empire (0.4102138578891754)\n",
      "Word:  exploit (0.40454578399658203)\n",
      "Word:  profit (0.40126335620880127)\n",
      "Word:  efficient (0.3994191884994507)\n",
      "Word:  research (0.3981981873512268)\n",
      "Word:  magic (0.39635539054870605)\n",
      "Word:  business (0.3901450037956238)\n",
      "Word:  model (0.3874862790107727)\n",
      "Word:  powerful (0.3851955533027649)\n",
      "Word:  source (0.3826749622821808)\n",
      "Word:  military (0.3826080560684204)\n",
      "Word:  pile (0.38221943378448486)\n",
      "Word:  era (0.37886372208595276)\n",
      "Word:  secondary (0.37806522846221924)\n",
      "Word:  company (0.375629186630249)\n",
      "Word:  common (0.37445443868637085)\n",
      "Word:  acquisition (0.37384963035583496)\n",
      "Word:  accumulate (0.3729535639286041)\n",
      "Word:  cycle (0.3700864315032959)\n",
      "Word:  similarly (0.36957085132598877)\n",
      "Word:  spell (0.36825060844421387)\n",
      "Word:  anti (0.36633944511413574)\n",
      "Word:  advisor (0.36609092354774475)\n",
      "Word:  hire (0.3642790913581848)\n",
      "Word:  vps (0.36374688148498535)\n",
      "Word:  churn (0.3635721206665039)\n",
      "Word:  shipping (0.36250779032707214)\n",
      "Word:  support (0.3623567223548889)\n",
      "Word:  recruit (0.3617451786994934)\n",
      "Word:  country (0.36116787791252136)\n",
      "Word:  grow (0.36090385913848877)\n",
      "Word:  craft (0.35929110646247864)\n",
      "Word:  achieve (0.35900208353996277)\n",
      "Word:  struggle (0.3581308126449585)\n",
      "Word:  tech (0.3581305742263794)\n",
      "Word:  growth (0.35775452852249146)\n",
      "Word:  farm (0.356820285320282)\n",
      "Word:  blindly (0.3563153147697449)\n",
      "Word:  corporation (0.3562104105949402)\n",
      "Word:  industry (0.3552047610282898)\n",
      "Word:  acquire (0.35509639978408813)\n",
      "Word:  distinct (0.3550320267677307)\n",
      "Word:  expand (0.35375791788101196)\n",
      "Word:  dismantle (0.3532654643058777)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  relatively (0.49423155188560486)\n",
      "Word:  outstay (0.45962077379226685)\n",
      "Word:  lengthy (0.45630645751953125)\n",
      "Word:  minute (0.45278117060661316)\n",
      "Word:  playtime (0.45272761583328247)\n",
      "Word:  teardown (0.4432416260242462)\n",
      "Word:  meat (0.44219842553138733)\n",
      "Word:  overstay (0.441379189491272)\n",
      "Word:  paced (0.4278489947319031)\n",
      "Word:  last (0.4221384823322296)\n",
      "Word:  meaty (0.41965991258621216)\n",
      "Word:  filler (0.4149833917617798)\n",
      "Word:  hearted (0.4149061143398285)\n",
      "Word:  teach (0.39326196908950806)\n",
      "Word:  quick (0.39325568079948425)\n",
      "Word:  pace (0.3749251961708069)\n",
      "Word:  teaching (0.3733217418193817)\n",
      "Word:  snappy (0.37170496582984924)\n",
      "Word:  newbie (0.3709036111831665)\n",
      "Word:  paralysis (0.3692029118537903)\n",
      "Word:  fairly (0.36305803060531616)\n",
      "Word:  bogge (0.36251217126846313)\n",
      "Word:  analysis (0.36161714792251587)\n",
      "Word:  explanation (0.35954296588897705)\n",
      "Word:  downtime (0.3593657314777374)\n",
      "Word:  overhead (0.35737884044647217)\n",
      "Word:  reasonably (0.3528880774974823)\n",
      "Word:  ap (0.34644395112991333)\n",
      "Word:  welcome (0.3442138731479645)\n",
      "Word:  setup (0.34385430812835693)\n",
      "Word:  hour (0.3425709307193756)\n",
      "Word:  thinky (0.3420633375644684)\n",
      "Word:  agonizing (0.3414613604545593)\n",
      "Word:  explain (0.3377476930618286)\n",
      "Word:  nuance (0.33749765157699585)\n",
      "Word:  playing (0.33689936995506287)\n",
      "Word:  long (0.33650442957878113)\n",
      "Word:  crunchy (0.3341197967529297)\n",
      "Word:  forever (0.33374467492103577)\n",
      "Word:  prone (0.330965518951416)\n",
      "Word:  complicated (0.3304016590118408)\n",
      "Word:  smoothly (0.32794222235679626)\n",
      "Word:  induce (0.32737481594085693)\n",
      "Word:  accessible (0.3270717263221741)\n",
      "Word:  duration (0.325696736574173)\n",
      "Word:  min (0.32557469606399536)\n",
      "Word:  beginner (0.32552197575569153)\n",
      "Word:  ruleset (0.3250621557235718)\n",
      "Word:  deep (0.3249150514602661)\n",
      "Word:  short (0.3246726989746094)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  partner (0.5072317123413086)\n",
      "Word:  party (0.48951393365859985)\n",
      "Word:  adult (0.45588529109954834)\n",
      "Word:  young (0.42927905917167664)\n",
      "Word:  night (0.422067791223526)\n",
      "Word:  crowd (0.4132351875305176)\n",
      "Word:  casual (0.41239434480667114)\n",
      "Word:  alike (0.3937007188796997)\n",
      "Word:  friend (0.3931218683719635)\n",
      "Word:  laugh (0.3914254307746887)\n",
      "Word:  occasion (0.38992905616760254)\n",
      "Word:  child (0.38830405473709106)\n",
      "Word:  happily (0.38752585649490356)\n",
      "Word:  mood (0.38549181818962097)\n",
      "Word:  group (0.38273513317108154)\n",
      "Word:  kid (0.3820536434650421)\n",
      "Word:  audience (0.3806491494178772)\n",
      "Word:  wife (0.3739323914051056)\n",
      "Word:  social (0.37097543478012085)\n",
      "Word:  willing (0.3702414035797119)\n",
      "Word:  experienced (0.36098340153694153)\n",
      "Word:  tea (0.3489820957183838)\n",
      "Word:  hobby (0.348487913608551)\n",
      "Word:  suitable (0.34769758582115173)\n",
      "Word:  filler (0.34573647379875183)\n",
      "Word:  gamer (0.3449392318725586)\n",
      "Word:  girlfriend (0.3447672128677368)\n",
      "Word:  request (0.3445582687854767)\n",
      "Word:  parent (0.34417998790740967)\n",
      "Word:  shot (0.34305956959724426)\n",
      "Word:  convince (0.3376844525337219)\n",
      "Word:  family (0.337126761674881)\n",
      "Word:  inexperienced (0.3327062129974365)\n",
      "Word:  people (0.3284910321235657)\n",
      "Word:  loud (0.3281164765357971)\n",
      "Word:  ask (0.32544296979904175)\n",
      "Word:  memory (0.32295191287994385)\n",
      "Word:  evening (0.32271453738212585)\n",
      "Word:  hardcore (0.32159608602523804)\n",
      "Word:  entertain (0.3211071491241455)\n",
      "Word:  blast (0.3184581398963928)\n",
      "Word:  deduction (0.3182399868965149)\n",
      "Word:  husband (0.3162403702735901)\n",
      "Word:  rank (0.31623995304107666)\n",
      "Word:  absolute (0.31561651825904846)\n",
      "Word:  eager (0.31329256296157837)\n",
      "Word:  folk (0.31310924887657166)\n",
      "Word:  regularly (0.3081274926662445)\n",
      "Word:  bluffing (0.3066561222076416)\n",
      "Word:  person (0.3042634129524231)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  feb (0.3475222587585449)\n",
      "Word:  july (0.34400707483291626)\n",
      "Word:  ks (0.33444976806640625)\n",
      "Word:  copy (0.3326942026615143)\n",
      "Word:  nov (0.33064520359039307)\n",
      "Word:  arrive (0.32468509674072266)\n",
      "Word:  dec (0.32419896125793457)\n",
      "Word:  play (0.32030871510505676)\n",
      "Word:  price (0.3124712109565735)\n",
      "Word:  jan (0.3121911585330963)\n",
      "Word:  june (0.3088236451148987)\n",
      "Word:  rating (0.30550459027290344)\n",
      "Word:  own (0.30443379282951355)\n",
      "Word:  april (0.3014459013938904)\n",
      "Word:  purchase (0.3011385202407837)\n",
      "Word:  release (0.3006124496459961)\n",
      "Word:  sept (0.29551273584365845)\n",
      "Word:  update (0.28997787833213806)\n",
      "Word:  essen (0.28785446286201477)\n",
      "Word:  reprint (0.287165105342865)\n",
      "Word:  bgg (0.2855101227760315)\n",
      "Word:  sale (0.279075026512146)\n",
      "Word:  today (0.27707481384277344)\n",
      "Word:  deluxe (0.2658429443836212)\n",
      "Word:  lower (0.26452046632766724)\n",
      "Word:  shelf (0.2636394500732422)\n",
      "Word:  retail (0.2627565860748291)\n",
      "Word:  edit (0.2568184435367584)\n",
      "Word:  january (0.25545188784599304)\n",
      "Word:  traded (0.2543964982032776)\n",
      "Word:  expansion (0.25323155522346497)\n",
      "Word:  christmas (0.2527795433998108)\n",
      "Word:  october (0.25036001205444336)\n",
      "Word:  oct (0.24976275861263275)\n",
      "Word:  edition (0.24952614307403564)\n",
      "Word:  list (0.24731457233428955)\n",
      "Word:  sell (0.2455061674118042)\n",
      "Word:  gift (0.24467235803604126)\n",
      "Word:  c (0.2433231920003891)\n",
      "Word:  log (0.24276193976402283)\n",
      "Word:  pledge (0.2412346601486206)\n",
      "Word:  receive (0.24033062160015106)\n",
      "Word:  uk (0.23550626635551453)\n",
      "Word:  publisher (0.23467488586902618)\n",
      "Word:  delivered (0.23301902413368225)\n",
      "Word:  printing (0.23173439502716064)\n",
      "Word:  week (0.23155991733074188)\n",
      "Word:  deserve (0.23149341344833374)\n",
      "Word:  buy (0.23023608326911926)\n",
      "Word:  glad (0.2296336591243744)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  defeat (0.5088053345680237)\n",
      "Word:  horde (0.44930070638656616)\n",
      "Word:  death (0.44012510776519775)\n",
      "Word:  defend (0.4388909339904785)\n",
      "Word:  shield (0.4206327795982361)\n",
      "Word:  weapon (0.4128132462501526)\n",
      "Word:  minion (0.4125577509403229)\n",
      "Word:  fight (0.40689972043037415)\n",
      "Word:  kill (0.39986833930015564)\n",
      "Word:  enemy (0.3968580961227417)\n",
      "Word:  raid (0.39473387598991394)\n",
      "Word:  alien (0.39391210675239563)\n",
      "Word:  health (0.3927544057369232)\n",
      "Word:  equipment (0.3887178897857666)\n",
      "Word:  creature (0.38590091466903687)\n",
      "Word:  tank (0.3828932046890259)\n",
      "Word:  conquer (0.3814040422439575)\n",
      "Word:  attack (0.38110166788101196)\n",
      "Word:  spawn (0.37640732526779175)\n",
      "Word:  wizard (0.3754342794418335)\n",
      "Word:  rampage (0.37258729338645935)\n",
      "Word:  zombie (0.3699563145637512)\n",
      "Word:  protect (0.36924129724502563)\n",
      "Word:  soldier (0.36727645993232727)\n",
      "Word:  damage (0.3672561049461365)\n",
      "Word:  loot (0.3671531081199646)\n",
      "Word:  armor (0.36569204926490784)\n",
      "Word:  survive (0.36116620898246765)\n",
      "Word:  boss (0.3575511872768402)\n",
      "Word:  unlimited (0.3564634323120117)\n",
      "Word:  ally (0.3537355661392212)\n",
      "Word:  troop (0.35323566198349)\n",
      "Word:  hunter (0.3502379059791565)\n",
      "Word:  pit (0.35000714659690857)\n",
      "Word:  army (0.3474392890930176)\n",
      "Word:  defender (0.34640029072761536)\n",
      "Word:  battle (0.3438168168067932)\n",
      "Word:  tide (0.3431987762451172)\n",
      "Word:  wound (0.34310272336006165)\n",
      "Word:  shadow (0.33954912424087524)\n",
      "Word:  stat (0.3358490467071533)\n",
      "Word:  citizen (0.3354980945587158)\n",
      "Word:  horse (0.33508002758026123)\n",
      "Word:  pirate (0.3346039652824402)\n",
      "Word:  capital (0.33252424001693726)\n",
      "Word:  trap (0.33178895711898804)\n",
      "Word:  crew (0.3285009562969208)\n",
      "Word:  spell (0.3280577063560486)\n",
      "Word:  monster (0.3257540464401245)\n",
      "Word:  die (0.32353508472442627)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  crawler (0.6000641584396362)\n",
      "Word:  rpg (0.5660765767097473)\n",
      "Word:  dungeon (0.5229064226150513)\n",
      "Word:  crawl (0.5129481554031372)\n",
      "Word:  cooperative (0.4728277921676636)\n",
      "Word:  pandemic (0.44797447323799133)\n",
      "Word:  adventure (0.4456910789012909)\n",
      "Word:  marvel (0.4452017545700073)\n",
      "Word:  fantasy (0.4296448230743408)\n",
      "Word:  legacy (0.4052024483680725)\n",
      "Word:  deckbuilding (0.39280128479003906)\n",
      "Word:  coop (0.383415162563324)\n",
      "Word:  skirmish (0.3830471634864807)\n",
      "Word:  campaign (0.37734055519104004)\n",
      "Word:  villain (0.3750152289867401)\n",
      "Word:  deckbuilder (0.37366682291030884)\n",
      "Word:  addictive (0.3702346980571747)\n",
      "Word:  standalone (0.36382895708084106)\n",
      "Word:  crawling (0.3626595139503479)\n",
      "Word:  unmatched (0.3594368100166321)\n",
      "Word:  formula (0.3588078022003174)\n",
      "Word:  itch (0.35789674520492554)\n",
      "Word:  lotr (0.352627158164978)\n",
      "Word:  cthulhu (0.35232365131378174)\n",
      "Word:  introduction (0.35005682706832886)\n",
      "Word:  spirit (0.34562408924102783)\n",
      "Word:  intro (0.34526753425598145)\n",
      "Word:  clank (0.3425496220588684)\n",
      "Word:  implementation (0.33996182680130005)\n",
      "Word:  storytelling (0.3387179374694824)\n",
      "Word:  variation (0.3384403586387634)\n",
      "Word:  boss (0.3376050889492035)\n",
      "Word:  immersive (0.3357442021369934)\n",
      "Word:  horror (0.3330915570259094)\n",
      "Word:  superhero (0.33169662952423096)\n",
      "Word:  dc (0.3295527696609497)\n",
      "Word:  legendary (0.3276591897010803)\n",
      "Word:  lovecraft (0.3196874260902405)\n",
      "Word:  popular (0.31782442331314087)\n",
      "Word:  realm (0.31584498286247253)\n",
      "Word:  fi (0.31490558385849)\n",
      "Word:  hero (0.31250491738319397)\n",
      "Word:  vein (0.31217241287231445)\n",
      "Word:  playable (0.3116069436073303)\n",
      "Word:  mode (0.31153684854507446)\n",
      "Word:  investigator (0.31099870800971985)\n",
      "Word:  operative (0.3108818829059601)\n",
      "Word:  yahtzee (0.3088931441307068)\n",
      "Word:  explore (0.3069937825202942)\n",
      "Word:  deck (0.30640560388565063)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  mouth (0.5467023849487305)\n",
      "Word:  meh (0.5432122349739075)\n",
      "Word:  sour (0.5117270946502686)\n",
      "Word:  disappointing (0.49527376890182495)\n",
      "Word:  comment (0.4840237498283386)\n",
      "Word:  disappointment (0.48127180337905884)\n",
      "Word:  unfortunately (0.4786982536315918)\n",
      "Word:  tad (0.472995400428772)\n",
      "Word:  impression (0.4669150710105896)\n",
      "Word:  honestly (0.4653041958808899)\n",
      "Word:  unfair (0.46468013525009155)\n",
      "Word:  opinion (0.46383413672447205)\n",
      "Word:  terrible (0.46271657943725586)\n",
      "Word:  hype (0.45816469192504883)\n",
      "Word:  surprised (0.450543612241745)\n",
      "Word:  ultimately (0.4497203230857849)\n",
      "Word:  taste (0.44782501459121704)\n",
      "Word:  having (0.44723355770111084)\n",
      "Word:  slight (0.4459298849105835)\n",
      "Word:  flaw (0.4454191029071808)\n",
      "Word:  repetitive (0.444877952337265)\n",
      "Word:  slog (0.44062280654907227)\n",
      "Word:  fault (0.43701672554016113)\n",
      "Word:  positive (0.43570125102996826)\n",
      "Word:  sadly (0.43526941537857056)\n",
      "Word:  desire (0.4280206561088562)\n",
      "Word:  dry (0.4264426529407501)\n",
      "Word:  edit (0.4261333644390106)\n",
      "Word:  disappointed (0.42107099294662476)\n",
      "Word:  certainly (0.4177221655845642)\n",
      "Word:  okay (0.4140767753124237)\n",
      "Word:  boring (0.4137495160102844)\n",
      "Word:  ok (0.40895938873291016)\n",
      "Word:  enjoyment (0.40891599655151367)\n",
      "Word:  significantly (0.40881454944610596)\n",
      "Word:  rating (0.404278039932251)\n",
      "Word:  reflect (0.4006223678588867)\n",
      "Word:  negative (0.3995722532272339)\n",
      "Word:  forgettable (0.39733442664146423)\n",
      "Word:  chrome (0.3965095281600952)\n",
      "Word:  cause (0.39592212438583374)\n",
      "Word:  extreme (0.3956497013568878)\n",
      "Word:  comparison (0.39533349871635437)\n",
      "Word:  disappoint (0.394933819770813)\n",
      "Word:  downtime (0.39259839057922363)\n",
      "Word:  complain (0.3922465443611145)\n",
      "Word:  bother (0.39107829332351685)\n",
      "Word:  dislike (0.3909289240837097)\n",
      "Word:  overall (0.3872759938240051)\n",
      "Word:  worried (0.38603612780570984)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  outcome (0.49046891927719116)\n",
      "Word:  probability (0.4110201895236969)\n",
      "Word:  trump (0.4106459319591522)\n",
      "Word:  wildly (0.4030269980430603)\n",
      "Word:  variance (0.38836735486984253)\n",
      "Word:  factor (0.37798643112182617)\n",
      "Word:  unpredictable (0.3730359673500061)\n",
      "Word:  degree (0.3681039810180664)\n",
      "Word:  swing (0.3651205897331238)\n",
      "Word:  swingy (0.36117249727249146)\n",
      "Word:  success (0.360313355922699)\n",
      "Word:  dominate (0.35715049505233765)\n",
      "Word:  agency (0.3567161560058594)\n",
      "Word:  mercy (0.3548377752304077)\n",
      "Word:  mitigation (0.3481980562210083)\n",
      "Word:  heavily (0.3447313904762268)\n",
      "Word:  overpowered (0.3447261452674866)\n",
      "Word:  die (0.34360164403915405)\n",
      "Word:  dummy (0.3428582549095154)\n",
      "Word:  loss (0.34083592891693115)\n",
      "Word:  side (0.33842021226882935)\n",
      "Word:  mitigate (0.33784520626068115)\n",
      "Word:  potentially (0.3377218246459961)\n",
      "Word:  suit (0.3376518785953522)\n",
      "Word:  fate (0.3312000036239624)\n",
      "Word:  favor (0.3299821615219116)\n",
      "Word:  thumbsdown (0.3297346830368042)\n",
      "Word:  dependant (0.3275388479232788)\n",
      "Word:  zero (0.3263639807701111)\n",
      "Word:  decrease (0.3258697986602783)\n",
      "Word:  hunter (0.32574859261512756)\n",
      "Word:  stat (0.3254137337207794)\n",
      "Word:  largely (0.3228454291820526)\n",
      "Word:  situation (0.32284003496170044)\n",
      "Word:  reliant (0.32219040393829346)\n",
      "Word:  trait (0.32063406705856323)\n",
      "Word:  elimination (0.3192140460014343)\n",
      "Word:  uncontrollable (0.3181372880935669)\n",
      "Word:  randomness (0.317392498254776)\n",
      "Word:  chaos (0.31716403365135193)\n",
      "Word:  attacker (0.31683963537216187)\n",
      "Word:  danger (0.31508538126945496)\n",
      "Word:  lead (0.3150818347930908)\n",
      "Word:  teammate (0.31419438123703003)\n",
      "Word:  cooperation (0.3135624825954437)\n",
      "Word:  relative (0.31092846393585205)\n",
      "Word:  rng (0.308846116065979)\n",
      "Word:  alter (0.3078381419181824)\n",
      "Word:  perceive (0.30754661560058594)\n",
      "Word:  dictate (0.30728888511657715)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  egg (0.502539873123169)\n",
      "Word:  send (0.4923381805419922)\n",
      "Word:  uk (0.47736072540283203)\n",
      "Word:  eat (0.477228045463562)\n",
      "Word:  g (0.4732604920864105)\n",
      "Word:  crop (0.46476566791534424)\n",
      "Word:  instagram (0.4551159739494324)\n",
      "Word:  glass (0.45232605934143066)\n",
      "Word:  horse (0.4452012777328491)\n",
      "Word:  black (0.4447305500507355)\n",
      "Word:  podcast (0.44334104657173157)\n",
      "Word:  male (0.4408327341079712)\n",
      "Word:  south (0.43385857343673706)\n",
      "Word:  n (0.4333980679512024)\n",
      "Word:  tom (0.431911438703537)\n",
      "Word:  lidless (0.4299474358558655)\n",
      "Word:  earth (0.4298292100429535)\n",
      "Word:  rat (0.4282298684120178)\n",
      "Word:  bridge (0.4281359016895294)\n",
      "Word:  soft (0.4275078773498535)\n",
      "Word:  woman (0.4240427017211914)\n",
      "Word:  february (0.4231454133987427)\n",
      "Word:  phil (0.41991913318634033)\n",
      "Word:  chinese (0.41940945386886597)\n",
      "Word:  song (0.4184476435184479)\n",
      "Word:  expo (0.4178934693336487)\n",
      "Word:  zee (0.4129254221916199)\n",
      "Word:  t (0.41211602091789246)\n",
      "Word:  mass (0.4085084795951843)\n",
      "Word:  wine (0.40792521834373474)\n",
      "Word:  w (0.4064059257507324)\n",
      "Word:  christmas (0.4063922166824341)\n",
      "Word:  white (0.4044587016105652)\n",
      "Word:  shop (0.40403568744659424)\n",
      "Word:  bat (0.40401628613471985)\n",
      "Word:  merchant (0.40238356590270996)\n",
      "Word:  wood (0.4020366668701172)\n",
      "Word:  <UNK> (0.4016488194465637)\n",
      "Word:  artist (0.40161818265914917)\n",
      "Word:  gift (0.40040844678878784)\n",
      "Word:  flgs (0.39999866485595703)\n",
      "Word:  workshop (0.399297833442688)\n",
      "Word:  disk (0.3990665674209595)\n",
      "Word:  finger (0.39898407459259033)\n",
      "Word:  hat (0.3979949355125427)\n",
      "Word:  hot (0.39459189772605896)\n",
      "Word:  paper (0.3943905532360077)\n",
      "Word:  fruit (0.3939244747161865)\n",
      "Word:  mark (0.3933597207069397)\n",
      "Word:  october (0.393348753452301)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  neoprene (0.6151208877563477)\n",
      "Word:  tray (0.6136331558227539)\n",
      "Word:  playmat (0.6042203903198242)\n",
      "Word:  insert (0.5954815149307251)\n",
      "Word:  custom (0.5874666571617126)\n",
      "Word:  mat (0.587461531162262)\n",
      "Word:  holder (0.5621674060821533)\n",
      "Word:  sleeved (0.5544724464416504)\n",
      "Word:  metal (0.5543687343597412)\n",
      "Word:  printed (0.5505377054214478)\n",
      "Word:  cards (0.5485683679580688)\n",
      "Word:  accessory (0.5474342107772827)\n",
      "Word:  thick (0.54643315076828)\n",
      "Word:  etsy (0.5446043014526367)\n",
      "Word:  sleeves (0.5345938205718994)\n",
      "Word:  includes (0.533226490020752)\n",
      "Word:  arcane (0.5308806896209717)\n",
      "Word:  orange (0.5281413793563843)\n",
      "Word:  acrylic (0.5266827344894409)\n",
      "Word:  deluxe (0.5255779027938843)\n",
      "Word:  sized (0.5237959623336792)\n",
      "Word:  fold (0.5231562256813049)\n",
      "Word:  standee (0.5226285457611084)\n",
      "Word:  tinmen (0.521885097026825)\n",
      "Word:  storage (0.5193136930465698)\n",
      "Word:  premium (0.5184355974197388)\n",
      "Word:  print (0.5163414478302002)\n",
      "Word:  container (0.512751042842865)\n",
      "Word:  mini (0.5119497179985046)\n",
      "Word:  tokens (0.5099326968193054)\n",
      "Word:  resin (0.5036271214485168)\n",
      "Word:  exclusive (0.5025674700737)\n",
      "Word:  organizer (0.4996032118797302)\n",
      "Word:  pink (0.4944515824317932)\n",
      "Word:  kit (0.48948922753334045)\n",
      "Word:  european (0.487684965133667)\n",
      "Word:  yellow (0.4856913983821869)\n",
      "Word:  sleeve (0.48373502492904663)\n",
      "Word:  minis (0.4832298755645752)\n",
      "Word:  brown (0.4800523519515991)\n",
      "Word:  expansions (0.47910934686660767)\n",
      "Word:  plastic (0.47898006439208984)\n",
      "Word:  usa (0.478969544172287)\n",
      "Word:  gray (0.47668617963790894)\n",
      "Word:  pledge (0.4761546850204468)\n",
      "Word:  cloth (0.474122554063797)\n",
      "Word:  mm (0.4735739231109619)\n",
      "Word:  amazon (0.47332286834716797)\n",
      "Word:  chest (0.4706154465675354)\n",
      "Word:  binder (0.469510018825531)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  column (0.478649377822876)\n",
      "Word:  district (0.462035208940506)\n",
      "Word:  contract (0.42773759365081787)\n",
      "Word:  colored (0.4186997413635254)\n",
      "Word:  distribute (0.40530484914779663)\n",
      "Word:  activate (0.40480339527130127)\n",
      "Word:  matching (0.39748501777648926)\n",
      "Word:  action (0.39744266867637634)\n",
      "Word:  determine (0.39421480894088745)\n",
      "Word:  income (0.3926471173763275)\n",
      "Word:  adjacent (0.38816213607788086)\n",
      "Word:  trigger (0.38262224197387695)\n",
      "Word:  harvest (0.3724334239959717)\n",
      "Word:  chit (0.3724225163459778)\n",
      "Word:  colour (0.3718957304954529)\n",
      "Word:  tile (0.3697158098220825)\n",
      "Word:  manipulate (0.3686155676841736)\n",
      "Word:  pepper (0.36811280250549316)\n",
      "Word:  grid (0.36605894565582275)\n",
      "Word:  palace (0.3614729940891266)\n",
      "Word:  resource (0.36089032888412476)\n",
      "Word:  access (0.36031413078308105)\n",
      "Word:  fulfill (0.3569292724132538)\n",
      "Word:  place (0.3548680543899536)\n",
      "Word:  vp (0.35380101203918457)\n",
      "Word:  worker (0.3529285192489624)\n",
      "Word:  disk (0.35245585441589355)\n",
      "Word:  cube (0.3511963486671448)\n",
      "Word:  display (0.3511805236339569)\n",
      "Word:  brick (0.3473850190639496)\n",
      "Word:  pawn (0.34673255681991577)\n",
      "Word:  disc (0.3455035388469696)\n",
      "Word:  rotate (0.3432215452194214)\n",
      "Word:  dictate (0.3422699570655823)\n",
      "Word:  correspond (0.3422023057937622)\n",
      "Word:  currency (0.3376375138759613)\n",
      "Word:  select (0.3374179005622864)\n",
      "Word:  tableau (0.336986780166626)\n",
      "Word:  conveyor (0.33643847703933716)\n",
      "Word:  bonus (0.3345959186553955)\n",
      "Word:  timing (0.33275800943374634)\n",
      "Word:  fulfil (0.3303825259208679)\n",
      "Word:  maximize (0.3267100155353546)\n",
      "Word:  perform (0.325203001499176)\n",
      "Word:  passenger (0.3232395350933075)\n",
      "Word:  cascade (0.3212452530860901)\n",
      "Word:  vps (0.31910303235054016)\n",
      "Word:  method (0.3190455138683319)\n",
      "Word:  scoring (0.31861814856529236)\n",
      "Word:  valuable (0.3173079788684845)\n",
      "\n",
      "Given aspect most representative words are:\n",
      "Word:  theme (0.6446172595024109)\n",
      "Word:  artwork (0.5123491883277893)\n",
      "Word:  aesthetic (0.43919724225997925)\n",
      "Word:  art (0.4350498914718628)\n",
      "Word:  presentation (0.4330867826938629)\n",
      "Word:  gorgeous (0.41602373123168945)\n",
      "Word:  colorful (0.40188515186309814)\n",
      "Word:  thematic (0.39237409830093384)\n",
      "Word:  integration (0.3874858021736145)\n",
      "Word:  mechanics (0.3851022720336914)\n",
      "Word:  attractive (0.37558406591415405)\n",
      "Word:  illustration (0.3735518455505371)\n",
      "Word:  beautiful (0.367923378944397)\n",
      "Word:  charming (0.3638280928134918)\n",
      "Word:  visually (0.36371931433677673)\n",
      "Word:  beautifully (0.3622892498970032)\n",
      "Word:  theming (0.3611178398132324)\n",
      "Word:  thematically (0.3584868013858795)\n",
      "Word:  execution (0.34273838996887207)\n",
      "Word:  setting (0.3374001979827881)\n",
      "Word:  looking (0.32995423674583435)\n",
      "Word:  evocative (0.3295383155345917)\n",
      "Word:  atmosphere (0.3288863003253937)\n",
      "Word:  functional (0.32774850726127625)\n",
      "Word:  graphic (0.3253664970397949)\n",
      "Word:  quirky (0.3216332197189331)\n",
      "Word:  aesthetically (0.31965172290802)\n",
      "Word:  stunning (0.31665199995040894)\n",
      "Word:  mechanically (0.31561845541000366)\n",
      "Word:  wonderfully (0.3122217059135437)\n",
      "Word:  paste (0.3112303614616394)\n",
      "Word:  cartoony (0.30968013405799866)\n",
      "Word:  sci (0.30338364839553833)\n",
      "Word:  notch (0.30206143856048584)\n",
      "Word:  visual (0.30000248551368713)\n",
      "Word:  tactile (0.29831239581108093)\n",
      "Word:  ugly (0.2960318326950073)\n",
      "Word:  cute (0.292214572429657)\n",
      "Word:  component (0.28924494981765747)\n",
      "Word:  fi (0.28657999634742737)\n",
      "Word:  outstanding (0.285323828458786)\n",
      "Word:  aesthetics (0.28343725204467773)\n",
      "Word:  quality (0.2826980948448181)\n",
      "Word:  integrate (0.2809438705444336)\n",
      "Word:  unattractive (0.278813898563385)\n",
      "Word:  superb (0.2753487229347229)\n",
      "Word:  elegant (0.27103859186172485)\n",
      "Word:  lovely (0.2649843096733093)\n",
      "Word:  immersive (0.2587738037109375)\n",
      "Word:  bland (0.2583070993423462)\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/94248 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d94e11185c864c498b2f429e12e71bdb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  416 . The limit is set to 80 tokens.\n",
      "We loose information on 21 points.This is 0.022281639928698752% of the dataset.\n",
      "Padding sequences to length (80).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary<7882 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary<11292 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary<13709 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary<15621 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary<17363 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary<18957 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary<20461 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary<21806 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary<23061 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<23504 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 94248 documents (total 790458 corpus positions)\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Dictionary\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<23504 unique tokens: ['game', 'german', 'involve', 'long', 'politic']...> from 94248 documents (total 790458 corpus positions)\", 'datetime': '2025-02-08T15:46:40.044265', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator<processes=15, batch_size=64> to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:15 batches submitted to accumulate stats from 960 documents (-1026 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:24 batches submitted to accumulate stats from 1536 documents (-1484 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:26 batches submitted to accumulate stats from 1664 documents (-1476 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:33 batches submitted to accumulate stats from 2112 documents (-1814 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:47 batches submitted to accumulate stats from 3008 documents (-2865 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:57 batches submitted to accumulate stats from 3648 documents (-3346 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:84 batches submitted to accumulate stats from 5376 documents (-4888 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:85 batches submitted to accumulate stats from 5440 documents (-4884 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:89 batches submitted to accumulate stats from 5696 documents (-5030 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:98 batches submitted to accumulate stats from 6272 documents (-5546 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:99 batches submitted to accumulate stats from 6336 documents (-5475 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:103 batches submitted to accumulate stats from 6592 documents (-5566 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:107 batches submitted to accumulate stats from 6848 documents (-5722 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:108 batches submitted to accumulate stats from 6912 documents (-5596 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:109 batches submitted to accumulate stats from 6976 documents (-5593 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:112 batches submitted to accumulate stats from 7168 documents (-5755 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:124 batches submitted to accumulate stats from 7936 documents (-6417 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:133 batches submitted to accumulate stats from 8512 documents (-6801 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:139 batches submitted to accumulate stats from 8896 documents (-6947 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:146 batches submitted to accumulate stats from 9344 documents (-7196 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:148 batches submitted to accumulate stats from 9472 documents (-7222 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:152 batches submitted to accumulate stats from 9728 documents (-7332 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:154 batches submitted to accumulate stats from 9856 documents (-7369 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:158 batches submitted to accumulate stats from 10112 documents (-7557 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:167 batches submitted to accumulate stats from 10688 documents (-7920 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:168 batches submitted to accumulate stats from 10752 documents (-7898 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:171 batches submitted to accumulate stats from 10944 documents (-8032 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:174 batches submitted to accumulate stats from 11136 documents (-8038 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:175 batches submitted to accumulate stats from 11200 documents (-8005 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:177 batches submitted to accumulate stats from 11328 documents (-8112 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:178 batches submitted to accumulate stats from 11392 documents (-8082 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:184 batches submitted to accumulate stats from 11776 documents (-8244 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:186 batches submitted to accumulate stats from 11904 documents (-8269 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:188 batches submitted to accumulate stats from 12032 documents (-8271 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:195 batches submitted to accumulate stats from 12480 documents (-8703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:197 batches submitted to accumulate stats from 12608 documents (-8585 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:200 batches submitted to accumulate stats from 12800 documents (-8654 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:205 batches submitted to accumulate stats from 13120 documents (-8830 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:206 batches submitted to accumulate stats from 13184 documents (-8828 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:210 batches submitted to accumulate stats from 13440 documents (-8936 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:214 batches submitted to accumulate stats from 13696 documents (-9062 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:218 batches submitted to accumulate stats from 13952 documents (-9070 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:225 batches submitted to accumulate stats from 14400 documents (-9398 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:230 batches submitted to accumulate stats from 14720 documents (-9540 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:231 batches submitted to accumulate stats from 14784 documents (-9522 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:232 batches submitted to accumulate stats from 14848 documents (-9501 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:251 batches submitted to accumulate stats from 16064 documents (-10462 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:252 batches submitted to accumulate stats from 16128 documents (-10433 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:255 batches submitted to accumulate stats from 16320 documents (-10453 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:257 batches submitted to accumulate stats from 16448 documents (-10524 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:258 batches submitted to accumulate stats from 16512 documents (-10433 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:261 batches submitted to accumulate stats from 16704 documents (-10520 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:266 batches submitted to accumulate stats from 17024 documents (-10728 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:267 batches submitted to accumulate stats from 17088 documents (-10708 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:270 batches submitted to accumulate stats from 17280 documents (-10746 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:273 batches submitted to accumulate stats from 17472 documents (-10826 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:276 batches submitted to accumulate stats from 17664 documents (-10856 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:278 batches submitted to accumulate stats from 17792 documents (-10789 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:279 batches submitted to accumulate stats from 17856 documents (-10788 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:288 batches submitted to accumulate stats from 18432 documents (-11029 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:289 batches submitted to accumulate stats from 18496 documents (-10996 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:296 batches submitted to accumulate stats from 18944 documents (-11229 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:297 batches submitted to accumulate stats from 19008 documents (-11217 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:299 batches submitted to accumulate stats from 19136 documents (-11211 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:302 batches submitted to accumulate stats from 19328 documents (-11240 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:303 batches submitted to accumulate stats from 19392 documents (-11164 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:304 batches submitted to accumulate stats from 19456 documents (-11114 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:305 batches submitted to accumulate stats from 19520 documents (-11101 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:306 batches submitted to accumulate stats from 19584 documents (-10959 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:309 batches submitted to accumulate stats from 19776 documents (-11004 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:311 batches submitted to accumulate stats from 19904 documents (-11025 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:312 batches submitted to accumulate stats from 19968 documents (-10979 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:316 batches submitted to accumulate stats from 20224 documents (-11063 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:325 batches submitted to accumulate stats from 20800 documents (-11518 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:334 batches submitted to accumulate stats from 21376 documents (-11917 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:337 batches submitted to accumulate stats from 21568 documents (-11959 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:354 batches submitted to accumulate stats from 22656 documents (-12714 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:360 batches submitted to accumulate stats from 23040 documents (-12923 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:361 batches submitted to accumulate stats from 23104 documents (-12906 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:362 batches submitted to accumulate stats from 23168 documents (-12818 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:366 batches submitted to accumulate stats from 23424 documents (-12889 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:372 batches submitted to accumulate stats from 23808 documents (-13185 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:375 batches submitted to accumulate stats from 24000 documents (-13345 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:385 batches submitted to accumulate stats from 24640 documents (-13790 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:390 batches submitted to accumulate stats from 24960 documents (-13975 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:398 batches submitted to accumulate stats from 25472 documents (-14574 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:402 batches submitted to accumulate stats from 25728 documents (-14659 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:413 batches submitted to accumulate stats from 26432 documents (-15247 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:428 batches submitted to accumulate stats from 27392 documents (-16168 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:435 batches submitted to accumulate stats from 27840 documents (-16461 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:436 batches submitted to accumulate stats from 27904 documents (-16434 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:437 batches submitted to accumulate stats from 27968 documents (-16407 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:438 batches submitted to accumulate stats from 28032 documents (-16373 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:440 batches submitted to accumulate stats from 28160 documents (-16413 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:441 batches submitted to accumulate stats from 28224 documents (-16391 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:457 batches submitted to accumulate stats from 29248 documents (-17272 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:458 batches submitted to accumulate stats from 29312 documents (-17217 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:459 batches submitted to accumulate stats from 29376 documents (-17196 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:462 batches submitted to accumulate stats from 29568 documents (-17201 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:464 batches submitted to accumulate stats from 29696 documents (-17309 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:476 batches submitted to accumulate stats from 30464 documents (-17957 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:483 batches submitted to accumulate stats from 30912 documents (-18414 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:492 batches submitted to accumulate stats from 31488 documents (-18955 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:496 batches submitted to accumulate stats from 31744 documents (-19069 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:504 batches submitted to accumulate stats from 32256 documents (-19492 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:505 batches submitted to accumulate stats from 32320 documents (-19440 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:519 batches submitted to accumulate stats from 33216 documents (-20190 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:526 batches submitted to accumulate stats from 33664 documents (-20406 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:532 batches submitted to accumulate stats from 34048 documents (-20490 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:537 batches submitted to accumulate stats from 34368 documents (-20691 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:539 batches submitted to accumulate stats from 34496 documents (-20689 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:548 batches submitted to accumulate stats from 35072 documents (-21242 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:554 batches submitted to accumulate stats from 35456 documents (-21517 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:578 batches submitted to accumulate stats from 36992 documents (-22659 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:582 batches submitted to accumulate stats from 37248 documents (-22731 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:583 batches submitted to accumulate stats from 37312 documents (-22550 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:585 batches submitted to accumulate stats from 37440 documents (-22608 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:586 batches submitted to accumulate stats from 37504 documents (-22607 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:594 batches submitted to accumulate stats from 38016 documents (-23115 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:595 batches submitted to accumulate stats from 38080 documents (-23095 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:596 batches submitted to accumulate stats from 38144 documents (-23066 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:598 batches submitted to accumulate stats from 38272 documents (-22930 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:602 batches submitted to accumulate stats from 38528 documents (-23111 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:604 batches submitted to accumulate stats from 38656 documents (-23170 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:622 batches submitted to accumulate stats from 39808 documents (-24276 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:627 batches submitted to accumulate stats from 40128 documents (-24457 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:632 batches submitted to accumulate stats from 40448 documents (-24577 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:637 batches submitted to accumulate stats from 40768 documents (-24754 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:638 batches submitted to accumulate stats from 40832 documents (-24703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:640 batches submitted to accumulate stats from 40960 documents (-24703 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:642 batches submitted to accumulate stats from 41088 documents (-24758 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:648 batches submitted to accumulate stats from 41472 documents (-25142 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:657 batches submitted to accumulate stats from 42048 documents (-25673 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:659 batches submitted to accumulate stats from 42176 documents (-25746 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:661 batches submitted to accumulate stats from 42304 documents (-25772 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:662 batches submitted to accumulate stats from 42368 documents (-25668 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:667 batches submitted to accumulate stats from 42688 documents (-25749 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:672 batches submitted to accumulate stats from 43008 documents (-25925 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:673 batches submitted to accumulate stats from 43072 documents (-25897 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:675 batches submitted to accumulate stats from 43200 documents (-25926 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:691 batches submitted to accumulate stats from 44224 documents (-27058 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:692 batches submitted to accumulate stats from 44288 documents (-26963 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:695 batches submitted to accumulate stats from 44480 documents (-27142 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:696 batches submitted to accumulate stats from 44544 documents (-27063 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:698 batches submitted to accumulate stats from 44672 documents (-27115 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:702 batches submitted to accumulate stats from 44928 documents (-27374 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:704 batches submitted to accumulate stats from 45056 documents (-27326 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:710 batches submitted to accumulate stats from 45440 documents (-27619 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:712 batches submitted to accumulate stats from 45568 documents (-27688 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:724 batches submitted to accumulate stats from 46336 documents (-28461 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:729 batches submitted to accumulate stats from 46656 documents (-28685 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:730 batches submitted to accumulate stats from 46720 documents (-28630 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:731 batches submitted to accumulate stats from 46784 documents (-28591 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:732 batches submitted to accumulate stats from 46848 documents (-28478 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:743 batches submitted to accumulate stats from 47552 documents (-29131 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:744 batches submitted to accumulate stats from 47616 documents (-29109 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:745 batches submitted to accumulate stats from 47680 documents (-29094 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:748 batches submitted to accumulate stats from 47872 documents (-29183 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:749 batches submitted to accumulate stats from 47936 documents (-29121 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:768 batches submitted to accumulate stats from 49152 documents (-30537 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:770 batches submitted to accumulate stats from 49280 documents (-30560 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:778 batches submitted to accumulate stats from 49792 documents (-31007 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:779 batches submitted to accumulate stats from 49856 documents (-30674 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:781 batches submitted to accumulate stats from 49984 documents (-30779 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:785 batches submitted to accumulate stats from 50240 documents (-31052 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:788 batches submitted to accumulate stats from 50432 documents (-31176 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:789 batches submitted to accumulate stats from 50496 documents (-31057 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:798 batches submitted to accumulate stats from 51072 documents (-31277 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:799 batches submitted to accumulate stats from 51136 documents (-31250 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:805 batches submitted to accumulate stats from 51520 documents (-31507 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:816 batches submitted to accumulate stats from 52224 documents (-31894 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:820 batches submitted to accumulate stats from 52480 documents (-32082 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:836 batches submitted to accumulate stats from 53504 documents (-32912 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:842 batches submitted to accumulate stats from 53888 documents (-33231 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:853 batches submitted to accumulate stats from 54592 documents (-33866 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:856 batches submitted to accumulate stats from 54784 documents (-33747 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:857 batches submitted to accumulate stats from 54848 documents (-33745 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:858 batches submitted to accumulate stats from 54912 documents (-33680 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:859 batches submitted to accumulate stats from 54976 documents (-33636 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:875 batches submitted to accumulate stats from 56000 documents (-34610 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:877 batches submitted to accumulate stats from 56128 documents (-34567 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:880 batches submitted to accumulate stats from 56320 documents (-34445 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:882 batches submitted to accumulate stats from 56448 documents (-34307 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:883 batches submitted to accumulate stats from 56512 documents (-34286 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:884 batches submitted to accumulate stats from 56576 documents (-34266 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:885 batches submitted to accumulate stats from 56640 documents (-34184 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:886 batches submitted to accumulate stats from 56704 documents (-34143 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:888 batches submitted to accumulate stats from 56832 documents (-34206 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:890 batches submitted to accumulate stats from 56960 documents (-34207 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:892 batches submitted to accumulate stats from 57088 documents (-34232 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:896 batches submitted to accumulate stats from 57344 documents (-34249 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:897 batches submitted to accumulate stats from 57408 documents (-34176 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:904 batches submitted to accumulate stats from 57856 documents (-34533 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:909 batches submitted to accumulate stats from 58176 documents (-34715 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:916 batches submitted to accumulate stats from 58624 documents (-35068 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:922 batches submitted to accumulate stats from 59008 documents (-35284 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:931 batches submitted to accumulate stats from 59584 documents (-35667 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:933 batches submitted to accumulate stats from 59712 documents (-35597 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:936 batches submitted to accumulate stats from 59904 documents (-35716 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:952 batches submitted to accumulate stats from 60928 documents (-36271 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:955 batches submitted to accumulate stats from 61120 documents (-36367 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:960 batches submitted to accumulate stats from 61440 documents (-36494 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:962 batches submitted to accumulate stats from 61568 documents (-36524 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:968 batches submitted to accumulate stats from 61952 documents (-36767 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:975 batches submitted to accumulate stats from 62400 documents (-37138 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:981 batches submitted to accumulate stats from 62784 documents (-37354 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:993 batches submitted to accumulate stats from 63552 documents (-38153 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:994 batches submitted to accumulate stats from 63616 documents (-38146 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1003 batches submitted to accumulate stats from 64192 documents (-38450 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1019 batches submitted to accumulate stats from 65216 documents (-39530 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1020 batches submitted to accumulate stats from 65280 documents (-39516 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1021 batches submitted to accumulate stats from 65344 documents (-39489 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1023 batches submitted to accumulate stats from 65472 documents (-39304 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1030 batches submitted to accumulate stats from 65920 documents (-39696 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1031 batches submitted to accumulate stats from 65984 documents (-39655 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1032 batches submitted to accumulate stats from 66048 documents (-39640 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1035 batches submitted to accumulate stats from 66240 documents (-39719 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1040 batches submitted to accumulate stats from 66560 documents (-39984 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1048 batches submitted to accumulate stats from 67072 documents (-40559 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1051 batches submitted to accumulate stats from 67264 documents (-40581 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1052 batches submitted to accumulate stats from 67328 documents (-40545 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1064 batches submitted to accumulate stats from 68096 documents (-41423 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1072 batches submitted to accumulate stats from 68608 documents (-42032 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1080 batches submitted to accumulate stats from 69120 documents (-42451 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1084 batches submitted to accumulate stats from 69376 documents (-42506 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1087 batches submitted to accumulate stats from 69568 documents (-42562 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1094 batches submitted to accumulate stats from 70016 documents (-42806 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1095 batches submitted to accumulate stats from 70080 documents (-42782 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1103 batches submitted to accumulate stats from 70592 documents (-43065 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1106 batches submitted to accumulate stats from 70784 documents (-43110 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1108 batches submitted to accumulate stats from 70912 documents (-43120 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1110 batches submitted to accumulate stats from 71040 documents (-43182 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1114 batches submitted to accumulate stats from 71296 documents (-43455 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1117 batches submitted to accumulate stats from 71488 documents (-43597 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1135 batches submitted to accumulate stats from 72640 documents (-44794 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1136 batches submitted to accumulate stats from 72704 documents (-44790 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1142 batches submitted to accumulate stats from 73088 documents (-45135 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1152 batches submitted to accumulate stats from 73728 documents (-45741 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1166 batches submitted to accumulate stats from 74624 documents (-46295 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1167 batches submitted to accumulate stats from 74688 documents (-46200 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1168 batches submitted to accumulate stats from 74752 documents (-46118 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1169 batches submitted to accumulate stats from 74816 documents (-46067 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1171 batches submitted to accumulate stats from 74944 documents (-46158 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1172 batches submitted to accumulate stats from 75008 documents (-46018 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1181 batches submitted to accumulate stats from 75584 documents (-46323 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1182 batches submitted to accumulate stats from 75648 documents (-46186 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1183 batches submitted to accumulate stats from 75712 documents (-46126 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1184 batches submitted to accumulate stats from 75776 documents (-46073 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1187 batches submitted to accumulate stats from 75968 documents (-46041 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1188 batches submitted to accumulate stats from 76032 documents (-46002 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1189 batches submitted to accumulate stats from 76096 documents (-45972 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1190 batches submitted to accumulate stats from 76160 documents (-45961 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1195 batches submitted to accumulate stats from 76480 documents (-46201 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1196 batches submitted to accumulate stats from 76544 documents (-46161 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1197 batches submitted to accumulate stats from 76608 documents (-46156 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1210 batches submitted to accumulate stats from 77440 documents (-46932 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1211 batches submitted to accumulate stats from 77504 documents (-46913 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1214 batches submitted to accumulate stats from 77696 documents (-47056 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1215 batches submitted to accumulate stats from 77760 documents (-47011 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1218 batches submitted to accumulate stats from 77952 documents (-47144 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1221 batches submitted to accumulate stats from 78144 documents (-47265 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1225 batches submitted to accumulate stats from 78400 documents (-47361 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1230 batches submitted to accumulate stats from 78720 documents (-47408 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1239 batches submitted to accumulate stats from 79296 documents (-47980 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1242 batches submitted to accumulate stats from 79488 documents (-48035 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1243 batches submitted to accumulate stats from 79552 documents (-48018 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1246 batches submitted to accumulate stats from 79744 documents (-48104 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1247 batches submitted to accumulate stats from 79808 documents (-48012 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1248 batches submitted to accumulate stats from 79872 documents (-47714 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1250 batches submitted to accumulate stats from 80000 documents (-47760 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1257 batches submitted to accumulate stats from 80448 documents (-48188 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1259 batches submitted to accumulate stats from 80576 documents (-48278 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1268 batches submitted to accumulate stats from 81152 documents (-48758 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1272 batches submitted to accumulate stats from 81408 documents (-48967 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1286 batches submitted to accumulate stats from 82304 documents (-49457 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1293 batches submitted to accumulate stats from 82752 documents (-49769 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1301 batches submitted to accumulate stats from 83264 documents (-50211 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1316 batches submitted to accumulate stats from 84224 documents (-51035 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1318 batches submitted to accumulate stats from 84352 documents (-51125 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1320 batches submitted to accumulate stats from 84480 documents (-51149 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1321 batches submitted to accumulate stats from 84544 documents (-51045 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1322 batches submitted to accumulate stats from 84608 documents (-51014 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1333 batches submitted to accumulate stats from 85312 documents (-51721 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1335 batches submitted to accumulate stats from 85440 documents (-51821 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1337 batches submitted to accumulate stats from 85568 documents (-51887 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1340 batches submitted to accumulate stats from 85760 documents (-52053 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1346 batches submitted to accumulate stats from 86144 documents (-52387 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1372 batches submitted to accumulate stats from 87808 documents (-54052 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1377 batches submitted to accumulate stats from 88128 documents (-54210 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1386 batches submitted to accumulate stats from 88704 documents (-54609 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1397 batches submitted to accumulate stats from 89408 documents (-55073 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1399 batches submitted to accumulate stats from 89536 documents (-55043 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1412 batches submitted to accumulate stats from 90368 documents (-55686 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1419 batches submitted to accumulate stats from 90816 documents (-55995 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1423 batches submitted to accumulate stats from 91072 documents (-56103 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1429 batches submitted to accumulate stats from 91456 documents (-56375 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1430 batches submitted to accumulate stats from 91520 documents (-56374 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1440 batches submitted to accumulate stats from 92160 documents (-56954 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1449 batches submitted to accumulate stats from 92736 documents (-57493 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1450 batches submitted to accumulate stats from 92800 documents (-57423 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1451 batches submitted to accumulate stats from 92864 documents (-57307 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1453 batches submitted to accumulate stats from 92992 documents (-57254 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1454 batches submitted to accumulate stats from 93056 documents (-57208 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1461 batches submitted to accumulate stats from 93504 documents (-57486 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1462 batches submitted to accumulate stats from 93568 documents (-57468 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1464 batches submitted to accumulate stats from 93696 documents (-57482 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1465 batches submitted to accumulate stats from 93760 documents (-57429 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:1466 batches submitted to accumulate stats from 93824 documents (-57330 virtual)\n",
      "INFO:gensim.topic_coherence.text_analysis:15 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 196893 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.06975844388455707"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.008273400Z",
     "start_time": "2025-02-08T14:47:18.088219Z"
    }
   },
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "65e0b816a06ee9b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.07990060733246114,\n",
       " 0.2743863220801869,\n",
       " -0.039693072474737336,\n",
       " -0.06331046435950176,\n",
       " -0.2756276683915752,\n",
       " -0.024395088608448762,\n",
       " -0.0701294019252142,\n",
       " -0.1982382135709934,\n",
       " -0.03980300302191894,\n",
       " 0.4425553892722734,\n",
       " -0.12215550305377003,\n",
       " -0.2685362188016142,\n",
       " -0.48079022183623965,\n",
       " 0.08988606782609228,\n",
       " -0.5107417881005712,\n",
       " 0.2503583701455803]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.evaluation import coherence_per_aspect\n",
    "\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]\n",
    "c, m = coherence_per_aspect(aspect_words, ds.text_ds, 3)"
   ],
   "id": "a1b8c8af12ad8460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "c",
   "id": "8ead46ac716e85b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m.get_coherence()",
   "id": "569dd10217a1e3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, get_aspect_top_k_words\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "config = ABAEModelConfiguration(corpus_file=corpus, model_name=f\"hands_on\")\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "# word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "# aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=3) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=3)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "13e9619f5dead821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! <br>\n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The main goal of ABAE is to extract interpretable and meaningful aspects, which makes coherence the more aligned metric.<br> Reconstruction error might help guide training but doesn’t guarantee that the extracted aspects are semantically useful.",
   "id": "cd4dfe8c62445d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper, HyperparameterTuningManager\n",
    "\n",
    "configurations = 4  # We try 15 different configurations\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "print(f\"Starting procedure. We try a total of {configurations}\")\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "hp_tuning_manager = HyperparameterTuningManager(hp_wrapper, corpus_file, \"./output\")"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hp_tuning_manager(different_configurations=configurations, repeat=3) # todo tune not on metric but on loss. We want the best reconstruciton model.\n",
    "# Bottom line: Prioritize classification performance, but monitor coherence if it affects usability.\n",
    "# The output of this cell is very long therefore I deleted it"
   ],
   "id": "8b31050bf118ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    columns=[\n",
    "        'name', 'mean_loss', 'mean_coherence', 'aspect_size', 'embedding_size', 'epochs', 'batch_size', 'learning_rate',\n",
    "        'decay_rate', 'momentum', 'negative_sample_size'\n",
    "    ]\n",
    ")\n",
    "\n",
    "i = 0\n",
    "# Inspect results we stored for process. todo: Quando finisci salvalo come file da \"tenere\" in repo cosi che chi controlla lo ha gia.\n",
    "for element in Path(\"./output\").iterdir():\n",
    "\n",
    "    # Search only for the tuning process runs.\n",
    "    if element.is_dir() and element.name.startswith(\"tuning\"):\n",
    "        run_result = json.load(open(str(element) + \"/run_results.json\"))\n",
    "        # We want the best configurations.\n",
    "\n",
    "        obj = {\n",
    "            'name': element.name,\n",
    "            'mean_loss': np.mean(run_result[\"evaluation_loss\"]),\n",
    "            'mean_coherence': np.mean(run_result[\"coherence\"])\n",
    "        }\n",
    "        temp = pd.DataFrame(run_result['params'] | obj, index=[i])\n",
    "        dataframe = pd.concat([temp, dataframe])\n",
    "        i += 1"
   ],
   "id": "6f11e2391039f8fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds.text_ds.apply(lambda x: x.split(' '))",
   "id": "83f0afe3f02329d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# THIS IS OKAY!\n",
    "\n",
    "from gensim import corpora\n",
    "# See if another coherence metric might be better.\n",
    "from core.evaluation import coherence_model_generation, normalize, get_aspect_top_k_words\n",
    "import json\n",
    "from core.dataset import TokenizedDataset\n",
    "from core.train import ABAEModelManager, ABAEModelConfiguration\n",
    "\n",
    "model_name = 'tuning_7128da5c-04c0-4e91-abf2-462f9646b48d'\n",
    "c_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "# Load aspects\n",
    "parameters = json.load(open(f\"./output/{model_name}/run_results.json\"))['params']\n",
    "config = ABAEModelConfiguration(corpus_file=c_file, model_name=model_name, **parameters)\n",
    "\n",
    "manager = ABAEModelManager(config)\n",
    "iteration_model = manager.get_inference_model()\n",
    "\n",
    "word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "aspect_embeddings = normalize(iteration_model.get_layer('aspect_embedding').w)\n",
    "\n",
    "# word_emb = iteration_model.get_layer('word_embedding').weights[0].value.data\n",
    "# aspect_embeddings = iteration_model.get_layer('aspect_embedding').w\n",
    "\n",
    "inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=20) for a in aspect_embeddings]\n",
    "aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]  # Remap\n",
    "\n",
    "ds = TokenizedDataset(c_file, manager.embedding_model.vocabulary())\n",
    "dictionary = corpora.Dictionary(ds.text_ds.apply(lambda x: x.split(' ')).to_list())\n",
    "\n",
    "_, m = coherence_model_generation(aspect_words, ds.text_ds.apply(lambda x: x.split(' ')), dictionary, topn=10)\n",
    "\n",
    "m.get_coherence()"
   ],
   "id": "804ef5608c713f42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "manager.embedding_model.model.wv",
   "id": "6f3bd8c300970a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(ds)",
   "id": "49c5e904ed44fc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.047733400Z",
     "start_time": "2025-02-08T14:48:15.311164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from core.dataset import SimpleWord2VecEmbeddingsDataset\n",
    "\n",
    "iteration_model = manager.get_inference_model()\n",
    "custom_review = [\n",
    "    'game hard fun', # 0\n",
    "    'brother forever decide',\n",
    "    'zero luck all skill',\n",
    "    'lot downtime', # 3\n",
    "    'i have banana',\n",
    "    'plastic inserts',\n",
    "    'complex game rules' # 6\n",
    "]\n",
    "# todo rebuild a prediciton model senza negative predict. Devo ristrutturare tutto1\n",
    "ds = SimpleWord2VecEmbeddingsDataset(custom_review, manager.embedding_model.model)\n",
    "\n",
    "res = iteration_model.predict(DataLoader(ds, batch_size=32))"
   ],
   "id": "2a96d8734cbb8224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.047733400Z",
     "start_time": "2025-02-08T14:48:16.527624Z"
    }
   },
   "cell_type": "code",
   "source": "res[0]",
   "id": "4d41a4a20d0ec5fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19045994, 0.20182857, 0.60771143],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.78310496, 0.10843912, 0.10845596],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09478945, 0.7004037 , 0.09501231, 0.10979458],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11920297, 0.880797  ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46831053, 0.06337895, 0.46831053],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.73402816, 0.26597184],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34070113, 0.07860252, 0.5806964 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:50:37.063766800Z",
     "start_time": "2025-02-08T14:48:17.234528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Il modello fa schifo non serve a niente.\n",
    "res[1]  # Riddle correctly yields a classification to the aspect for representative."
   ],
   "id": "70e3c77deb50ee48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06208516, 0.03988646, 0.06572662, 0.07758018, 0.05268136,\n",
       "        0.07338277, 0.07092224, 0.05171687, 0.05588178, 0.07390964,\n",
       "        0.05801188, 0.05768691, 0.05894613, 0.06593319, 0.05774988,\n",
       "        0.07789893],\n",
       "       [0.04758715, 0.07573349, 0.04342123, 0.06475626, 0.05591242,\n",
       "        0.05736869, 0.09891937, 0.08390705, 0.08602379, 0.06010805,\n",
       "        0.06240946, 0.04144665, 0.06161851, 0.06739597, 0.04670087,\n",
       "        0.04669097],\n",
       "       [0.04568482, 0.03296549, 0.06882677, 0.02682647, 0.02996501,\n",
       "        0.07791319, 0.08192724, 0.04530492, 0.10266625, 0.0589298 ,\n",
       "        0.07926102, 0.09818587, 0.05211449, 0.06563818, 0.07404153,\n",
       "        0.05974901],\n",
       "       [0.05294081, 0.03399593, 0.06246865, 0.05362319, 0.04215889,\n",
       "        0.13148265, 0.04610087, 0.06178078, 0.05657477, 0.03980442,\n",
       "        0.11232825, 0.07897851, 0.03860776, 0.06544577, 0.06100322,\n",
       "        0.06270555],\n",
       "       [0.05987815, 0.06911524, 0.07219891, 0.06626628, 0.07097723,\n",
       "        0.06333521, 0.04333451, 0.05645394, 0.06578687, 0.04910159,\n",
       "        0.05067483, 0.07636393, 0.0876514 , 0.08682899, 0.04215069,\n",
       "        0.03988221],\n",
       "       [0.06090822, 0.05416001, 0.05484923, 0.07206284, 0.04447555,\n",
       "        0.0423054 , 0.0376127 , 0.08502938, 0.04854946, 0.03835498,\n",
       "        0.04996435, 0.05233507, 0.09316815, 0.1278492 , 0.07096464,\n",
       "        0.06741078],\n",
       "       [0.08807737, 0.04694306, 0.0781647 , 0.05858212, 0.06564748,\n",
       "        0.06792763, 0.05896348, 0.07480329, 0.04973661, 0.0593056 ,\n",
       "        0.05940574, 0.04858971, 0.05704661, 0.06931483, 0.05103586,\n",
       "        0.06645595]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "m.get_coherence_per_topic()",
   "id": "64b81ef255245128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "len(ds.dataset.to_list())\n",
    "list(filter(lambda x: x.any() is None or len(x) < 1, ds.dataset.to_list()))"
   ],
   "id": "b3d2de1dcde3bd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Focus on learning rate",
   "id": "cf07a77d637e025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We fix other params and now focus entirely on lr.\n",
    "# We have already a \"promising\" range defined.\n",
    "# We look in that space so we redefine lr on ABAERandomHyperparametersSelectionWrapper"
   ],
   "id": "c6f003172b40c1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See if the Hp tuning really improved upon our results:\n",
    "We used SGD anda learned its parameters under the assumption that we would do better. <br>\n",
    "Let's see if it really is the case, or we just wasted time.\n",
    "\n",
    "For comparison we use Adam that has the advantage of being robust enough without parameter scouting."
   ],
   "id": "9b8089270d722ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#todo",
   "id": "84bcef4b87f44595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test accuracy on small test sample we filled out\n",
    "\n",
    "### Test set definition"
   ],
   "id": "fcf1124990b1034f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.pre_processing import PreProcessingService\n",
    "\n",
    "# We take around 1k records that were not seen yet from the model and label them by hand.\n",
    "dataset = pd.read_csv(\"../data/corpus.csv\")\n",
    "# Handle game names. Or not? I don't need the full pipeline I guess. todo\n",
    "game_names = pd.read_csv(\"../resources/2024-08-18.csv\")['Name']\n",
    "game_names = pd.concat([game_names, pd.Series([\"Quick\", \"Catan\"])], ignore_index=True)\n",
    "document_game_names = game_names.swifter.apply(lambda x: nlp(x)).tolist()\n",
    "\n",
    "pipeline = PreProcessingService.full_pipeline(document_game_names, \"../data/processed-dataset/full\")\n",
    "\n",
    "# Extract 1k from dataset that are not in 200k\n",
    "train_ds = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "\n",
    "# Take top 2k. (We will select some good ones and reduce the number to 1k)\n",
    "test_set = dataset[~dataset[\"comments\"].isin(train_ds[\"comments\"])]"
   ],
   "id": "ae3809bf35d257e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to use labels:",
   "id": "f792cdc1d37508dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = {\n",
    "    '0': \"Luck/Alea\",\n",
    "    '1': 'Bookkeeping',\n",
    "    '2': 'Downtime',\n",
    "    '3': 'Interaction',\n",
    "    '4': 'Bash',\n",
    "    '5': 'Complicated/Complex',  # I could watch weight to see if there is a ratio relation.\n",
    "    '6': 'Misc'\n",
    "}"
   ],
   "id": "b87ccaeccafba997",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
