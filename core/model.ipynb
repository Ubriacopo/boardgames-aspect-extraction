{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO pulisci questo file",
   "id": "b94d8a5d18957ca5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on first attempt:\n",
   "id": "6f0d71f28a41b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " # Without any hp tuning we just try and see how it goes.",
   "id": "ae6aa4303bfa4f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset.\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] \n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! What size to search is a good question and should be studied (Which I may be doing later). \n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "To tune our parameters we use a filtered version of the 50k ds. <br>\n",
    "We filter out rows that can be found on the 200k ds."
   ],
   "id": "2fc847f0fc2c3597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This is based on the idea that our dataset are generated with different seeds else it won't work\n",
    "large = pd.read_csv(\"../output/dataset/pre-processed/200k.preprocessed.csv\")\n",
    "small = pd.read_csv(\"../output/dataset/pre-processed/100k.preprocessed.csv\")\n",
    "tuning_set = small[~small[\"comments\"].isin(large[\"comments\"])]\n",
    "\n",
    "tuning_set.to_csv(\"../output/dataset/pre-processed/tuning.preprocessed.csv\", index=False)"
   ],
   "id": "a2fe0760d85289df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.evaluation import normalize, get_aspect_top_k_words, coherence_per_aspect\n",
    "from core.hp_tuning import ABAERandomHyperparametersSelectionWrapper\n",
    "from core.train import AbaeModelManager, AbaeModelConfiguration\n",
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hp_wrapper = ABAERandomHyperparametersSelectionWrapper.create()\n",
    "configurations = 15  # We try 15 different configurations\n",
    "\n",
    "seen_configurations = set()\n",
    "scores = list()\n",
    "\n",
    "corpus_file = \"../output/dataset/pre-processed/tuning.preprocessed.csv\"\n",
    "\n",
    "for i in range(configurations):\n",
    "    uuid = uuid4()\n",
    "    parameters = next(hp_wrapper)\n",
    "    while seen_configurations.__contains__(frozenset(parameters.items())):\n",
    "        print(f\"We already worked on configuration: {parameters}\")\n",
    "        parameters = next(hp_wrapper)  # In case we fetch the same config more than once.\n",
    "    print(f\"Working on configuration: {parameters}\")\n",
    "    seen_configurations.add(frozenset(parameters.items()))\n",
    "\n",
    "    # Train process\n",
    "    config = AbaeModelConfiguration(corpus_file=corpus_file, model_name=f\"tuning_{uuid}\", **parameters)\n",
    "    manager = AbaeModelManager(config)  # todo pass \"persist\". We dont want to persist these\n",
    "\n",
    "    # The dataset generation depends on the embedding model\n",
    "    ds = PositiveNegativeCommentGeneratorDataset(\n",
    "        vocabulary=manager.embedding_model.vocabulary(),\n",
    "        csv_dataset_path=config.corpus_file, negative_size=15\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=ds, batch_size=config.batch_size, shuffle=True)\n",
    "    iteration_model = manager.prepare_training_model()\n",
    "    iteration_model.fit(train_dataloader, epochs=config.epochs)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # We evaluate on the relative coherence between topics.\n",
    "    print(\"Evaluating model\")\n",
    "    word_emb = normalize(iteration_model.get_layer('word_embedding').weights[0].value.data)\n",
    "\n",
    "    aspect_embeddings = normalize(iteration_model.get_layer('aspect_emb').W)\n",
    "    print(f\"Word embeddings shape: {word_emb.shape}\")\n",
    "    inv_vocab = manager.embedding_model.model.wv.index_to_key\n",
    "\n",
    "    aspects_top_k_words = [get_aspect_top_k_words(a, word_emb, inv_vocab, top_k=50) for a in aspect_embeddings]\n",
    "\n",
    "    aspect_words = [[word[0] for word in aspect] for aspect in aspects_top_k_words]\n",
    "    coherence, coherence_model = coherence_per_aspect(aspect_words, ds.text_ds, 10)\n",
    "    scores.append(dict(coherence=coherence_model.get_coherence(), parameters=parameters))\n",
    "\n",
    "# End done.\n",
    "print(scores)"
   ],
   "id": "acbaf8a817277d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best found model training:",
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 64k - Default",
   "id": "3cbf621e796d8608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have too much data for my little PC:\n",
    "\n",
    "> Sampling: Randomly select a subset of your data that represents the overall distribution of aspects. This will help maintain diversity while reducing the size.\n",
    "Filtering: Focus on the most informative or high-quality samples. For example, if certain reviews are very short, irrelevant, or don't have useful context for aspect extraction, remove them.\n",
    "Focus on Diversity: If you reduce the data, make sure the remaining dataset is still representative of the diversity of aspects you're trying to capture."
   ],
   "id": "54caba09394445b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# How to Address Issues (If Any):\n",
    "# Introduce Hard Negatives:\n",
    "# Instead of randomly selecting negative samples, use hard negatives—examples that are more challenging to distinguish from positive pairs. This keeps the max-margin loss informative and prevents the model from converging too quickly.\n",
    "\n",
    "# Regularization:\n",
    "# Apply regularization (e.g., L2 regularization) to prevent overfitting and ensure the model generalizes well.\n",
    "\n",
    "# Early Stopping:\n",
    "# If the loss plateaus and aspect quality is satisfactory, consider using early stopping to avoid unnecessary training."
   ],
   "id": "64c6ea7554d341a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters\n",
    "These should have been discussed earlier. <br>\n",
    "We could do hyperparmeter optimization, but how do we 'validate' our model? <br>"
   ],
   "id": "12cc28937090ae66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
