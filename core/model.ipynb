{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO pulisci questo file",
   "id": "b94d8a5d18957ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:51:55.977966Z",
     "start_time": "2025-01-02T09:51:55.974153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "id": "40757908fc8c86f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:08.831997Z",
     "start_time": "2025-01-02T09:52:06.709669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "id": "9b8a8833fcaa7e6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1be19d2ab70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    ">We hope to learn vector representations of the most representative aspects for a review dataset.\n",
    "However, the aspect embedding matrix T may suffer from redundancy problems during training. [...] \n",
    "> The regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors\n",
    "> ~ Ruidan\n",
    "\n",
    "We use an Orthogonal Regulizer definition of the method can be found here: https://paperswithcode.com/method/orthogonal-regularization. <br/>\n",
    "For the code we use the default implementation provided by Keras (https://keras.io/api/layers/regularizers/)"
   ],
   "id": "a67bb8c9beca9d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aspect Embedding Size\n",
    "The aspect embedding size is what will be inferring aspects. It is closest to representative words (?). <br />\n",
    "We have to identify 7 actual aspects (luck, bookkeeping, downtime...) but that does not mean our matrix should be limited to rows only! What size to search is a good question and should be studied (Which I may be doing later). \n",
    "\n",
    "For the first try we setup the aspect_size:\n",
    ">The optimal number of rows is problem-dependent, so it’s crucial to: <br/>\n",
    "> Start with a heuristic: Begin with 2–3x the number of aspects."
   ],
   "id": "c4957d1b3784a455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For **aspect extraction**, which involves identifying key aspects or topics in text, the best early stopping method depends on your approach:\n",
    "\n",
    "### 1. Embedding-based Methods (e.g., Clustering Embeddings)\n",
    "- **Silhouette Score**: Measure the separation and compactness of clusters. Stop when the score stabilizes.\n",
    "- **Inertia/Distortion**: Track the sum of squared distances within clusters and stop when improvement flattens.\n",
    "- **Centroid Movement**: Stop when the change in cluster centroids across iterations is minimal.\n",
    "\n",
    "### 2. Topic Modeling (e.g., LDA)\n",
    "- **Perplexity**: Monitor the perplexity on a held-out dataset and stop when it stops decreasing significantly.\n",
    "- **Coherence Score**: Measure the semantic consistency of extracted topics and stop when it stabilizes.\n",
    "\n",
    "### 3. Autoencoder-based Aspect Extraction\n",
    "- **Reconstruction Loss**: Stop training when the validation reconstruction error no longer improves.\n",
    "\n",
    "### 4. Qualitative Evaluation (if feasible)\n",
    "- Periodically inspect extracted aspects for meaningfulness and diversity to decide on stopping.\n",
    "\n",
    "For **aspect extraction**, combining an automated metric (like coherence score or silhouette score) with manual inspection often yields the best results.\n"
   ],
   "id": "712e1c6f9ae346b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:11.360617Z",
     "start_time": "2025-01-02T09:52:11.358064Z"
    }
   },
   "cell_type": "code",
   "source": "aspect_size = 2 * 7 + 2  # 16 seems reasonable. We should fine tune this parameter. todo",
   "id": "319cff948ad5e033",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process:\n",
    "We have a ```AbaeModelConfiguration``` dataclass for each config we want to train on. <br>\n",
    "In the notebook we will handle things manually but there is a script: ```todo``` to run all at once."
   ],
   "id": "1dea99b96f59ac5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:18.504274Z",
     "start_time": "2025-01-02T09:52:13.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train import AbaeModelConfiguration\n",
    "\n",
    "# todo vedi se passare altri parametri.\n",
    "configs = [\n",
    "    AbaeModelConfiguration(\n",
    "        corpus_file=\"../data/processed-dataset/default/64k.preprocessed.csv\",\n",
    "        model_name=\"abae.default.64k\", aspect_size=aspect_size\n",
    "    ),\n",
    "\n",
    "    AbaeModelConfiguration(\n",
    "        corpus_file=\"../data/processed-dataset/full/64k.preprocessed.csv.\",\n",
    "        model_name=\"abae.full.64k\", aspect_size=aspect_size\n",
    "    ),\n",
    "\n",
    "    AbaeModelConfiguration(\n",
    "        corpus_file=\"../data/processed-dataset/full/256k.preprocessed.csv\",\n",
    "        model_name=\"abae.full.256k\", aspect_size=aspect_size\n",
    "    ),\n",
    "\n",
    "    AbaeModelConfiguration(\n",
    "        corpus_file=\"../data/processed-dataset/full/512k.preprocessed.csv.preprocessed.csv\",\n",
    "        model_name=\"abae.full.512k\", aspect_size=aspect_size\n",
    "    ),\n",
    "]"
   ],
   "id": "f7c703c7e5bf14aa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 64k - Default",
   "id": "3cbf621e796d8608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:19.997659Z",
     "start_time": "2025-01-02T09:52:19.995249Z"
    }
   },
   "cell_type": "code",
   "source": "config = configs[1]",
   "id": "63dfbcf5de5efd72",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:24.161148Z",
     "start_time": "2025-01-02T09:52:21.295441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.train import AbaeModelManager\n",
    "\n",
    "manager = AbaeModelManager(config)"
   ],
   "id": "43bc5f80207dcd45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80286 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "768170d0705941a6b8ca35cdaa4643d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80286 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b1ca221bd224122846b70646eb11494"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 69874 words, keeping 5554 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 144092 words, keeping 7208 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 218736 words, keeping 8081 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 295254 words, keeping 8600 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 372434 words, keeping 8811 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 445937 words, keeping 8934 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 517713 words, keeping 8976 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 593358 words, keeping 9003 word types\n",
      "INFO:gensim.models.word2vec:collected 9004 word types from a corpus of 595438 raw words and 80286 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for Word2Vec\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 9004 unique words (100.00% of original 9004, drops 0)', 'datetime': '2025-01-02T10:52:22.615792', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 595438 word corpus (100.00% of original 595438, drops 0)', 'datetime': '2025-01-02T10:52:22.616791', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 9004 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 488862.66073955694 word corpus (82.1%% of prior 595438)', 'datetime': '2025-01-02T10:52:22.646333', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 9004 words and 128 dimensions: 13722096 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-01-02T10:52:22.696723', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 8 workers on 9004 vocabulary and 128 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-01-02T10:52:22.696723', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 60 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 595438 raw words (488953 effective words) took 0.2s, 2298834 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 60 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 595438 raw words (488869 effective words) took 0.2s, 2307180 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 60 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 595438 raw words (488692 effective words) took 0.2s, 2154797 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 60 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 595438 raw words (488661 effective words) took 0.2s, 2218256 effective words/s\n",
      "DEBUG:gensim.models.word2vec:job loop exiting, total 60 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 7 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.word2vec:worker exiting, processed 8 jobs\n",
      "DEBUG:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 595438 raw words (488797 effective words) took 0.2s, 2296629 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 2977190 raw words (2443972 effective words) took 1.1s, 2128332 effective words/s', 'datetime': '2025-01-02T10:52:23.845874', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=9004, vector_size=128, alpha=0.025>', 'datetime': '2025-01-02T10:52:23.845874', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': './output/abae.full.64k/embeddings.keras', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-01-02T10:52:23.846873', 'gensim': '4.3.3', 'python': '3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': './output/abae.full.64k/embeddings.keras', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO:gensim.utils:saved ./output/abae.full.64k/embeddings.keras\n",
      "D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\PycharmProjects\\nlp-course-project\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\jacop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptions must derive from BaseException\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:52:56.083084Z",
     "start_time": "2025-01-02T09:52:55.887606Z"
    }
   },
   "cell_type": "code",
   "source": "train_model = manager.prepare_training_model('adam')",
   "id": "5f8f0c03dd95dcc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: filepath=./output/abae.full.64k/abae.full.64k.keras. Please ensure the file is an accessible `.keras` zip file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\nlp-course-project\\core\\layer.py:126: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super(WeightedAspectEmb, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:53:10.622799Z",
     "start_time": "2025-01-02T09:53:09.488579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.dataset import PositiveNegativeCommentGeneratorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = PositiveNegativeCommentGeneratorDataset(\n",
    "    vocabulary=manager.embedding_model.vocabulary(),\n",
    "    csv_dataset_path=config.corpus_file, negative_size=15\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "5067acf6d8066a28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file: ../data/processed-dataset/full/64k.preprocessed.csv.\n",
      "Generating numeric representation for each word of ds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/80286 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "264d8ef779684f929a8a40a17ac12cb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length calculation in progress...\n",
      "Max sequence length is:  206 . The limit is set to 256 tokens.\n",
      "Padding sequences to length (256).\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T10:23:50.501730Z",
     "start_time": "2025-01-02T09:53:13.242831Z"
    }
   },
   "cell_type": "code",
   "source": "train_model.fit(train_dataloader, epochs=7, batch_size=64)",
   "id": "e4bea36a8fe77a21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m260s\u001B[0m 207ms/step - loss: 14.6209 - max_margin_loss: 14.6209\n",
      "Epoch 2/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m261s\u001B[0m 208ms/step - loss: 10.0960 - max_margin_loss: 10.0960\n",
      "Epoch 3/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m264s\u001B[0m 210ms/step - loss: 8.2702 - max_margin_loss: 8.2702\n",
      "Epoch 4/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m264s\u001B[0m 210ms/step - loss: 7.8211 - max_margin_loss: 7.8211\n",
      "Epoch 5/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m261s\u001B[0m 208ms/step - loss: 7.6295 - max_margin_loss: 7.6295\n",
      "Epoch 6/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m264s\u001B[0m 210ms/step - loss: 7.5364 - max_margin_loss: 7.5364\n",
      "Epoch 7/7\n",
      "\u001B[1m1255/1255\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m263s\u001B[0m 210ms/step - loss: 7.4515 - max_margin_loss: 7.4515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1be4a3f7410>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:33:54.455043Z",
     "start_time": "2024-12-21T18:33:54.411260Z"
    }
   },
   "cell_type": "code",
   "source": "manager.persist_model()",
   "id": "3cff79d0ecdade69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:37:09.409347Z",
     "start_time": "2024-11-30T17:37:09.406072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "cc29011d02db49fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have too much data for my little PC:\n",
    "\n",
    "> Sampling: Randomly select a subset of your data that represents the overall distribution of aspects. This will help maintain diversity while reducing the size.\n",
    "Filtering: Focus on the most informative or high-quality samples. For example, if certain reviews are very short, irrelevant, or don't have useful context for aspect extraction, remove them.\n",
    "Focus on Diversity: If you reduce the data, make sure the remaining dataset is still representative of the diversity of aspects you're trying to capture."
   ],
   "id": "54caba09394445b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:53:50.259142Z",
     "start_time": "2024-11-30T17:53:50.257504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How to Address Issues (If Any):\n",
    "# Introduce Hard Negatives:\n",
    "# Instead of randomly selecting negative samples, use hard negatives—examples that are more challenging to distinguish from positive pairs. This keeps the max-margin loss informative and prevents the model from converging too quickly.\n",
    "\n",
    "# Regularization:\n",
    "# Apply regularization (e.g., L2 regularization) to prevent overfitting and ensure the model generalizes well.\n",
    "\n",
    "# Early Stopping:\n",
    "# If the loss plateaus and aspect quality is satisfactory, consider using early stopping to avoid unnecessary training."
   ],
   "id": "64c6ea7554d341a9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters\n",
    "These should have been discussed earlier. <br>\n",
    "We could do hyperparmeter optimization, but how do we 'validate' our model? <br>"
   ],
   "id": "12cc28937090ae66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
