{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T14:33:36.861942Z",
     "start_time": "2024-11-14T14:33:36.859397Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"torch\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation\n",
    "Our dataset is a corpus of reviews scrapped from the BGG API. <br /> \n",
    "In order to download the comments we make use of the ```bgg_corpus_service.py``` content."
   ],
   "id": "dbc6de67c270e005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BGG Does not directly provide a way to list all the games it has in archive therefore we used a dump created by the community (2024-08-18).",
   "id": "a4b9c9e4a84a36cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:33:36.865349Z",
     "start_time": "2024-11-14T14:33:36.863586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# /resources/2024-08-18.csv\n",
    "corpus_file = \"../data/corpus.preprocessed.csv\""
   ],
   "id": "f6c2596f30f3030c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:33:36.950334Z",
     "start_time": "2024-11-14T14:33:36.866142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import model.embeddings as embeddings\n",
    "\n",
    "embeddings_model = embeddings.WordEmbedding(\n",
    "    embeddings.LoadCorpusUtility(), max_vocab_size=10000, embedding_size=128,\n",
    "    target_model_file=\"./../data/word-embeddings.model\", corpus_file=corpus_file\n",
    ")"
   ],
   "id": "f3768d3e3077cd5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom Dataloaders Definitions\n",
    "To train the model we require a way to get elements of our dataset. I designed 3 classes for this:"
   ],
   "id": "c3a49e3e02abe587"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:34:40.741576Z",
     "start_time": "2024-11-14T14:33:36.951224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataset\n",
    "\n",
    "lazy = dataset.LazyCommentDataset(\"../data/corpus.preprocessed.csv\")\n",
    "dt = dataset.CommentDataset(embeddings_model.get_vocab(), \"../data/corpus.preprocessed.csv\")"
   ],
   "id": "3f6f491309333bda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1939904 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f1e603cfa33447b98c9a57c9858e964"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477790308\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1939904 entries, 0 to 1939903\n",
      "Series name: comments\n",
      "Non-Null Count    Dtype \n",
      "--------------    ----- \n",
      "1939904 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 455.7 MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Without preprocessing on dataset: 379211268 <br />\n",
    "If the dataset is processed already: 677671688 (Twice the size) <br />"
   ],
   "id": "88fd8fecaff7c6c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:34:40.744568Z",
     "start_time": "2024-11-14T14:34:40.742254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dt_dataloader = DataLoader(dt, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
    "lazy_dataloader = DataLoader(lazy, batch_size=32, shuffle=True, collate_fn=lambda x: x)"
   ],
   "id": "19a5bcbd8b3ab0ca",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:34:40.747239Z",
     "start_time": "2024-11-14T14:34:40.745074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dt_iterator = iter(dt_dataloader)\n",
    "lazy_iterator = iter(lazy_dataloader)"
   ],
   "id": "56b334fbcf82492b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PositiveNegativeCommentGeneratorDataset",
   "id": "bbe1db3e4a470c03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:34:44.261740Z",
     "start_time": "2024-11-14T14:34:40.747736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset import PositiveNegativeCommentGeneratorDataset\n",
    "\n",
    "embeddings_model.load_model()\n",
    "embeddings_model.get_vocab()\n",
    "vocabulary = embeddings_model.model.wv.key_to_index\n",
    "\n",
    "ds = PositiveNegativeCommentGeneratorDataset(\"./../data/corpus.preprocessed.csv\", vocabulary, 10)"
   ],
   "id": "35bf3e211ffacf6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6945b26f9994bda822397de8852a38c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (1000, 1017) instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m embeddings_model\u001B[38;5;241m.\u001B[39mget_vocab()\n\u001B[1;32m      5\u001B[0m vocabulary \u001B[38;5;241m=\u001B[39m embeddings_model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mkey_to_index\n\u001B[0;32m----> 7\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mPositiveNegativeCommentGeneratorDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./../data/corpus.preprocessed.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/model/dataset.py:95\u001B[0m, in \u001B[0;36mPositiveNegativeCommentGeneratorDataset.__init__\u001B[0;34m(self, csv_dataset_path, vocabulary, negative_size)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomments\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m1000\u001B[39m)\u001B[38;5;241m.\u001B[39mswifter\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_numeric_representation([token\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnlp(x)])\n\u001B[1;32m     92\u001B[0m )\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(x))\u001B[38;5;241m.\u001B[39mmax()\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(pre\u001B[38;5;241m.\u001B[39msequence\u001B[38;5;241m.\u001B[39mpad_sequences(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, maxlen\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length)\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnegative_size \u001B[38;5;241m=\u001B[39m negative_size\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/series.py:584\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    582\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 584\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    586\u001B[0m     manager \u001B[38;5;241m=\u001B[39m _get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.data_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/construction.py:659\u001B[0m, in \u001B[0;36msanitize_array\u001B[0;34m(data, index, dtype, copy, allow_2d)\u001B[0m\n\u001B[1;32m    656\u001B[0m             subarr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, subarr)\n\u001B[1;32m    657\u001B[0m             subarr \u001B[38;5;241m=\u001B[39m maybe_infer_to_datetimelike(subarr)\n\u001B[0;32m--> 659\u001B[0m subarr \u001B[38;5;241m=\u001B[39m \u001B[43m_sanitize_ndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_2d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subarr, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001B[39;00m\n\u001B[1;32m    663\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mdtype, dtype)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-course-project/venv/lib/python3.12/site-packages/pandas/core/construction.py:718\u001B[0m, in \u001B[0;36m_sanitize_ndim\u001B[0;34m(result, data, dtype, index, allow_2d)\u001B[0m\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allow_2d:\n\u001B[1;32m    717\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m--> 718\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData must be 1-dimensional, got ndarray of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    720\u001B[0m     )\n\u001B[1;32m    721\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_object_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype):\n\u001B[1;32m    722\u001B[0m     \u001B[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001B[39;00m\n\u001B[1;32m    724\u001B[0m     result \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39masarray_tuplesafe(data, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mValueError\u001B[0m: Data must be 1-dimensional, got ndarray of shape (1000, 1017) instead"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
